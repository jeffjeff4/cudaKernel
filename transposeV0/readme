1. åŠ ä¸Š PyTorch CUDA extension ç¤ºä¾‹ï¼ˆtorch.utils.cpp_extensionï¼‰æ¥è°ƒç”¨è¿™ä¸ª transpose kernel
2. å†åŠ ä¸ª PyTorch ä¸Šçš„éªŒè¯ä»£ç 

ä½ è¯·æ±‚çš„ç¤ºä¾‹åœ¨ç¼–è¯‘ PyTorch CUDA extension è¿‡ç¨‹ä¸­é‡åˆ°äº†å†…å­˜ä¸è¶³ï¼ˆMemoryErrorï¼‰ï¼Œè¿™é€šå¸¸å‡ºç°åœ¨èµ„æºå—é™ç¯å¢ƒï¼ˆå¦‚æŸäº›äº‘ notebook æˆ–æ²™ç›’ç¯å¢ƒï¼‰ä¸­ã€‚

ä¸è¿‡ï¼Œæˆ‘ä¼šåœ¨è¿™é‡Œå®Œæ•´æä¾› PyTorch CUDA extension + Shared Memory Transpose Kernel çš„ä»£ç ç»“æ„ä¸éªŒè¯æ–¹å¼ï¼Œä½ å¯ä»¥åœ¨æœ¬åœ°æœºå™¨æˆ–å…·å¤‡ CUDA ç¯å¢ƒçš„æœåŠ¡å™¨ä¸Šè¿è¡Œå®ƒã€‚

âœ… 1. CUDA kernel æ–‡ä»¶ï¼štranspose_cuda_kernel.cu
cpp
Copy
Edit
#include <cuda_runtime.h>
#include <torch/extension.h>

#define TILE_DIM 32
#define BLOCK_ROWS 8

__global__ void transpose_shared_kernel(float* odata, const float* idata, int width, int height) {
    __shared__ float tile[TILE_DIM][TILE_DIM + 1];  // é¿å… bank conflict

    int x = blockIdx.x * TILE_DIM + threadIdx.x;
    int y = blockIdx.y * TILE_DIM + threadIdx.y;

    if (x < width && y < height)
        tile[threadIdx.y][threadIdx.x] = idata[y * width + x];

    __syncthreads();

    x = blockIdx.y * TILE_DIM + threadIdx.x;
    y = blockIdx.x * TILE_DIM + threadIdx.y;

    if (x < height && y < width)
        odata[y * height + x] = tile[threadIdx.x][threadIdx.y];
}

void transpose_shared(torch::Tensor input, torch::Tensor output) {
    const int width = input.size(1);
    const int height = input.size(0);

    dim3 blockDim(TILE_DIM, BLOCK_ROWS);
    dim3 gridDim((width + TILE_DIM - 1) / TILE_DIM,
                 (height + TILE_DIM - 1) / TILE_DIM);

    transpose_shared_kernel<<<gridDim, blockDim>>>(
        output.data_ptr<float>(),
        input.data_ptr<float>(),
        width,
        height
    );
}

PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {
    m.def("transpose_shared", &transpose_shared, "Shared Memory Transpose");
}
âœ… 2. Python æ„å»ºä¸æµ‹è¯•ä»£ç ï¼štest_transpose.py
python
Copy
Edit
import torch
from torch.utils.cpp_extension import load

transpose_ext = load(
    name="transpose_cuda",
    sources=["transpose_cuda_kernel.cu"],
    verbose=True,
    extra_cuda_cflags=["-O3"]
)

# æ„å»ºè¾“å…¥æ•°æ®
input_tensor = torch.randn(128, 64, device='cuda')
output_tensor = torch.empty(64, 128, device='cuda')

# è°ƒç”¨ CUDA kernel
transpose_ext.transpose_shared(input_tensor, output_tensor)

# éªŒè¯æ­£ç¡®æ€§
expected = input_tensor.t()
diff = (output_tensor - expected).abs().max()
print("Max difference:", diff.item())
ğŸ“Œ ç¼–è¯‘è¦æ±‚
ä½ éœ€è¦æœ¬åœ°å®‰è£…ä»¥ä¸‹å†…å®¹ï¼š

PyTorchï¼ˆå»ºè®® 2.0 ä»¥ä¸Šï¼‰

NVIDIA CUDA Toolkitï¼ˆå»ºè®® 11.x æˆ– 12.xï¼‰

nvcc å¯ç”¨äºç¼–è¯‘ .cu æ–‡ä»¶

è¿è¡Œå‘½ä»¤ï¼š

bash
Copy
Edit
python test_transpose.py
ğŸ“ˆ å»¶ä¼¸ä¼˜åŒ–è¯´æ˜ï¼ˆåŸç†ï¼‰
tile[TILE_DIM][TILE_DIM+1]: åŠ  +1 æ˜¯ä¸ºäº†é¿å… shared memory bank conflict

shared memory + coalesced global memory accessï¼šæ¯” naive transpose å¿« 5x+

