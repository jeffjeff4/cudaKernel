#include <cuda_runtime.h>

constexpr int NUM_THREADS = 256;
constexpr int WARP_SIZE = 32;
constexpr int NUM_WARPS = (NUM_THREADS + WARP_SIZE - 1) / WARP_SIZE;

template <const int kWarpSize = WARP_SIZE>
__device__ __forceinline__ float warp_reduce_sum_f32(float val) {
    #pragma unroll
    for (int mask = kWarpSize >> 1; mask >= 1; mask >>= 1) {
        val += __shfl_xor_sync(0xffffffff, val, mask);
    }
    return val;
}

__global__ void monte_carlo_intergration_kernel(const float* y_samples, float* result, float a, float b, int n_samples) {
    int tid = threadIdx.x;
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    __shared__ float reduce_smem[NUM_WARPS];
    float sum = (idx < n_samples) ? y_samples[idx] : 0.0f;
    sum = warp_reduce_sum_f32<WARP_SIZE>(sum);
    int warp = tid / WARP_SIZE;
    int lane = tid % WARP_SIZE;
    if (lane == 0)
      reduce_smem[warp] = sum;
    __syncthreads();
    sum = (lane < NUM_WARPS) ? reduce_smem[lane] : 0.0f;
    if (warp == 0)
      sum = warp_reduce_sum_f32<WARP_SIZE>(sum);
    if (tid == 0)
    {
      atomicAdd(result, sum);
    }
}

// y_samples, result are device pointers
extern "C" void solve(const float* y_samples, float* result, float a, float b, int n_samples) {
    int threadsPerBlock = NUM_THREADS;
    int blocksPerGrid = (n_samples + threadsPerBlock - 1) / threadsPerBlock;
    monte_carlo_intergration_kernel<<<blocksPerGrid, threadsPerBlock>>>(y_samples, result, a, b, n_samples);
    float mem_res;
    cudaMemcpy(&mem_res, result, sizeof(float), cudaMemcpyDeviceToHost);
    mem_res *= (b-a) / n_samples;
    cudaMemcpy(result, &mem_res, sizeof(float), cudaMemcpyHostToDevice);
}


//--------------------------------------------------------------------------------------------------
/*
question0:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

è¿™æ®µä»£ç å®ç°äº†ä¸€ä¸ªåŸºäº **è’™ç‰¹å¡æ´›æ–¹æ³• (Monte Carlo Method)** çš„ **æ•°å€¼ç§¯åˆ†** CUDA Kernelã€‚å®ƒåˆ©ç”¨äº† GPU çš„å¹¶è¡Œå½’çº¦ (Reduction) èƒ½åŠ›ï¼Œé«˜æ•ˆåœ°è®¡ç®—å¤§é‡éšæœºé‡‡æ ·çš„å‡½æ•°å€¼ä¹‹å’Œã€‚

-----

## âš™ï¸ I. æ ¸å¿ƒæ•°å­¦åŸç†ï¼šè’™ç‰¹å¡æ´›ç§¯åˆ†

è’™ç‰¹å¡æ´›ç§¯åˆ†ä½¿ç”¨éšæœºæŠ½æ ·æ¥è¿‘ä¼¼è®¡ç®—å®šç§¯åˆ† $\int_a^b f(x) dx$ã€‚

åŸºæœ¬å…¬å¼ä¸ºï¼š
$$\int_a^b f(x) dx \approx (b - a) \cdot \frac{1{N \sum_{i=1^N f(x_i)$$

è¯¥ Kernel çš„ä»»åŠ¡æ˜¯å¹¶è¡Œè®¡ç®— $\sum f(x_i)$ï¼Œå³ y_samples æ•°ç»„ä¸­æ‰€æœ‰å…ƒç´ çš„æ€»å’Œã€‚æœ€ç»ˆçš„ä¹˜æ³• $(b-a)/N$ åœ¨ä¸»æœºç«¯ (solve å‡½æ•°) å®Œæˆã€‚

-----

## ğŸš€ II. è¾…åŠ©å‡½æ•°ï¼šWarp å½’çº¦ (`warp_reduce_sum_f32`)

c
// ...
for (int mask = kWarpSize >> 1; mask >= 1; mask >>= 1) {
    val += __shfl_xor_sync(0xffffffff, val, mask);

return val;


  * **ç›®çš„:** åœ¨ä¸€ä¸ª Warp (32 ä¸ªçº¿ç¨‹) å†…éƒ¨ï¼Œä½¿ç”¨ **`__shfl_xor_sync`** æŒ‡ä»¤é«˜æ•ˆåœ°æ±‚å’Œã€‚
  * **æœºåˆ¶:** è¿™æ˜¯ä¸€ä¸ªæ ‘å½¢æ±‚å’Œç®—æ³•ã€‚å®ƒé€šè¿‡å¯„å­˜å™¨ä¹‹é—´çš„ç›´æ¥æ•°æ®äº¤æ¢æ¥ç´¯åŠ ï¼Œé¿å…äº†æ…¢é€Ÿçš„å…±äº«å†…å­˜è®¿é—®å’Œ `__syncthreads()` åŒæ­¥ï¼Œæ˜¯ GPU ä¸Šæœ€å¿«çš„å½’çº¦æ–¹å¼ã€‚
  * **ç¤ºä¾‹:** å‡è®¾ kWarpSize=32$ã€‚
    1.  mask=16$ï¼šçº¿ç¨‹ $tx$ æ¥æ”¶æ¥è‡ª $tx \oplus 16$ çš„å€¼ã€‚
    2.  mask=8$ï¼šçº¿ç¨‹ $tx$ æ¥æ”¶æ¥è‡ª $tx \oplus 8$ çš„å€¼ã€‚
    3.  ...
    4.  mask=1$ï¼šçº¿ç¨‹ $tx$ æ¥æ”¶æ¥è‡ª $tx \oplus 1$ çš„å€¼ã€‚
    <!-- end list -->
      * **ç»“æœ:** Warp çš„æ€»å’Œæœ€ç»ˆé›†ä¸­åˆ° **çº¿ç¨‹ 0**ï¼ˆ`lane=0`ï¼‰çš„ `val` å˜é‡ä¸­ã€‚

-----

## ğŸ§  III. Kernel æ‰§è¡Œæµç¨‹ï¼šä¸¤çº§å½’çº¦

Kernel `monte_carlo_intergration_kernel` æ‰§è¡Œä¸€ä¸ª **ä¸¤çº§å½’çº¦ (Two-Level Reduction)**ï¼šå…ˆåœ¨ Warp å†…æ±‚å’Œï¼Œå†åœ¨ Block å†…åˆå¹¶ Warp ç»“æœã€‚

### 1\. çº¿ç¨‹èº«ä»½å’Œå±€éƒ¨æ±‚å’Œ

c
int tid = threadIdx.x;
int idx = blockDim.x * blockIdx.x + threadIdx.x;
__shared__ float reduce_smem[NUM_WARPS];
float sum = (idx < n_samples) ? y_samples[idx] : 0.0f; // è¾¹ç•Œæ£€æŸ¥
sum = warp_reduce_sum_f32<WARP_SIZE>(sum);


  * **å…¨å±€ç´¢å¼• (idx):** ç¡®ä¿æ¯ä¸ªçº¿ç¨‹å¤„ç† y_samples æ•°ç»„ä¸­çš„ä¸€ä¸ªç‹¬ç‰¹å…ƒç´ ã€‚
  * **å±€éƒ¨æ±‚å’Œ (sum):** æ¯ä¸ªçº¿ç¨‹åŠ è½½å…¶æ•°æ®ç‚¹ï¼Œç„¶å **Warp å½’çº¦** å°†è¿™ 32 ä¸ªçº¿ç¨‹çš„ sum é›†ä¸­åˆ° lane=0 çš„çº¿ç¨‹ä¸­ã€‚

### 2\. ç¬¬ä¸€çº§å½’çº¦ï¼šWarp --> Shared Memory

c
int warp = tid / WARP_SIZE; // 0 åˆ° 7
int lane = tid % WARP_SIZE; // 0 åˆ° 31
if (lane == 0)
  reduce_smem[warp] = sum; // åªæœ‰ lane=0 çš„çº¿ç¨‹å†™å…¥
__syncthreads();


  * **ç›®çš„:** å°†æ¯ä¸ª Warp çš„æ€»å’Œå­˜å‚¨åˆ° Shared Memory ä¸­ã€‚
  * **ç¤ºä¾‹:** å‡è®¾ NUM_THREADS=256$ï¼ŒNUM_WARPS=8$ã€‚
      * çº¿ç¨‹ tx=0 (warp=0, \text{lane=0$) å°† Warp 0 çš„æ€»å’Œå†™å…¥ reduce_smem[0]ã€‚
      * çº¿ç¨‹ tx=32 (warp=1, \text{lane=0$) å°† Warp 1 çš„æ€»å’Œå†™å…¥ reduce_smem[1]ã€‚
  * **`__syncthreads()`:** ç¡®ä¿æ‰€æœ‰ 8 ä¸ª Warp çš„æ€»å’Œéƒ½å®‰å…¨åœ°å†™å…¥äº† reduce_smem æ•°ç»„ã€‚

### 3\. ç¬¬äºŒçº§å½’çº¦ï¼šShared Memory --> æœ€ç»ˆç»“æœ

c
sum = (lane < NUM_WARPS) ? reduce_smem[lane] : 0.0f; // çº¿ç¨‹ 0-7 è¯»å–æ•°æ®
if (warp == 0)
  sum = warp_reduce_sum_f32<WARP_SIZE>(sum); // åªæœ‰ Warp 0 è¿›è¡Œå½’çº¦
if (tid == 0)
{
  atomicAdd(result, sum); // çº¿ç¨‹ 0 å†™å…¥å…¨å±€ç»“æœ



  * **æ•°æ®æ”¶é›†:** åªæœ‰ Warp 0 ä¸­çš„å‰ 8 ä¸ªçº¿ç¨‹ (lane=0$ åˆ° $7$) ä» reduce_smem ä¸­è¯»å– 8 ä¸ª Warp çš„æ€»å’Œã€‚
  * **äºŒæ¬¡å½’çº¦:** **Warp 0** å¯¹è¿™ 8 ä¸ªå€¼è¿›è¡Œç¬¬äºŒæ¬¡ **Warp å½’çº¦**ï¼Œå¾—åˆ°æ•´ä¸ª Block çš„æ€»å’Œã€‚
  * **åŸå­ç´¯åŠ :** æœ€ç»ˆï¼Œåªæœ‰ tid=0 çš„çº¿ç¨‹ä½¿ç”¨ **`atomicAdd`** å°†è¯¥ Block çš„æœ€ç»ˆæ€»å’Œç´¯åŠ åˆ°å…¨å±€ç»“æœ result ä¸­ã€‚

## ğŸ IV. ä¸»æœºç«¯ (`solve` å‡½æ•°)

c
// ...
compute_xty<<<blocksPerGrid, threadsPerBlock>>>(...);
float mem_res;
cudaMemcpy(&mem_res, result, sizeof(float), cudaMemcpyDeviceToHost);
mem_res *= (b-a) / n_samples; // åº”ç”¨è’™ç‰¹å¡æ´›ç§¯åˆ†å…¬å¼çš„ (b-a)/N éƒ¨åˆ†
cudaMemcpy(result, &mem_res, sizeof(float), cudaMemcpyHostToDevice);


  * **Grid/Block å¯åŠ¨:** è®¡ç®—å¹¶å¯åŠ¨ Kernelï¼Œè¦†ç›–æ‰€æœ‰ n_samplesã€‚
  * **æœ€ç»ˆå…¬å¼:** åœ¨ GPU å®Œæˆæ±‚å’Œåï¼Œä¸»æœºç«¯å°†æ€»å’Œ (`mem_res`) ä¹˜ä»¥ (b-a) / n_samplesï¼Œå®Œæˆæœ€ç»ˆçš„ç§¯åˆ†è®¡ç®—ã€‚

//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question1:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

è¿™æ®µä»£ç æ˜¯ **è’™ç‰¹å¡æ´›æ•°å€¼ç§¯åˆ†** çš„æ ¸å¿ƒ CUDA Kernelï¼Œå®ƒæ‰§è¡Œä¸€ä¸ª\*\*ä¸¤çº§å¹¶è¡Œå½’çº¦ï¼ˆTwo-Level Parallel Reductionï¼‰\*\*æ¥è®¡ç®—æ‰€æœ‰é‡‡æ ·å€¼ y_samples çš„æ€»å’Œã€‚

è¿™ä¸ª Kernel çš„ç›®æ ‡æ˜¯ï¼šå°†å·¨å¤§çš„æ±‚å’Œä»»åŠ¡åˆ†è§£åˆ°æ¯ä¸ªçº¿ç¨‹å—ï¼ˆBlockï¼‰ä¸­ï¼Œç„¶ååœ¨å—å†…éƒ¨è¿›è¡Œé«˜æ•ˆæ±‚å’Œã€‚

-----

## âš™ï¸ I. ç¤ºä¾‹å‚æ•°å’Œçº¿ç¨‹èº«ä»½

æˆ‘ä»¬å‡è®¾ä»¥ä¸‹å‚æ•°ï¼š

  * **çº¿ç¨‹æ•° (NUM_THREADS)** = 256
  * **Warp å¤§å° (WARP_SIZE)** = 32
  * **Warp æ•°é‡ (NUM_WARPS)** = 256 / 32 = **8**
  * **Block å°ºå¯¸ (blockDim.x)** = 256

### 1\. çº¿ç¨‹èº«ä»½åˆ†è§£

c
int tid = threadIdx.x;
int idx = blockDim.x * blockIdx.x + threadIdx.x;
// ...
int warp = tid / WARP_SIZE; // 0 åˆ° 7
int lane = tid % WARP_SIZE; // 0 åˆ° 31


  * idxï¼š**å…¨å±€æ•°æ®ç´¢å¼•**ã€‚ç¡®å®šå½“å‰çº¿ç¨‹è´Ÿè´£å¤„ç† `y_samples` æ•°ç»„ä¸­çš„å“ªä¸€ä¸ªå…ƒç´ ã€‚
  * warpï¼šçº¿ç¨‹å—å†…çš„ **Warp ID** (0 åˆ° 7)ã€‚
  * laneï¼šWarp å†…éƒ¨çš„ **çº¿ç¨‹ ID** (0 åˆ° 31)ã€‚

-----

## ğŸš€ II. ç¬¬ä¸€çº§å½’çº¦ï¼šWarp å†…éƒ¨æ±‚å’Œ (æœ€å¿«é€Ÿåº¦)

c
float sum = (idx < n_samples) ? y_samples[idx] : 0.0f;
sum = warp_reduce_sum_f32<WARP_SIZE>(sum);


1.  **æ•°æ®åŠ è½½:** æ¯ä¸ªçº¿ç¨‹åŠ è½½å…¶å¯¹åº”çš„ y_samples[idx] å€¼åˆ°ç§æœ‰å˜é‡ sum ä¸­ï¼Œå¹¶è¿›è¡Œè¾¹ç•Œæ£€æŸ¥ã€‚
2.  **Warp å½’çº¦:** è°ƒç”¨ `warp_reduce_sum_f32`ã€‚è¿™æ˜¯ä¸€ä¸ªåŸºäº **Shuffle æŒ‡ä»¤**çš„å‡½æ•°ã€‚
3.  **ç»“æœ:** æ¯ä¸ª Warpï¼ˆ32 ä¸ªçº¿ç¨‹ï¼‰çš„æ€»å’Œè¢«ç´¯ç§¯ï¼Œå¹¶å­˜å‚¨åˆ°è¯¥ Warp çš„ **`lane=0` çº¿ç¨‹**çš„ sum å˜é‡ä¸­ã€‚

> **ç¤ºä¾‹:** çº¿ç¨‹ tx=0 åˆ° tx=31ï¼ˆWarp 0ï¼‰çš„æ•°æ®æ€»å’Œï¼Œç°åœ¨å­˜å‚¨åœ¨çº¿ç¨‹ tx=0 çš„ sum å˜é‡ä¸­ã€‚

-----

## ğŸ§  III. ç¬¬äºŒçº§å½’çº¦ï¼šBlock å†…éƒ¨æ±‚å’Œ (Warp é—´åä½œ)

è¿™ä¸€é˜¶æ®µå°†æ‰€æœ‰ 8 ä¸ª Warp çš„ç»“æœåˆå¹¶æˆ Block çš„æ€»å’Œã€‚

### 1\. å†™å…¥å…±äº«å†…å­˜ (Shared Memory)

c
__shared__ float reduce_smem[NUM_WARPS]; // 8 ä¸ªæµ®ç‚¹æ•°
if (lane == 0)
  reduce_smem[warp] = sum;
__syncthreads();


  * **å†™å…¥:** åªæœ‰æ¯ä¸ª Warp çš„ lane=0 çº¿ç¨‹æ‰§è¡Œå†™å…¥æ“ä½œã€‚
      * çº¿ç¨‹ tx=0 (warp=0$) å°† Warp 0 çš„æ€»å’Œå†™å…¥ reduce_smem[0]ã€‚
      * çº¿ç¨‹ tx=32 (warp=1$) å°† Warp 1 çš„æ€»å’Œå†™å…¥ reduce_smem[1]ã€‚
      * ... å…±æœ‰ 8 ä¸ª Warp çš„æ€»å’Œè¢«å†™å…¥ `reduce_smem[0]` åˆ° `reduce_smem[7]`ã€‚
  * **`__syncthreads()`:** **å…³é”®åŒæ­¥ç‚¹ã€‚** ç¡®ä¿æ‰€æœ‰ 8 ä¸ª Warp çš„ç»“æœéƒ½å·²å®‰å…¨åœ°å†™å…¥å…±äº«å†…å­˜ï¼Œæ‰èƒ½è¿›å…¥ä¸‹ä¸€æ­¥çš„è¯»å–å’Œå½’å¹¶ã€‚

### 2\. è¯»å–å’Œæœ€ç»ˆå½’çº¦

c
sum = (lane < NUM_WARPS) ? reduce_smem[lane] : 0.0f; // NUM_WARPS = 8
if (warp == 0)
  sum = warp_reduce_sum_f32<WARP_SIZE>(sum);


  * **æ•°æ®æ”¶é›†:** åªæœ‰ Warp 0 ä¸­çš„å‰ 8 ä¸ªçº¿ç¨‹ï¼ˆlane=0$ åˆ° $7$ï¼‰ä¼šæ‰§è¡Œè¯»å–æ“ä½œï¼Œå®ƒä»¬è¯»å– `reduce_smem[0]` åˆ° `reduce_smem[7]`ã€‚å…¶ä»–çº¿ç¨‹ (lane \ge 8$) å°† sum è®¾ä¸º 0ã€‚
  * **äºŒæ¬¡å½’çº¦:** æ•´ä¸ª Warp 0 å†æ¬¡æ‰§è¡Œ **Warp å½’çº¦**ã€‚è¿™æ¬¡æ˜¯å°† 8 ä¸ª Warp æ€»å’Œï¼ˆä»¥åŠ 24 ä¸ªé›¶å€¼ï¼‰åˆå¹¶æˆä¸€ä¸ªæ€»å’Œã€‚
  * **ç»“æœ:** æ•´ä¸ªçº¿ç¨‹å—çš„æ€»å’Œç°åœ¨é›†ä¸­åœ¨ **çº¿ç¨‹ tx=0** çš„ sum å˜é‡ä¸­ã€‚

-----

## ğŸ IV. ç»“æœå†™å›

c
if (tid == 0)
{
  atomicAdd(result, sum);



  * åªæœ‰ Block å†…çš„ç¬¬ä¸€ä¸ªçº¿ç¨‹ (tid=0$) æ‰§è¡Œæ­¤æ“ä½œã€‚
  * **`atomicAdd(result, sum)`:** å°†å½“å‰ Block è®¡ç®—å‡ºçš„æ€»å’Œå®‰å…¨åœ°ç´¯åŠ åˆ°å…¨å±€ç»“æœ result å˜é‡ä¸­ã€‚**åŸå­æ“ä½œ** ç¡®ä¿äº†æ¥è‡ªä¸åŒ Block çš„æ€»å’Œèƒ½å¤Ÿæ­£ç¡®ã€æ— å†²çªåœ°åˆå¹¶ã€‚

**æ€»ç»“ï¼š** è¿™ä¸ª Kernel åˆ©ç”¨ä¸¤çº§å½’çº¦ï¼ˆShuffle --> Shared Memory --> Shuffleï¼‰å®ç°äº†å¯¹ n_samples ä¸ªæ•°æ®çš„è¶…é«˜æ•ˆæ±‚å’Œï¼Œå¹¶å°†ç»“æœå®‰å…¨åœ°è´¡çŒ®ç»™æœ€ç»ˆç§¯åˆ†ç»“æœã€‚


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question2:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

sum = (lane < NUM_WARPS) ? reduce_smem[lane] : 0.0f; // NUM_WARPS = 8


è¿™æ®µä»£ç æ˜¯ CUDA å½’çº¦ (Reduction) ç®—æ³•çš„ç¬¬äºŒé˜¶æ®µï¼Œç›®çš„æ˜¯å°† **Shared Memory** ä¸­å­˜å‚¨çš„å±€éƒ¨ç»“æœï¼ˆå³æ¯ä¸ª Warp çš„æ€»å’Œï¼‰åˆ†å‘ç»™ä¸€ä¸ª Warpï¼ˆé€šå¸¸æ˜¯ Warp 0ï¼‰ï¼Œä»¥ä¾¿è¿›è¡Œæœ€ç»ˆçš„åˆå¹¶è®¡ç®—ã€‚

---

## âš™ï¸ I. æ ¸å¿ƒç›®æ ‡ï¼šæ•°æ®æ”¶é›†

* **æºæ•°æ®:** reduce_smem æ•°ç»„ï¼Œå®ƒå­˜å‚¨äº† Block å†…æ‰€æœ‰ Warp çš„æ€»å’Œã€‚
* **ç›®æ ‡:** å°† reduce_smem ä¸­çš„æœ‰æ•ˆæ•°æ®ï¼Œå®‰å…¨åœ°å¯¼å…¥åˆ° **Warp 0** çš„å¯„å­˜å™¨ä¸­ã€‚

## ğŸ”¢ II. ç¤ºä¾‹å‚æ•°è®¾å®š

æˆ‘ä»¬å‡è®¾ä»¥ä¸‹å‚æ•°ï¼š

* **NUM_WARPS (Warp æ€»æ•°)** = 8
* reduce_smem æ•°ç»„çš„é•¿åº¦æ˜¯ 8ï¼Œå…¶ä¸­ reduce_smem[0] åˆ° reduce_smem[7] å­˜å‚¨äº† 8 ä¸ª Warp çš„æ€»å’Œã€‚
* **çº¿ç¨‹èº«ä»½:** lane èŒƒå›´æ˜¯ 0 åˆ° 31ã€‚

### 1. é€»è¾‘åˆ†å·¥

ä»£ç åªåœ¨ **Warp 0** ä¸­æ‰§è¡Œï¼Œå› ä¸º `if (warp == 0)` æ£€æŸ¥ç´§éšå…¶åã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬åªçœ‹ lane \in [0, 31]$ çš„çº¿ç¨‹ã€‚

| çº¿ç¨‹ Lane ID (lane) | æ£€æŸ¥æ¡ä»¶ lane < 8 | ç»“æœ (sum çš„æ–°å€¼) | å«ä¹‰ |
| :--- | :--- | :--- | :--- |
| **0** | True (0 < 8) | reduce_smem[0]$ | çº¿ç¨‹ 0 è¯»å– Warp 0 çš„æ€»å’Œã€‚ |
| **1** | True (1 < 8) | reduce_smem[1]$ | çº¿ç¨‹ 1 è¯»å– Warp 1 çš„æ€»å’Œã€‚ |
| **7** | True (7 < 8) | reduce_smem[7]$ | çº¿ç¨‹ 7 è¯»å– Warp 7 çš„æ€»å’Œã€‚ |
| **8** | **False** (8 ä¸å°äº 8) | 0.0f | çº¿ç¨‹ 8 å°†å…¶ sum è®¾ä¸º 0ã€‚ |
| **31** | False (31 ä¸å°äº 8) | 0.0f | çº¿ç¨‹ 31 å°†å…¶ sum è®¾ä¸º 0ã€‚ |

### 2. ä¸ºä»€ä¹ˆéœ€è¦ `if/else`ï¼Ÿ

* **æœ‰æ•ˆæ•°æ®å®šä½:** æ•°ç»„ reduce_smem åªæœ‰ 8 ä¸ªæœ‰æ•ˆå…ƒç´ ã€‚æˆ‘ä»¬åªéœ€è¦ Warp 0 ä¸­çš„ **å‰ 8 ä¸ªçº¿ç¨‹** æ¥è¯»å–è¿™ 8 ä¸ªå…ƒç´ ã€‚
* **ä¿æŒæ´»è·ƒ:** Warp 0 ä¸­çš„å…¶ä½™ 24 ä¸ªçº¿ç¨‹ (lane=8$ åˆ° $31$) å¿…é¡»ä¿æŒæ´»è·ƒï¼ˆä¸èƒ½é€€å‡ºï¼‰ï¼Œä»¥ä¾¿å‚ä¸åç»­çš„ **Warp å½’çº¦** (`warp_reduce_sum_f32`)ã€‚å°†å®ƒä»¬çš„ sum è®¾ä¸º $0.0f$ å¯ä»¥ç¡®ä¿å®ƒä»¬åœ¨å½’çº¦ä¸­ä¸å½±å“æœ€ç»ˆç»“æœï¼Œä½†ä»èƒ½ä¿æŒ Warp çš„åŒæ­¥ã€‚

---

## ğŸ¯ æ€»ç»“

è¿™æ®µä»£ç é€šè¿‡ **æ•°æ®æ‰‡å…¥ (Fan-In)** æœºåˆ¶ï¼Œå°† 8 ä¸ªä¸åŒçš„ Warp ç»“æœï¼Œå·§å¦™åœ°åˆ†é…ç»™äº† Warp 0 ä¸­ 32 ä¸ªçº¿ç¨‹ä¸­çš„ **å‰ 8 ä¸ªçº¿ç¨‹**ã€‚è¿™äº›çº¿ç¨‹çš„ sum å˜é‡ç°åœ¨æŒæœ‰ç­‰å¾…æœ€ç»ˆåˆå¹¶çš„ 8 ä¸ªæ•°å€¼ï¼Œä¸ºæœ€åçš„ Warp å½’çº¦å¥ å®šäº†åŸºç¡€ã€‚
//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question3:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­


//--------------------------------------------------------------------------------------------------
