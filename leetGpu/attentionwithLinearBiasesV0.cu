
#include <cuda_runtime.h>
#include <math.h>
__global__ void QKTKernel(const float * q,const float* k,float* output,int m ,int n,int d,float alpha)
{
    int row=blockIdx.y*blockDim.y+threadIdx.y;
    int col=blockIdx.x*blockDim.x+threadIdx.x;
    int index=row*n+col;
    if(row<m && col<n)
    {
        float val=0.0f;
        for(int i=0;i<d;i++)
        {
            val+=q[row*d+i]*k[col*d+i];
        }
        val/=sqrtf((float)d);
        output[index]=val+alpha*(row-col);
    }
}

__global__ void SoftmaxKernel(const float * QKt,float * output,int m,int n)
{
    int row=blockIdx.x*blockDim.x+threadIdx.x;
    if(row<m)
    {
        float row_max=-INFINITY;
        for(int col=0;col<n;col++)
        {
            row_max=fmaxf(row_max,QKt[row*n+col]);
        }
        float sum=0.0f;
        for(int col=0;col<n;col++)
        {
            float val=expf(QKt[row*n+col]-row_max);
            output[row*n+col]=val;
            sum+=val;
        }
        for(int col=0;col<n;col++)
        {
            output[row*n+col]/=sum;
        }
    }
}
__global__ void SVKernel(const float * S,const float* V,float* output,int m ,int n,int d)
{
    int row=blockIdx.y*blockDim.y+threadIdx.y;
    int col=blockIdx.x*blockDim.x+threadIdx.x;
    int index=row*d+col;
    if(row<m && col<d)
    {
        float val=0.0f;
        for(int i=0;i<n;i++)
        {
            val+=S[row*n+i]*V[i*d+col];
        }
        output[index]=val;
    }
}

// Q, K, V, output are device pointers
extern "C" void solve(const float* Q, const float* K, const float* V, float* output, int M, int N, int d,float alpha) {
    float * QKT;
    float * sftmax;
    cudaMalloc(&QKT,M*N*sizeof(float));
    cudaMalloc(&sftmax,M*N*sizeof(float));
    dim3 threads1(16,16);
    dim3 blocks1((N+threads1.x-1)/threads1.x,(M+threads1.y-1)/threads1.y);
    QKTKernel<<<blocks1,threads1>>>(Q,K,QKT,M,N,d,alpha);
    cudaDeviceSynchronize();
    int threads2=256;
    int blocks2=(M+threads2-1)/threads2;
    SoftmaxKernel<<<blocks2,threads2>>>(QKT,sftmax,M,N);
    cudaDeviceSynchronize();
    dim3 threads3(16,16);
    dim3 blocks3((d+threads3.x-1)/threads3.x,(M+threads3.y-1)/threads3.y);
    SVKernel<<<blocks3,threads3>>>(sftmax,V,output,M,N,d);
    cudaDeviceSynchronize();
    cudaFree(QKT);
    cudaFree(sftmax);
}




//--------------------------------------------------------------------------------------------------
/*
question0:
ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

è¿™æ®µä»£ç å®ç°äº†ä¸€ä¸ªç®€åŒ–çš„ **Transformer æ³¨æ„åŠ›æœºåˆ¶ (Attention Mechanism)** çš„æ ¸å¿ƒè®¡ç®—æµç¨‹ã€‚å®ƒå°†è‘—åçš„æ³¨æ„åŠ›å…¬å¼åˆ†è§£ä¸ºä¸‰ä¸ªç‹¬ç«‹çš„ CUDA Kernelï¼Œå¹¶åœ¨ä¸»æœºç«¯ (`solve` å‡½æ•°) ä¾æ¬¡è°ƒç”¨å®ƒä»¬ã€‚

**æ ¸å¿ƒå…¬å¼ (ç®€åŒ–):** Attention(Q, K, V) = Softmaxï¼ˆQ*K^T/sqrt{d + Bias) * V

-----

## âš™ï¸ I. æ ¸å¿ƒç»„ä»¶ä¸æ•°æ®æµ

ä»£ç æµç¨‹éµå¾ªä¸‰ä¸ªçŸ©é˜µè¿ç®—æ­¥éª¤ï¼š

1.  **QKTKernel:** è®¡ç®— Q K^T å¹¶è¿›è¡Œç¼©æ”¾å’Œåå·®é¡¹è°ƒæ•´ã€‚
2.  **SoftmaxKernel:** å¯¹ Q K^T ç»“æœçš„æ¯ä¸€è¡Œåº”ç”¨ Softmax å‡½æ•°ã€‚
3.  **SVKernel:** è®¡ç®— Softmax ç»“æœä¸ V çŸ©é˜µçš„ä¹˜ç§¯ (S V)ï¼Œå¾—åˆ°æœ€ç»ˆè¾“å‡ºã€‚

### çŸ©é˜µç»´åº¦å®šä¹‰

  * Q: M * d (æŸ¥è¯¢çŸ©é˜µ)
  * K: N * d (é”®çŸ©é˜µ)
  * V: N * d (å€¼çŸ©é˜µ)
  * **QK^T (è¾“å‡º):** M * N çŸ©é˜µ

-----

## ğŸš€ II. Kernel 1: QK^T è®¡ç®— (`QKTKernel`)

è¿™ä¸ª Kernel è´Ÿè´£è®¡ç®— QK^T çš„å…ƒç´ ï¼Œå¹¶åŠ å…¥ç¼©æ”¾å’Œä½ç½®åå·®é¡¹ã€‚

### 1\. çº¿ç¨‹åˆ†å·¥ä¸ç´¢å¼•

```c
int row=blockIdx.y*blockDim.y+threadIdx.y; // C çŸ©é˜µçš„è¡Œç´¢å¼• (i)
int col=blockIdx.x*blockDim.x+threadIdx.x; // C çŸ©é˜µçš„åˆ—ç´¢å¼• (j)
if(row<m && col<n) // è¾¹ç•Œæ£€æŸ¥
```

  * **åˆ†å·¥:** æ¯ä¸ªçº¿ç¨‹ (row, col) è´Ÿè´£è®¡ç®— QK^T çŸ©é˜µï¼ˆè¾“å‡ºçŸ©é˜µï¼‰çš„ä¸€ä¸ªå…ƒç´  (row, col)ã€‚
  * **æ ¸å¿ƒè®¡ç®—:** QK^T çš„ (row, col) å…ƒç´ ç­‰äº Q çŸ©é˜µçš„ row è¡Œä¸ K çŸ©é˜µçš„ col è¡Œï¼ˆå³ K^T çš„ col åˆ—ï¼‰çš„**ç‚¹ç§¯**ã€‚

### 2\. ç‚¹ç§¯å’Œç¼©æ”¾

```c
float val=0.0f;
for(int i=0;i<d;i++) // i å¾ªç¯éå†ç»´åº¦ d
{
    val+=q[row*d+i]*k[col*d+i]; // ç‚¹ç§¯ç´¯åŠ 

val/=sqrtf((float)d); // Scale by 1/sqrt(d)
output[index]=val+alpha*(row-col); // Add bias
```

  * **ç‚¹ç§¯:** å¾ªç¯å˜é‡ i éå† d ç»´åº¦ï¼Œè®¡ç®— Q çŸ©é˜µçš„ row è¡Œå’Œ K çŸ©é˜µçš„ col è¡Œçš„ç‚¹ç§¯ã€‚
  * **ç¼©æ”¾ (val /= ...):** æŒ‰ç…§æ³¨æ„åŠ›æœºåˆ¶çš„æƒ¯ä¾‹ï¼Œç‚¹ç§¯ç»“æœé™¤ä»¥ \sqrt{dã€‚
  * **åå·®é¡¹ (alpha * (row - col)):** åŠ ä¸Šä¸€ä¸ªç®€åŒ–çš„**ç›¸å¯¹ä½ç½®åå·®**é¡¹ï¼ˆSelf-Attention ä¸­å¸¸è§çš„ Bias å˜ä½“ï¼‰ã€‚

## ğŸ§  III. Kernel 2: Softmax (`SoftmaxKernel`)

è¿™ä¸ª Kernel è´Ÿè´£å¯¹ QK^T çŸ©é˜µçš„**æ¯ä¸€è¡Œ**ç‹¬ç«‹åº”ç”¨ Softmax å‡½æ•°ã€‚

### 1\. çº¿ç¨‹åˆ†å·¥

```c
int row=blockIdx.x*blockDim.x+threadIdx.x; // çº¿ç¨‹è®¡ç®—çš„è¡Œç´¢å¼•
if(row<m)
```

  * **åˆ†å·¥:** æ¯ä¸ªçº¿ç¨‹è´Ÿè´£è®¡ç®— QK^T çŸ©é˜µçš„**ä¸€è¡Œ** (å³ä¸€ä¸ªåºåˆ—çš„ Softmax)ã€‚
  * ç”±äº Softmax çš„è®¡ç®—åœ¨è¡Œå†…æ˜¯ç‹¬ç«‹çš„ï¼Œçº¿ç¨‹ä½¿ç”¨ **Grid-Stride Loop** éå†æ‰€æœ‰è¡Œã€‚

### 2\. æ•°å€¼ç¨³å®š Softmax æ­¥éª¤

æ ‡å‡†çš„ Softmax æ˜¯ e^x / (sum e^x)ã€‚ä¸ºäº†é¿å…æ•°å€¼æº¢å‡ºï¼Œä»£ç é‡‡ç”¨äº†**å‡å»æœ€å¤§å€¼**çš„ç¨³å®šæŠ€å·§ï¼š

  * **è®¡ç®—è¡Œæœ€å¤§å€¼ (row_max):**
    ```c
    for(int col=0;col<n;col++) { row_max=fmaxf(row_max,QKt[row*n+col]); 
    ```
    çº¿ç¨‹ä¸²è¡Œéå†å½“å‰è¡Œï¼Œæ‰¾åˆ°æœ€å¤§å€¼ row_maxã€‚
  * **è®¡ç®—æŒ‡æ•°å’Œ (sum):**
    ```c
    float sum=0.0f;
    for(int col=0;col<n;col++) {
        float val=expf(QKt[row*n+col]-row_max); // å‡å»æœ€å¤§å€¼åè®¡ç®—æŒ‡æ•°
        output[row*n+col]=val; // æš‚æ—¶å­˜å‚¨ e^(x - x_max)
        sum+=val; // è®¡ç®—åˆ†æ¯
    
    ```
  * **æœ€ç»ˆå½’ä¸€åŒ–:**
    ```c
    for(int col=0;col<n;col++) {
        output[row*n+col]/=sum; // output[i, j] = e^(x - x_max) / sum
    
    ```

## ğŸ IV. Kernel 3: SV ä¹˜ç§¯ (`SVKernel`)

è¿™ä¸ª Kernel è®¡ç®— Softmax ç»“æœ (S) ä¸ V çŸ©é˜µçš„ä¹˜ç§¯ï¼Œå¾—åˆ°æœ€ç»ˆçš„ Attention è¾“å‡ºã€‚

  * **è¾“å…¥:** S (M * N çŸ©é˜µï¼Œå³ Softmax è¾“å‡º), V (N * d çŸ©é˜µ)
  * **è¾“å‡º:** Output (M * d çŸ©é˜µ)

<!-- end list -->

```c
// ... (çº¿ç¨‹åˆ†å·¥ä¸ QKTKernel ç±»ä¼¼)
if(row<m && col<d)
{
    float val=0.0f;
    for(int i=0;i<n;i++) // i å¾ªç¯éå†å†…ç§¯ç»´åº¦ N
    {
        val+=S[row*n+i]*V[i*d+col]; // C[row, col] = sum_i S[row, i] * V[i, col]
    
    output[index]=val;

```

  * **åˆ†å·¥:** æ¯ä¸ªçº¿ç¨‹è´Ÿè´£è®¡ç®—æœ€ç»ˆè¾“å‡º Output çŸ©é˜µçš„ä¸€ä¸ªå…ƒç´  (row, col)ã€‚
  * **æ ¸å¿ƒè®¡ç®—:** å†…å¾ªç¯ i éå† N ç»´åº¦ï¼Œæ‰§è¡ŒçŸ©é˜µä¹˜æ³•çš„ç‚¹ç§¯ç´¯åŠ ã€‚

-----

## ğŸ’» V. ä¸»æœºç«¯æµç¨‹ (`solve`)

`solve` å‡½æ•°æ˜¯æ•´ä¸ª Attention æœºåˆ¶çš„è°ƒåº¦å™¨ï¼š

1.  **å†…å­˜åˆ†é…:** ä¸ºä¸­é—´ç»“æœ QK^T å’Œ Softmax è¾“å‡ºåˆ†é… GPU å†…å­˜ (QKT, sftmax)ã€‚
2.  **Kernel å¯åŠ¨:** ä¸¥æ ¼æŒ‰ç…§æ•°å­¦é¡ºåºä¾æ¬¡å¯åŠ¨ä¸‰ä¸ª Kernelï¼Œå¹¶åœ¨æ¯æ¬¡ Kernel åä½¿ç”¨ `cudaDeviceSynchronize()` ç¡®ä¿å‰ä¸€ä¸ª Kernel å®Œæˆæ‰èƒ½å¯åŠ¨åä¸€ä¸ªã€‚
3.  **å†…å­˜é‡Šæ”¾:** é‡Šæ”¾ä¸­é—´ç»“æœå ç”¨çš„ GPU å†…å­˜ã€‚

//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question1:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question2:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­



//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question3:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­




//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question4:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­



//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question5:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­


//--------------------------------------------------------------------------------------------------




//--------------------------------------------------------------------------------------------------
/*
question6:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œæœ‰ä¾‹å­




//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question7:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­




//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question8:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­



//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question10:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question11:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question12:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question13:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question14:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question15:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question16:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question17:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question18:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question19:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question20:


//--------------------------------------------------------------------------------------------------
