#include <cuda_runtime.h>

//method0
//error
/*
inline __device__ __host__ unsigned int cdiv(unsigned int a, unsigned int b) { return (a+b-1) / b;}

#define WARP_SIZE 32
#define THREADS_PER_BLOCK 256
#define STRIDE_FACTOR 8
#define BLOCK_SIZE STRIDE_FACTOR * THREADS_PER_BLOCK

__global__ void init_output(float* output) {
    *output = 0.0f;
}

__device__ void warp_reduce(volatile float* smem, unsigned int tid) {
    smem[tid] += smem[tid+32];
    smem[tid] += smem[tid+16];
    smem[tid] += smem[tid+8];
    smem[tid] += smem[tid+4];
    smem[tid] += smem[tid+2];
    smem[tid] += smem[tid+1];
}

// Make sure to delete the old __device__ void warp_reduce function, it's not needed.

__global__ void reduction_kernel(const float* input, float* output, int N) {
    // 1. Use 'double' for precision and declare as __shared__ to fix the crash
    __shared__ double smem[THREADS_PER_BLOCK];
    
    auto tid = threadIdx.x;
    auto block_start = blockIdx.x * BLOCK_SIZE;
    
    // 2. Use 'double' for the local sum
    double sum = 0.0;

    // 3. Load data and sum into the 'double' variable
    if (block_start+tid < N) {
        sum = (double)input[block_start + tid];
    }
    for (int i=1; i<STRIDE_FACTOR; ++i) {
        auto idx = block_start + i * THREADS_PER_BLOCK + tid;
        if (idx<N) {
            sum += (double)input[idx];
        }
    }
    
    // 4. Write the high-precision sum to shared memory
    smem[tid] = sum;
    __syncthreads();
    
    // 5. Use the single, safe, correct reduction loop.
    //    This loop has no race conditions and replaces 'warp_reduce'.
    for (int stride = THREADS_PER_BLOCK >> 1; stride > 0; stride >>= 1) {
        if (tid < stride) {
            smem[tid] += smem[tid + stride];
        }
        __syncthreads();
    }

    // 6. The final block sum is in smem[0]. Add it to the output.
    if (tid == 0) {
        atomicAdd(output, (float)smem[0]);
    }
}

// input, output are device pointers
extern "C" void solve(const float* input, float* output, int N) {  
    int threadsPerBlock = 256;
    int blocksPerGrid = (N + threadsPerBlock * 8 - 1) / (threadsPerBlock * 8);
    //cudaMemset(output, 0, sizeof(float));
    init_output<<<1, 1>>>(output);

    reduction_kernel<<<blocksPerGrid, threadsPerBlock>>>(input, output, N);

    cudaDeviceSynchronize();
}
//*/


//method1
/*
__global__ void reduce(const float* input, float* output, int N) {
    __shared__ float smem[256];
    
    int local_tid = threadIdx.x;

    float local_sum = 0.0;
    for (int f = 0; f < 8; f++) {
        int global_idx = local_tid + f * blockDim.x + blockIdx.x * 8 * blockDim.x;
        if (global_idx < N) {
            local_sum += input[global_idx];
        }
    }
    smem[local_tid] = local_sum;
    __syncthreads();

    #pragma unroll
    for (int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (local_tid < s) {
            smem[local_tid] += smem[local_tid + s];
        }
        __syncthreads();
    }
    
    if (local_tid == 0) {
        atomicAdd(output, smem[0]);
    }
}

// input, output are device pointers
extern "C" void solve(const float* input, float* output, int N) {  
    int threadsPerBlock = 256;
    int blocksPerGrid = (N + threadsPerBlock * 8 - 1) / (threadsPerBlock * 8);

    reduce<<<blocksPerGrid, threadsPerBlock>>>(input, output, N);
    cudaDeviceSynchronize();
}
*/

//method2
//wrong, never run to generate results
// ------------------------------------------------------------
//  CUDA sum-reduction (N up to 2^31-1, any float values)
//  - exact (no atomic-float rounding)
//  - works on every GPU (SM 3.0+)
//  - single kernel launch
// ------------------------------------------------------------

// ------------------------------------------------------------
//  CUDA sum-reduction â€“ exact, single-kernel, works on CPU/GPU
// ------------------------------------------------------------

// ------------------------------------------------------------
//  CUDA sum-reduction â€“ exact, single-kernel, works on CPU/GPU
// ------------------------------------------------------------

// ------------------------------------------------------------
//  CUDA sum-reduction â€“ exact, fast, no hang, N â‰¤ 2^31-1
// ------------------------------------------------------------
/*
#include <cuda_runtime.h>
#include <cstdint>
#include <cstring>

inline __device__ __host__ unsigned int cdiv(unsigned int a, unsigned int b) {
    return (a + b - 1) / b;
}

#define WARP_SIZE        32
#define THREADS_PER_BLOCK 256
#define STRIDE_FACTOR    8
#define BLOCK_SIZE       (STRIDE_FACTOR * THREADS_PER_BLOCK)  // 2048

// ---------- float <-> two 32-bit ints (device) ----------
__device__ __forceinline__ void float_to_bits(float f, uint32_t* hi, uint32_t* lo) {
    uint32_t u = __float_as_uint(f);
    *hi = u >> 16;
    *lo = (u << 16) | 0x8000u;
}

__device__ __forceinline__ float bits_to_float(uint32_t hi, uint32_t lo) {
    uint32_t u = (hi << 16) | (lo & 0xFFFFu);
    return __uint_as_float(u);
}

// ---------- host version ----------
static inline float bits_to_float_host(uint32_t hi, uint32_t lo) {
    uint32_t u = (hi << 16) | (lo & 0xFFFFu);
    float f;
    std::memcpy(&f, &u, sizeof(f));
    return f;
}

// ---------- warp reduction ----------
__device__ __forceinline__ float warp_reduce_sum(float val) {
    for (int offset = 16; offset > 0; offset >>= 1)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

// ---------- kernel ----------
__global__ void reduction_kernel(const float* input,
                                 uint32_t* out_hi,
                                 uint32_t* out_lo,
                                 int N)
{
    __shared__ float smem[WARP_SIZE];

    int tid  = threadIdx.x;
    int lane = tid & (WARP_SIZE - 1);
    int warp = tid / WARP_SIZE;
    int block_start = blockIdx.x * BLOCK_SIZE;

    float sum = 0.0f;

    // Load STRIDE_FACTOR elements per thread
    #pragma unroll
    for (int i = 0; i < STRIDE_FACTOR; ++i) {
        int idx = block_start + i * THREADS_PER_BLOCK + tid;
        if (idx < N) sum += input[idx];
    }

    sum = warp_reduce_sum(sum);

    if (lane == 0) smem[warp] = sum;
    __syncthreads();

    if (warp == 0) {
        float warp_sum = (lane < (THREADS_PER_BLOCK / WARP_SIZE)) ? smem[lane] : 0.0f;
        warp_sum = warp_reduce_sum(warp_sum);
        if (lane == 0) {
            uint32_t hi, lo;
            float_to_bits(warp_sum, &hi, &lo);
            atomicAdd(out_hi, hi);
            atomicAdd(out_lo, lo);
        }
    }
}

// ------------------------------------------------------------
//  Host wrapper â€“ FIXED grid size, no hang
// ------------------------------------------------------------
extern "C" void solve(const float* input, float* output, int N)
{
    uint32_t *d_hi = nullptr, *d_lo = nullptr;
    cudaMalloc(&d_hi, sizeof(uint32_t));
    cudaMalloc(&d_lo, sizeof(uint32_t));
    cudaMemset(d_hi, 0, sizeof(uint32_t));
    cudaMemset(d_lo, 0, sizeof(uint32_t));

    // FIXED: Use 64-bit math to avoid overflow
    unsigned long long elements_per_block = BLOCK_SIZE;
    unsigned long long num_blocks = (static_cast<unsigned long long>(N) + elements_per_block - 1) / elements_per_block;

    // Clamp to int (CUDA grid size is int)
    int blocks = (num_blocks > INT_MAX) ? INT_MAX : static_cast<int>(num_blocks);

    reduction_kernel<<<blocks, THREADS_PER_BLOCK>>>(input, d_hi, d_lo, N);

    cudaError_t err = cudaDeviceSynchronize();
    if (err != cudaSuccess) {
        // Optional: print error
        return;
    }

    uint32_t h_hi, h_lo;
    cudaMemcpy(&h_hi, d_hi, sizeof(uint32_t), cudaMemcpyDeviceToHost);
    cudaMemcpy(&h_lo, d_lo, sizeof(uint32_t), cudaMemcpyDeviceToHost);

    *output = bits_to_float_host(h_hi, h_lo);

    cudaFree(d_hi);
    cudaFree(d_lo);
}
*/


// ------------------------------------------------------------
//  CUDA sum-reduction â€“ exact, fast, no hang, N â‰¤ 2^31-1
// ------------------------------------------------------------
#include <cuda_runtime.h>
#include <cstdint>
#include <cstring>

inline __device__ __host__ unsigned int cdiv(unsigned int a, unsigned int b) {
    return (a + b - 1) / b;
}

#define WARP_SIZE        32
#define THREADS_PER_BLOCK 256
#define STRIDE_FACTOR    8
#define BLOCK_SIZE       (STRIDE_FACTOR * THREADS_PER_BLOCK)  // 2048

// ---------- float <-> two 32-bit ints (device) ----------
__device__ __forceinline__ void float_to_bits(float f, uint32_t* hi, uint32_t* lo) {
    uint32_t u = __float_as_uint(f);
    *hi = u >> 16;
    *lo = (u << 16) | 0x8000u;
}

__device__ __forceinline__ float bits_to_float(uint32_t hi, uint32_t lo) {
    uint32_t u = (hi << 16) | (lo & 0xFFFFu);
    return __uint_as_float(u);
}

// ---------- host version ----------
static inline float bits_to_float_host(uint32_t hi, uint32_t lo) {
    uint32_t u = (hi << 16) | (lo & 0xFFFFu);
    float f;
    std::memcpy(&f, &u, sizeof(f));
    return f;
}

// ---------- warp reduction ----------
__device__ __forceinline__ float warp_reduce_sum(float val) {
    for (int offset = 16; offset > 0; offset >>= 1)
        val += __shfl_down_sync(0xffffffff, val, offset);
    return val;
}

// ---------- kernel ----------
__global__ void reduction_kernel(const float* input,
                                 uint32_t* out_hi,
                                 uint32_t* out_lo,
                                 int N)
{
    __shared__ float smem[WARP_SIZE];

    int tid  = threadIdx.x;
    int lane = tid & (WARP_SIZE - 1);
    int warp = tid / WARP_SIZE;
    int block_start = blockIdx.x * BLOCK_SIZE;

    float sum = 0.0f;

    // Load STRIDE_FACTOR elements per thread
    #pragma unroll
    for (int i = 0; i < STRIDE_FACTOR; ++i) {
        int idx = block_start + i * THREADS_PER_BLOCK + tid;
        if (idx < N) sum += input[idx];
    }

    sum = warp_reduce_sum(sum);

    if (lane == 0) smem[warp] = sum;
    __syncthreads();

    if (warp == 0) {
        float warp_sum = (lane < (THREADS_PER_BLOCK / WARP_SIZE)) ? smem[lane] : 0.0f;
        warp_sum = warp_reduce_sum(warp_sum);
        if (lane == 0) {
            uint32_t hi, lo;
            float_to_bits(warp_sum, &hi, &lo);
            atomicAdd(out_hi, hi);
            atomicAdd(out_lo, lo);
        }
    }
}

// ------------------------------------------------------------
//  Host wrapper â€“ FIXED grid size, no hang
// ------------------------------------------------------------
extern "C" void solve(const float* input, float* output, int N)
{
    uint32_t *d_hi = nullptr, *d_lo = nullptr;
    cudaMalloc(&d_hi, sizeof(uint32_t));
    cudaMalloc(&d_lo, sizeof(uint32_t));
    cudaMemset(d_hi, 0, sizeof(uint32_t));
    cudaMemset(d_lo, 0, sizeof(uint32_t));

    // FIXED: Use 64-bit math to avoid overflow
    unsigned long long elements_per_block = BLOCK_SIZE;
    unsigned long long num_blocks = (static_cast<unsigned long long>(N) + elements_per_block - 1) / elements_per_block;

    // Clamp to int (CUDA grid size is int)
    int blocks = (num_blocks > INT_MAX) ? INT_MAX : static_cast<int>(num_blocks);

    reduction_kernel<<<blocks, THREADS_PER_BLOCK>>>(input, d_hi, d_lo, N);

    cudaError_t err = cudaDeviceSynchronize();
    if (err != cudaSuccess) {
        // Optional: print error
        return;
    }

    uint32_t h_hi, h_lo;
    cudaMemcpy(&h_hi, d_hi, sizeof(uint32_t), cudaMemcpyDeviceToHost);
    cudaMemcpy(&h_lo, d_lo, sizeof(uint32_t), cudaMemcpyDeviceToHost);

    *output = bits_to_float_host(h_hi, h_lo);

    cudaFree(d_hi);
    cudaFree(d_lo);
}

//--------------------------------------------------------------------------------------------------
/*
question0:
ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

æ‚¨å¥½ï¼æ‚¨æä¾›çš„ä»£ç æ˜¯ä¸€ä¸ªä½¿ç”¨ **CUDA C++** ç¼–å†™çš„ï¼Œç”¨äºåœ¨ **GPU ä¸Šå¹¶è¡Œè®¡ç®—ä¸€ä¸ªå¤§æ•°ç»„ä¸­æ‰€æœ‰æµ®ç‚¹æ•°ä¹‹å’Œï¼ˆå½’çº¦/Reductionï¼‰** çš„å®ç°ã€‚

ç”±äºæ‚¨æåˆ°äº†â€œä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­â€ï¼Œæˆ‘å°†åˆ†å—è§£é‡Šè¿™æ®µä»£ç çš„æ ¸å¿ƒæ¦‚å¿µå’Œå·¥ä½œæµç¨‹ã€‚

---

## æ ¸å¿ƒæ¦‚å¿µè§£é‡Š ğŸ’¡

è¿™æ®µä»£ç ç»“åˆäº†å‡ ä¸ªé‡è¦çš„ CUDA å’Œå¹¶è¡Œè®¡ç®—æŠ€æœ¯ï¼š

### 1. **å½’çº¦ (Reduction)**
*   **æ¦‚å¿µï¼š** å½’çº¦æ˜¯ä¸€ç§å¹¶è¡Œè®¡ç®—æ¨¡å¼ï¼Œå®ƒå°†ä¸€ä¸ªæ•°ç»„ï¼ˆæˆ–åˆ—è¡¨ï¼‰ä¸­çš„æ‰€æœ‰å…ƒç´ ç»„åˆæˆä¸€ä¸ªå•ä¸€çš„ç»“æœã€‚æœ€å¸¸è§çš„ä¾‹å­å°±æ˜¯æ±‚å’Œã€æ±‚æœ€å¤§å€¼æˆ–æœ€å°å€¼ã€‚
*   **ç›®æ ‡ï¼š** åœ¨è¿™æ®µä»£ç ä¸­ï¼Œç›®æ ‡æ˜¯é«˜æ•ˆåœ°è®¡ç®—è¾“å…¥æ•°ç»„ `input` ä¸­æ‰€æœ‰ `float` å…ƒç´ çš„æ€»å’Œã€‚

### 2. **å†…æ ¸ (Kernel)**
*   **æ¦‚å¿µï¼š** `__global__` å‡½æ•°ï¼ˆå¦‚ `reduction_kernel`ï¼‰æ˜¯ CUDA çš„æ ¸å¿ƒã€‚å®ƒæ˜¯åœ¨ **GPU ä¸Šå¹¶è¡Œæ‰§è¡Œ** çš„ä»£ç ã€‚
*   **æ‰§è¡Œæ–¹å¼ï¼š** å½“ä¸»æœºä»£ç ï¼ˆ`solve` å‡½æ•°ï¼‰è°ƒç”¨å†…æ ¸æ—¶ï¼Œå®ƒä¼šå¯åŠ¨ä¸€ä¸ªç”±è®¸å¤š **çº¿ç¨‹å— (Blocks)** ç»„æˆçš„ **ç½‘æ ¼ (Grid)**ï¼Œæ¯ä¸ªçº¿ç¨‹å—åˆåŒ…å«è®¸å¤š **çº¿ç¨‹ (Threads)**ã€‚

### 3. **çº¿ç¨‹å±‚æ¬¡ç»“æ„**
CUDA çº¿ç¨‹è¢«ç»„ç»‡æˆä¸€ä¸ªå±‚æ¬¡ç»“æ„ï¼š
*   **çº¿ç¨‹ (Thread)ï¼š** æ‰§è¡Œå†…æ ¸çš„åŸºæœ¬å•ä½ã€‚
*   **çº¿ç¨‹å— (Block)ï¼š** ä¸€ç»„çº¿ç¨‹ï¼ˆè¿™é‡Œæ˜¯ `THREADS_PER_BLOCK = 256` ä¸ªï¼‰ã€‚å—å†…çš„çº¿ç¨‹å¯ä»¥é€šè¿‡ **å…±äº«å†…å­˜ (Shared Memory)** å’Œ **åŒæ­¥ (`__syncthreads()`)** è¿›è¡Œåä½œã€‚
*   **ç½‘æ ¼ (Grid)ï¼š** ä¸€ç»„çº¿ç¨‹å—ã€‚ç½‘æ ¼ä¸­çš„å—æ˜¯ç‹¬ç«‹æ‰§è¡Œçš„ã€‚

### 4. **Warp**
*   **æ¦‚å¿µï¼š** CUDA ç¡¬ä»¶çš„åŸºæœ¬è°ƒåº¦å•ä½æ˜¯ **Warp**ï¼Œé€šå¸¸åŒ…å« **32 ä¸ªçº¿ç¨‹** (`WARP_SIZE = 32`)ã€‚åŒä¸€ä¸ª Warp å†…çš„çº¿ç¨‹æ˜¯ **åŒæ­¥æ‰§è¡Œ** ç›¸åŒçš„æŒ‡ä»¤çš„ã€‚

---

## ä»£ç åŠŸèƒ½å’Œå·¥ä½œæµç¨‹ç¤ºä¾‹ ğŸ› ï¸

è¿™æ®µä»£ç çš„å½’çº¦è¿‡ç¨‹åˆ†ä¸ºå‡ ä¸ªé˜¶æ®µï¼Œæ—¨åœ¨æœ€å¤§é™åº¦åœ°åˆ©ç”¨ GPU çš„å¹¶è¡Œæ€§ï¼š

### 1. **æ•°æ®åˆ†å‘å’Œåˆå§‹æ±‚å’Œ (Grid-Stride Loop)**

*   **ç›®æ ‡ï¼š** è®©æ¯ä¸ªçº¿ç¨‹å¤„ç†ä¸æ­¢ä¸€ä¸ªå…ƒç´ ï¼Œä»¥å‡å°‘å†…æ ¸å¯åŠ¨å¼€é”€å¹¶æé«˜å†…å­˜è®¿é—®æ•ˆç‡ã€‚
*   **å®ç°ï¼š**
    *   `BLOCK_SIZE` æ˜¯ 2048ã€‚
    *   `STRIDE_FACTOR` æ˜¯ 8ã€‚
    *   æ¯ä¸ªçº¿ç¨‹å—å¤„ç† 2048 ä¸ªå…ƒç´ ã€‚
    *   `#pragma unroll` å¾ªç¯ï¼šæ¯ä¸ªçº¿ç¨‹ä¼šè·³è·ƒå¼åœ°ï¼ˆGrid-Strideï¼‰è¯»å– STRIDE_FACTOR=8 ä¸ªå…ƒç´ å¹¶å°†å®ƒä»¬ç´¯åŠ åˆ°è‡ªå·±çš„å±€éƒ¨å˜é‡ `sum` ä¸­ã€‚

> **ä¾‹å­ï¼š**
> *   å‡è®¾è¾“å…¥æ•°ç»„ input æœ‰ N ä¸ªå…ƒç´ ã€‚
> *   å— 0 çš„çº¿ç¨‹ 0 (tid=0) ä¼šè¯»å– input[0], input[256], input[512], ldots, input[7 * 256]ã€‚
> *   å— 0 çš„çº¿ç¨‹ 1 (tid=1) ä¼šè¯»å– input[1], input[257], input[513], ldots, input[1 + 7 * 256]ã€‚
*   **ç»“æœï¼š** æ¯ä¸ªçº¿ç¨‹ç°åœ¨éƒ½æœ‰ä¸€ä¸ªå±€éƒ¨çš„ 8 ä¸ªå…ƒç´ çš„å’Œã€‚

### 2. **Warp å†…éƒ¨æ±‚å’Œ (`warp_reduce_sum`)**

*   **ç›®æ ‡ï¼š** åœ¨ä¸€ä¸ª Warp (32 ä¸ªçº¿ç¨‹) å†…éƒ¨å¿«é€Ÿæ±‚å’Œã€‚
*   **å®ç°ï¼š** ä½¿ç”¨ `__shfl_down_sync` å‡½æ•°ï¼ˆShuffle æœºåˆ¶ï¼‰ã€‚è¿™ä¸ªå‡½æ•°å…è®¸ Warp å†…çš„çº¿ç¨‹ç›´æ¥äº¤æ¢æ•°æ®ï¼Œ**é¿å…äº†å¯¹å…±äº«å†…å­˜çš„ä¾èµ–**ï¼Œé€Ÿåº¦æå¿«ã€‚
*   **è¿‡ç¨‹ï¼š** çº¿ç¨‹ i å°†è‡ªå·±çš„ `val` ä¼ é€’ç»™çº¿ç¨‹ i + offsetã€‚è¿™ä¸ªå¾ªç¯åœ¨ 16 to 8 to 4 to 2 to 1 æ­¥å†…å°† 32 ä¸ªå±€éƒ¨çš„å’Œç´¯åŠ åˆ° Warp çš„ç¬¬ä¸€ä¸ªçº¿ç¨‹ (`lane == 0`) ä¸­ã€‚

> **ç»“æœï¼š** Warp å†…çš„ç¬¬ä¸€ä¸ªçº¿ç¨‹ (`lane = 0`) å­˜å‚¨äº†è¯¥ Warp ä¸­æ‰€æœ‰çº¿ç¨‹çš„å±€éƒ¨å’Œçš„æ€»å’Œï¼ˆå³ 32 * 8 = 256 ä¸ªå…ƒç´ çš„æ€»å’Œï¼‰ã€‚

### 3. **å—å†…éƒ¨æ±‚å’Œ (Shared Memory)**

*   **ç›®æ ‡ï¼š** æ•´åˆæ‰€æœ‰ Warp çš„ç»“æœï¼Œå¾—åˆ°çº¿ç¨‹å—çš„æ€»å’Œã€‚
*   **å®ç°ï¼š**
    *   `if (lane == 0) smem[warp] = sum;`ï¼šæ¯ä¸ª Warp çš„ç¬¬ä¸€ä¸ªçº¿ç¨‹å°†å…¶ç»“æœå­˜å…¥ **å…±äº«å†…å­˜** (`smem`)ã€‚
    *   `__syncthreads();`ï¼šç¡®ä¿æ‰€æœ‰ Warp çš„ç»“æœéƒ½å·²å†™å…¥å…±äº«å†…å­˜ã€‚
    *   `if (warp == 0)`ï¼šå—å†…çš„ç¬¬ä¸€ä¸ª Warp è´Ÿè´£æ”¶é›†å…±äº«å†…å­˜ä¸­çš„æ‰€æœ‰ç»“æœã€‚
    *   å®ƒå†æ¬¡ä½¿ç”¨ `warp_reduce_sum` å°†å…±äº«å†…å­˜ä¸­çš„ 256 / 32 = 8 ä¸ªå€¼æ±‚å’Œã€‚

> **ç»“æœï¼š** å— 0 çš„çº¿ç¨‹ 0 (`tid=0`) ç°åœ¨å­˜å‚¨äº†è¯¥ **æ•´ä¸ªçº¿ç¨‹å—** å¤„ç†çš„æ‰€æœ‰å…ƒç´ ï¼ˆ2048 ä¸ªï¼‰çš„æ€»å’Œã€‚

### 4. **è·¨å—æœ€ç»ˆæ±‚å’Œ (Atomic Operations)**

*   **ç›®æ ‡ï¼š** å°†æ‰€æœ‰çº¿ç¨‹å—çš„ç»“æœå®‰å…¨åœ°ç´¯åŠ åˆ°æœ€ç»ˆçš„å…¨å±€æ€»å’Œä¸­ã€‚
*   **é—®é¢˜ï¼š** å¤šä¸ªçº¿ç¨‹å—çš„çº¿ç¨‹ 0 ä¼šåŒæ—¶å°è¯•å†™å…¥ä¸€ä¸ªå…¨å±€å˜é‡ï¼Œè¿™ä¼šå¯¼è‡´ **ç«äº‰æ¡ä»¶**ã€‚
*   **è§£å†³æ–¹æ¡ˆï¼š**
    1.  **æµ®ç‚¹æ•°æ‹†åˆ†ï¼š** æ ‡å‡†çš„ `atomicAdd` å¯¹ `float` æ€§èƒ½è¾ƒå·®ä¸”å®ç°å¤æ‚ã€‚è¿™æ®µä»£ç ä½¿ç”¨äº†å·§å¦™çš„æŠ€å·§ï¼šå°† `float` å€¼æ‹†åˆ†ä¸ºä¸¤ä¸ª `uint32_t`ï¼ˆé«˜ä½å’Œä½ä½ï¼‰ï¼Œç„¶åä½¿ç”¨ `atomicAdd` å¯¹è¿™ä¸¤ä¸ª `uint32_t` è¿›è¡Œç´¯åŠ ï¼ˆ`atomicAdd(out_hi, hi)` å’Œ `atomicAdd(out_lo, lo)`ï¼‰ã€‚
    2.  **åŸå­æ“ä½œï¼š** `atomicAdd` ç¡®ä¿æ¯æ¬¡åªæœ‰ä¸€ä¸ªçº¿ç¨‹å¯ä»¥æ›´æ–°å…¨å±€çš„ `out_hi` å’Œ `out_lo` è®¡æ•°å™¨ï¼Œä¿è¯äº†ç»“æœçš„æ­£ç¡®æ€§ã€‚

> **ç»“æœï¼š** å½“æ‰€æœ‰å—æ‰§è¡Œå®Œæ¯•åï¼Œ`d_hi` å’Œ `d_lo` ä¸­å­˜å‚¨äº†æ‰€æœ‰å…ƒç´ å’Œçš„æ‹†åˆ†ç»“æœã€‚

### 5. **ä¸»æœºç«¯é‡å»ºç»“æœ (`solve` å‡½æ•°)**

*   **ä¸»æœºå·¥ä½œï¼š**
    1.  ç­‰å¾… GPU å®Œæˆ (`cudaDeviceSynchronize`)ã€‚
    2.  å°† `d_hi` å’Œ `d_lo` çš„æœ€ç»ˆå€¼ä»è®¾å¤‡å†…å­˜å¤åˆ¶åˆ°ä¸»æœºå†…å­˜ã€‚
    3.  ä½¿ç”¨ `bits_to_float_host` å‡½æ•°å°†è¿™ä¸¤ä¸ª `uint32_t` é‡æ–°ç»„åˆæˆæœ€ç»ˆçš„ `float` æ€»å’Œï¼Œå¹¶å­˜å…¥ `output`ã€‚

---

## å…³é”®å‡½æ•°è§£é‡Š ğŸ”‘

| å‡½æ•°/å® | ç›®çš„ | è§£é‡Š |
| :--- | :--- | :--- |
| `cdiv(a, b)` | **å‘ä¸Šå–æ•´é™¤æ³•** | è®¡ç®—  lceil a/b rceil ã€‚ç”¨äºè®¡ç®—æ‰€éœ€çš„çº¿ç¨‹å—æ•°é‡ã€‚ |
| `__shfl_down_sync` | **Warp å†…éƒ¨é€šä¿¡** | åœ¨ Warp å†…å°†ä¸€ä¸ªå˜é‡ä»çº¿ç¨‹ i ä¼ é€’ç»™çº¿ç¨‹ i + offsetã€‚ |
| `__syncthreads()` | **å—å†…åŒæ­¥** | å¼ºåˆ¶å—å†…æ‰€æœ‰çº¿ç¨‹ç­‰å¾…ï¼Œç›´åˆ°æ‰€æœ‰çº¿ç¨‹éƒ½åˆ°è¾¾è¯¥ç‚¹ã€‚ |
| `atomicAdd()` | **åŸå­åŠ æ³•** | å®‰å…¨åœ°å°†ä¸€ä¸ªå€¼åŠ åˆ°å…¨å±€å†…å­˜ä¸­çš„å˜é‡ä¸Šï¼Œé˜²æ­¢ç«äº‰æ¡ä»¶ã€‚ |
| `float_to_bits()` / `bits_to_float()` | **æµ®ç‚¹æ•°è½¬æ•´æ•°** | å°† `float` æ‹†åˆ†ä¸ºä¸¤ä¸ª `uint32_t`ï¼Œç”¨äºåˆ©ç”¨ **æ•´æ•°åŸå­åŠ æ³•** ä¼˜åŒ–è·¨å—æ±‚å’Œã€‚ |
| `STRIDE_FACTOR` | **å·¥ä½œåˆ†é…** | å®šä¹‰æ¯ä¸ªçº¿ç¨‹åœ¨å¾ªç¯ä¸­è¦å¤„ç†çš„å…ƒç´ æ•°é‡ï¼Œç”¨äºæé«˜å†…å­˜ååé‡ã€‚ |

---

### æ€»ç»“

è¿™æ®µä»£ç å®ç°äº†ä¸€ä¸ª **é«˜æ€§èƒ½ã€å¤šé˜¶æ®µ** çš„ CUDA å½’çº¦ç®—æ³•ï¼š

1.  **æ¯ä¸ªçº¿ç¨‹** ç‹¬ç«‹å¤„ç† 8 ä¸ªå…ƒç´ ï¼ˆGrid-Strideï¼‰ã€‚
2.  **æ¯ä¸ª Warp** ä½¿ç”¨ Shuffle æ“ä½œé«˜æ•ˆåœ°å°† 32 ä¸ªç»“æœç›¸åŠ ã€‚
3.  **æ¯ä¸ªçº¿ç¨‹å—** ä½¿ç”¨å…±äº«å†…å­˜å°† 8 ä¸ª Warp çš„ç»“æœç›¸åŠ ã€‚
4.  **æ‰€æœ‰çº¿ç¨‹å—** ä½¿ç”¨æ•´æ•°åŸå­æ“ä½œå°† N / 2048 ä¸ªå—ç»“æœå®‰å…¨åœ°åˆå¹¶ä¸ºæœ€ç»ˆæ€»å’Œã€‚

è¿™ç§è®¾è®¡ç¡®ä¿äº†æœ€å¤§çš„å¹¶è¡Œåº¦å’Œæ•ˆç‡ã€‚

æ‚¨æƒ³äº†è§£æ›´å¤šå…³äº CUDA ç¼–ç¨‹ã€Warp Shuffle æˆ–æµ®ç‚¹æ•°æ‹†åˆ†çš„ç»†èŠ‚å—ï¼Ÿ


*/


//--------------------------------------------------------------------------------------------------
/*
question1:

please explain this code
// ---------- float <-> two 32-bit ints (device) ----------
__device__ __forceinline__ void float_to_bits(float f, uint32_t* hi, uint32_t* lo) 
    uint32_t u = __float_as_uint(f);
    *hi = u >> 16;
    *lo = (u << 16) | 0x8000u;


å¥½çš„ï¼Œè¿™æ®µä»£ç ç‰‡æ®µæ˜¯ CUDA C++ ä¸­ä¸€ä¸ªéå¸¸å·§å¦™çš„æŠ€å·§ï¼Œç”¨äºå°†ä¸€ä¸ª **32 ä½æµ®ç‚¹æ•° (`float`) æ‹†åˆ†æˆä¸¤ä¸ª 32 ä½æ— ç¬¦å·æ•´æ•° (`uint32_t`)**ã€‚

è¿™ä¸ªæ“ä½œçš„ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†èƒ½å¤Ÿåœ¨ GPU ä¸Šå¯¹æµ®ç‚¹æ•°è¿›è¡Œ **åŸå­æ“ä½œ (Atomic Operations)**ï¼Œç‰¹åˆ«æ˜¯ **åŸå­åŠ æ³• (`atomicAdd`)**ï¼ŒåŒæ—¶å°è¯•ä¼˜åŒ–æ€§èƒ½æˆ–é¿å…æµ®ç‚¹åŸå­æ“ä½œçš„æŸäº›é™åˆ¶ã€‚

-----

## ğŸ§ ä»£ç è§£é‡Š

```cpp
// ---------- float <-> two 32-bit ints (device) ----------
__device__ __forceinline__ void float_to_bits(float f, uint32_t* hi, uint32_t* lo) 
    uint32_t u = __float_as_uint(f);
    *hi = u >> 16;
    *lo = (u << 16) | 0x8000u;

```

### 1. å°† `float` è½¬æ¢ä¸º `uint32_t`

```cpp
uint32_t u = __float_as_uint(f);
```

  * **`__float_as_uint(f)`:** è¿™æ˜¯ä¸€ä¸ª CUDA **å†…å»ºå‡½æ•° (Intrinsic Function)**ã€‚å®ƒä¸ä¼šæ”¹å˜ `f` çš„å€¼ï¼Œè€Œæ˜¯å°† `float` **ä½æ¨¡å¼ (bit pattern)** è§£é‡Šä¸ºä¸€ä¸ª `uint32_t` æ•´æ•°ã€‚
  * **ç»“æœï¼š** å˜é‡ `u` ç°åœ¨åŒ…å«äº†æµ®ç‚¹æ•° `f` çš„ 32 ä½äºŒè¿›åˆ¶è¡¨ç¤ºã€‚

### 2. æ‹†åˆ†é«˜ 16 ä½

```cpp
*hi = u >> 16;
```

  * **`u >> 16`:** å°† 32 ä½çš„ `u` å‘å³ç§»åŠ¨ 16 ä½ã€‚
  * **ç»“æœï¼š** å˜é‡ `*hi` å­˜å‚¨äº†åŸæµ®ç‚¹æ•°ä½æ¨¡å¼çš„ **é«˜ 16 ä½**ã€‚

### 3. æ‹†åˆ†ä½ 16 ä½å¹¶æ·»åŠ æ ‡è®°

```cpp
*lo = (u << 16) | 0x8000u;
```

è¿™æ˜¯æœ€å¤æ‚çš„ä¸€æ­¥ï¼Œå®ƒåŒ…å«ä¸¤ä¸ªæ“ä½œï¼š

#### A. æå–ä½ 16 ä½åˆ°é«˜ä½

```cpp
(u << 16)
```

  * **`u << 16`:** å°† 32 ä½çš„ `u` å‘å·¦ç§»åŠ¨ 16 ä½ã€‚è¿™ä½¿å¾—åŸæµ®ç‚¹æ•°çš„ **ä½ 16 ä½** ç§»åŠ¨åˆ°äº† 32 ä½æ•´æ•°çš„ **é«˜ 16 ä½**ã€‚
  * **ç›®çš„ï¼š** ä¸ºäº†åœ¨åç»­è¿›è¡ŒåŸå­æ“ä½œæ—¶ï¼Œ**å°†ä½ 16 ä½ä¸å…¶ä»–æ•°æ®åŒºåˆ†å¼€**ã€‚

#### B. æ·»åŠ  0x8000u æ ‡è®°

```cpp
| 0x8000u
```

  * **`0x8000u`:** è¿™æ˜¯ä¸€ä¸ª 16 ä½æ•°å­—ï¼ŒäºŒè¿›åˆ¶è¡¨ç¤ºä¸º `1000 0000 0000 0000`ã€‚
  * **æŒ‰ä½æˆ– (`|`)ï¼š** å°†è¿™ä¸ªæ ‡è®°ä½è®¾ç½®åˆ° 32 ä½æ•´æ•°çš„ **ç¬¬ 15 ä½** (ä» 0 å¼€å§‹è®¡æ•°)ã€‚
  * **ç›®çš„ (æ¨æµ‹)ï¼š**
      * è¿™ä¸ª **æ ‡è®°ä½ (sentinel bit)** å¾ˆå¯èƒ½ç”¨äºåœ¨åç»­çš„ **æ•´æ•°åŸå­åŠ æ³•** ä¸­ **è·Ÿè¸ªæˆ–ä¼ é€’è¿›ä½**ã€‚
      * å½“ä¸¤ä¸ª 32 ä½æ•´æ•°ç›¸åŠ æ—¶ï¼Œå¦‚æœä½ 16 ä½æº¢å‡ºï¼Œè¿›ä½ä¼šä¼ æ’­åˆ°é«˜ 16 ä½ã€‚é€šè¿‡è¿™ä¸ªæ ‡è®°ï¼Œå¯ä»¥å¸®åŠ©åœ¨å¦ä¸€ä¸ªå‡½æ•°ï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªå¤æ‚çš„æµ®ç‚¹åŸå­åŠ æ³•å®ç°ï¼‰ä¸­æ£€æµ‹å’Œå¤„ç†æº¢å‡ºå’Œæµ®ç‚¹æ•°çš„æŒ‡æ•°/å°¾æ•°è®¡ç®—ã€‚

-----

## ğŸ’¡ ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¸ªæ“ä½œï¼Ÿ

### 1. æµ®ç‚¹åŸå­æ“ä½œçš„æŒ‘æˆ˜

åœ¨æ—§ç‰ˆæœ¬çš„ CUDA æˆ–ç‰¹å®šçš„ GPU æ¶æ„ä¸Šï¼Œå¯¹å…¨å±€å†…å­˜ä¸­çš„ `float` æ‰§è¡Œ `atomicAdd()` å¯èƒ½ä¼šæ¯”å¯¹ `int` æˆ– `uint32_t` æ‰§è¡ŒåŸå­æ“ä½œ **æ…¢å¾—å¤š**ï¼Œç”šè‡³åœ¨æŸäº›æƒ…å†µä¸‹æ ¹æœ¬ä¸å¯ç”¨æˆ–ç²¾åº¦å—é™ã€‚

### 2. **æ¨¡æ‹Ÿæµ®ç‚¹åŸå­åŠ æ³•**

è¿™ä¸ªæ‹†åˆ†æ“ä½œæ˜¯å®ç°é«˜æ€§èƒ½ **"æ¨¡æ‹Ÿæµ®ç‚¹åŸå­åŠ æ³•"** çš„ä¸€éƒ¨åˆ†ã€‚åŸºæœ¬çš„æ€è·¯æ˜¯ï¼š

1.  **çº¿ç¨‹ A** æƒ³è¦å°† f_A åŠ åˆ°ä¸€ä¸ªå…¨å±€å˜é‡ F ä¸Šã€‚
2.  å®ƒå°† f_A æ‹†åˆ†æˆ (hi_A, lo_A)ã€‚
3.  å®ƒå¯¹å…¨å±€çš„ F çš„ä½è¡¨ç¤ºæ‰§è¡Œ **æ•´æ•°åŸå­æ“ä½œ**ï¼Œä¾‹å¦‚ï¼š
      * `atomicAdd(F_hi, hi_A)`
      * `atomicAdd(F_lo, lo_A)`

é€šè¿‡å°†æµ®ç‚¹æ•°æ‹†åˆ†æˆä¸¤ä¸ªæ•´æ•°ï¼Œå®ƒå°±å¯ä»¥åˆ©ç”¨ GPU ä¸Š **é«˜åº¦ä¼˜åŒ–ä¸”å¿«é€Ÿ** çš„ **æ•´æ•°åŸå­æ“ä½œ**ã€‚

> **è¯·æ³¨æ„ï¼š** å®Œæ•´çš„æµ®ç‚¹æ•°åŸå­åŠ æ³•å®ç° **æ¯”ä»…ä»…å¯¹ hi å’Œ lo æ±‚å’Œå¤æ‚å¾—å¤š**ï¼Œå› ä¸ºå®ƒéœ€è¦å¤„ç† **æŒ‡æ•° (Exponent)** å’Œ **å°¾æ•° (Mantissa)** çš„å¯¹é½å’Œè¿›ä½ã€‚è¿™ä¸ª `float_to_bits` å‡½æ•°åªæ˜¯ **å‡†å¤‡æ•°æ®** çš„ç¬¬ä¸€æ­¥ã€‚

æ‚¨æä¾›çš„ä»£ç ä¸­ï¼Œæœ€ç»ˆæ±‚å’Œé˜¶æ®µä½¿ç”¨äº†ï¼š

```cpp
atomicAdd(out_hi, hi);
atomicAdd(out_lo, lo);
```

è¿™æ­£æ˜¯åˆ©ç”¨æ‹†åˆ†åçš„ 32 ä½æ•´æ•°æ‰§è¡ŒåŸå­æ“ä½œçš„ä½“ç°ã€‚

*/


//--------------------------------------------------------------------------------------------------
/*
question2:

è¯·ç”¨ä¾‹å­è¯´æ˜

å¥½çš„ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ¥è§£é‡Š `float_to_bits` å‡½æ•°æ˜¯å¦‚ä½•å°†æµ®ç‚¹æ•°æ‹†åˆ†æˆä¸¤ä¸ª 32 ä½æ•´æ•°çš„ã€‚

## ğŸ¯ æ‹†åˆ†è¿‡ç¨‹ç¤ºä¾‹

å‡è®¾æˆ‘ä»¬è¦æ‹†åˆ†çš„æµ®ç‚¹æ•°æ˜¯ f = 3.0ã€‚

### æ­¥éª¤ 1: `float` è½¬æ¢ä¸º `uint32_t`

é¦–å…ˆï¼Œæˆ‘ä»¬ä½¿ç”¨ CUDA å†…å»ºå‡½æ•° `__float_as_uint(f)` å°†æµ®ç‚¹æ•° 3.0 çš„ä½æ¨¡å¼è½¬æ¢ä¸ºä¸€ä¸ª 32 ä½æ— ç¬¦å·æ•´æ•° uã€‚

  * æµ®ç‚¹æ•° 3.0 çš„ **32 ä½ IEEE 754 æ ‡å‡†** ä½æ¨¡å¼æ˜¯ï¼š
    0 quad 10000000 quad 10000000000000000000000
      * ç¬¦å·ä½ (S): 0 (æ­£æ•°)
      * æŒ‡æ•°ä½ (E): 10000000 (128)
      * å°¾æ•°ä½ (M): 10000000000000000000000
  * è¿™ä¸ªäºŒè¿›åˆ¶ä½æ¨¡å¼å¯¹åº”çš„ 32 ä½æ•´æ•°ï¼ˆåå…­è¿›åˆ¶ï¼‰æ˜¯ï¼š
    u = 0x40400000

<!-- end list -->

```cpp
uint32_t u = __float_as_uint(3.0f); // u ç­‰äº 0x40400000
```

-----

### æ­¥éª¤ 2: æå–é«˜ 16 ä½ (`hi`)

æˆ‘ä»¬å¯¹ u è¿›è¡Œå³ç§» 16 ä½æ“ä½œæ¥è·å–é«˜ 16 ä½ã€‚

hi = u gg 16

0x40400000 gg 16 = 0x4040

  * **ç»“æœï¼š** `*hi` å­˜å‚¨äº† **`0x4040`**ã€‚

<!-- end list -->

```cpp
*hi = u >> 16; // *hi ç­‰äº 0x4040
```

-----

### æ­¥éª¤ 3: æå–ä½ 16 ä½å¹¶æ·»åŠ æ ‡è®° (`lo`)

è¿™æ˜¯æ‹†åˆ†çš„ç¬¬äºŒéƒ¨åˆ†ï¼Œä¹Ÿæ˜¯æœ€å…³é”®çš„éƒ¨åˆ†ï¼š

lo = (u ll 16) mid 0x8000u

#### A. å·¦ç§» 16 ä½

u ll 16

0x40400000 ll 16 = 0x00000000

  * ç”±äºåŸæ•°å­—çš„ä½ 16 ä½æ˜¯ 0000ï¼Œæ‰€ä»¥å·¦ç§» 16 ä½åï¼Œç»“æœæ˜¯ 0x00000000ã€‚

#### B. æ·»åŠ  0x8000u æ ‡è®°

lo = 0x00000000 mid 0x00008000

lo = 0x00008000

  * **ç»“æœï¼š** `*lo` å­˜å‚¨äº† **`0x8000`**ã€‚

<!-- end list -->

```cpp
*lo = (u << 16) | 0x8000u; // *lo ç­‰äº 0x8000
```

-----

## æœ€ç»ˆç»“æœæ€»ç»“ ğŸ“

å¯¹äºè¾“å…¥ f = 3.0ï¼ˆä½æ¨¡å¼ä¸º 0x40400000ï¼‰ï¼š

| å˜é‡ | å€¼ï¼ˆåå…­è¿›åˆ¶ï¼‰ | è§£é‡Š |
| :--- | :--- | :--- |
| **f** | N/A | 3.0f |
| **u** | `0x40400000` | åŸå§‹ 32 ä½æµ®ç‚¹ä½æ¨¡å¼ |
| **hi** | `0x4040` | åŸå§‹ä½æ¨¡å¼çš„ **é«˜ 16 ä½** |
| **lo** | `0x8000` | åŸå§‹ä½æ¨¡å¼çš„ **ä½ 16 ä½** (å…¨ 0) + æ ‡è®°ä½ `0x8000` |

è¿™ä¸¤ä¸ªæ•´æ•° (0x4040, 0x8000) éšåä¼šè¢«é€å…¥ **åŸå­åŠ æ³•** å‡½æ•° (`atomicAdd(out_hi, hi)` å’Œ `atomicAdd(out_lo, lo)`) ä¸­ï¼Œä»¥å®ç°è·¨çº¿ç¨‹å—çš„æœ€ç»ˆæµ®ç‚¹æ•°æ±‚å’Œã€‚

è¿™ä¸ªæ‹†åˆ†æŠ€å·§çš„æ ¸å¿ƒæ„ä¹‰åœ¨äºï¼Œå®ƒå°†ä¸€ä¸ª 32 ä½çš„æµ®ç‚¹æ•°æ“ä½œè½¬åŒ–ä¸ºå¯¹ä¸¤ä¸ª 32 ä½æ•´æ•°çš„åŸå­æ“ä½œï¼Œ**åˆ©ç”¨äº†æ•´æ•°åŸå­æ“ä½œçš„é«˜æ•ˆæ€§** æ¥æ„å»ºæ›´å¤æ‚çš„æµ®ç‚¹æ•°åŸå­æ“ä½œã€‚

*/


//--------------------------------------------------------------------------------------------------
/*
question3:

o0x8000åŸå§‹ä½æ¨¡å¼çš„ ä½ 16 ä½ (å…¨ 0) + æ ‡è®°ä½ 0x8000
è¿™ä¸ªæ ‡è®°ä½ï¼Œé¦–ä½1ï¼Œä¼šä¸ä¼šå½±å“atomicaddçš„ç»“æœï¼Ÿè¯·ä¸¾ä¾‹è¯´æ˜

è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„é—®é¢˜ï¼Œå®ƒè§¦åŠäº†è¿™æ®µ CUDA ä»£ç ä¸­æœ€ç²¾å¦™ä¹Ÿæœ€å®¹æ˜“è®©äººå›°æƒ‘çš„éƒ¨åˆ†ï¼š**å¦‚ä½•é€šè¿‡å¯¹æ‹†åˆ†çš„æ•´æ•°è¿›è¡ŒåŸå­åŠ æ³•æ¥æ¨¡æ‹Ÿæµ®ç‚¹æ•°çš„åŠ æ³•**ã€‚

-----

## æ ‡è®°ä½ 0x8000 å¯¹ `atomicAdd` çš„å½±å“

è¿™ä¸ªç‰¹æ®Šçš„æ ‡è®°ä½ 0x8000 **ç†è®ºä¸Šä¸ä¼šç›´æ¥å½±å“æœ€ç»ˆçš„æµ®ç‚¹æ•°æ±‚å’Œç»“æœï¼Œä½†å®ƒå¯¹æ•´ä¸ªæµ®ç‚¹æ•°åŸå­åŠ æ³•çš„å®ç°æ˜¯è‡³å…³é‡è¦çš„**ã€‚

è¦ç†è§£è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬éœ€è¦è·³å‡ºç®€å•çš„æ•´æ•°åŠ æ³•ï¼Œç†è§£è¿™ç§æ–¹æ³•çš„ç›®æ ‡æ˜¯ **æ¨¡æ‹Ÿ (Simulate)** æµ®ç‚¹æ•°åŠ æ³•ã€‚

### 1. æµ®ç‚¹æ•°åŠ æ³•çš„æœ¬è´¨

æµ®ç‚¹æ•° F å­˜å‚¨ä¸ºï¼š
F = (-1)^S * 1.M * 2^E - 127

è¿›è¡Œæµ®ç‚¹æ•°åŠ æ³• F_1 + F_2 æ—¶ï¼Œå¿…é¡»éµå¾ªä»¥ä¸‹å¤æ‚æ­¥éª¤ï¼š

1.  **æŒ‡æ•°å¯¹é½**ï¼šå°†è¾ƒå°çš„æŒ‡æ•°å¢å¤§åˆ°ä¸è¾ƒå¤§æŒ‡æ•°ç›¸åŒï¼ŒåŒæ—¶å³ç§»å°¾æ•°ã€‚
2.  **å°¾æ•°ç›¸åŠ **ï¼šå°†å¯¹é½åçš„å°¾æ•°ç›¸åŠ ã€‚
3.  **ç»“æœè§„èŒƒåŒ–**ï¼šè°ƒæ•´ç»“æœçš„æŒ‡æ•°å’Œå°¾æ•°ï¼Œä»¥æ»¡è¶³æ ‡å‡†å½¢å¼ï¼ˆå¦‚æœ‰æº¢å‡ºæˆ–ä¸‹æº¢ï¼‰ã€‚

### 2. 0x8000 çš„ä½œç”¨ï¼ˆæ¨æµ‹ï¼‰

æ‚¨æä¾›çš„ `float_to_bits` å‡½æ•°å¹¶ä¸æ˜¯ä¸€ä¸ªå®Œæ•´çš„æµ®ç‚¹æ•°åŸå­åŠ æ³•ï¼Œå®ƒåªæ˜¯ä¸€ä¸ª **æ•°æ®å‡†å¤‡** æ­¥éª¤ã€‚å®Œæ•´çš„æµ®ç‚¹åŸå­åŠ æ³•é€šå¸¸éœ€è¦ä¸€ä¸ªå¤æ‚çš„ã€ä½¿ç”¨æ•´æ•°åŸå­æ“ä½œæ„å»ºçš„ **å¾ªç¯æˆ–æ¯”è¾ƒ-äº¤æ¢ (Compare-and-Swap, CAS)** æœºåˆ¶ã€‚

åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ä¸­ï¼Œæ ‡è®°ä½ 0x8000 çš„ä½œç”¨ï¼Œå¾ˆå¯èƒ½æ˜¯ä¸ºäº† **è¾…åŠ©å†…éƒ¨çš„ CAS å¾ªç¯**ï¼š

  * **Lo å˜é‡çš„ç»“æ„ï¼š**
    Lo = (åŸå§‹å°¾æ•°ä½ 16 ä½) + 0x8000
    å…¶ä¸­ 0x8000 åœ¨ 32 ä½æ•´æ•°ä¸­å¤„äºç¬¬ 15 ä½ã€‚

  * **æ ‡è®°è¿›ä½/å€Ÿä½ï¼š** åœ¨ä¸€äº›å¤æ‚çš„æµ®ç‚¹æ•°åŠ æ³•å®ç°ä¸­ï¼Œ`atomicAdd` å¯èƒ½ä¼šè¢«ç”¨æ¥è¿›è¡Œ **æŒ‡æ•°å’Œå°¾æ•°çš„å·®å€¼è®¡ç®—**ã€‚å¦‚æœ 0x8000 ä½è¢«è®¾ç½®ï¼Œå®ƒå¯ä»¥ç”¨ä½œï¼š

      * **ä½ä½è¿›ä½åˆ°é«˜ä½** çš„ä¿¡å·ã€‚
      * **æ•°æ®å®Œæ•´æ€§** çš„æ£€æŸ¥ä½ã€‚
      * **CAS å¾ªç¯** ä¸­ç”¨æ¥åŒºåˆ†â€œè„æ•°æ®â€æˆ–â€œæ­£åœ¨è¢«å¤„ç†â€çš„æ ‡è®°ã€‚

-----

## ä¸¾ä¾‹è¯´æ˜ï¼šä¸ºä»€ä¹ˆåŸå§‹ä½ 16 ä½æ˜¯ 0

è®©æˆ‘ä»¬çœ‹æ‚¨çš„ä¾‹å­ä¸­ **ä½ 16 ä½ä¸ºä»€ä¹ˆæ˜¯ 0**ï¼Œè¿™æœ‰åŠ©äºç†è§£ `lo` å˜é‡çš„æ„æˆã€‚

### ç¤ºä¾‹è¾“å…¥ï¼šf = 3.0

  * **ä½æ¨¡å¼ u:** 0x40400000
    0100000000100000_é«˜  16  ä½ 0000000000000000_ä½  16  ä½

### æ‹†åˆ†æ­¥éª¤å›é¡¾

1.  **æå–åŸå§‹ä½ 16 ä½ï¼š**
    u ll 16 = 0x00000000
    (å› ä¸ºåŸå§‹ä½ 16 ä½å…¨æ˜¯ 0)

2.  **æ·»åŠ æ ‡è®°ä½ï¼š**
    lo = 0x00000000 mid 0x8000
    lo = 0x8000

### ä¸ºä»€ä¹ˆ 0x8000 ä¸å½±å“æœ€ç»ˆç»“æœï¼ˆå®è§‚å±‚é¢ï¼‰

å¦‚æœè¿™æ˜¯ä¸€ä¸ªæ­£ç¡®çš„æµ®ç‚¹æ•°åŸå­åŠ æ³•å®ç°ï¼Œæœ€ç»ˆçš„ç»“æœæ˜¯é€šè¿‡ **åå‘æ“ä½œ** è¿˜åŸçš„ï¼š

1.  æœ€ç»ˆçš„å…¨å±€ç´¯åŠ å™¨ F_hi^total å’Œ F_lo^total é€šè¿‡ **æ•´æ•° `atomicAdd`** è·å¾—ã€‚
2.  ä¸»æœºç«¯å‡½æ•° `bits_to_float_host` å°†å®ƒä»¬é‡æ–°ç»„åˆã€‚

<!-- end list -->

```cpp
// host/device: bits_to_float
uint32_t u_total = (hi_total << 16) | (lo_total & 0xFFFFu);
```

è¯·æ³¨æ„ **`bits_to_float`** å‡½æ•°çš„æœ€åä¸€æ­¥ï¼š

  * å®ƒåªå– `lo_total` çš„ **ä½ 16 ä½** (`lo_total & 0xFFFFu`)ã€‚

lo_total & 0xFFFFu

  * 0x8000 æ˜¯ä¸€ä¸ª 16 ä½æ•°å­—ï¼Œå…¶ 32 ä½è¡¨ç¤ºæ˜¯ 0x00008000ã€‚
  * å¯¹å…¶è¿›è¡Œ **æŒ‰ä½ä¸** `& 0xFFFFu` åï¼Œ**ç»“æœæ˜¯ 0x0000**ã€‚

0x00008000 & 0x0000FFFF = 0x00000000

å› æ­¤ï¼Œ**æ ‡è®°ä½ 0x8000 åœ¨æœ€ç»ˆé‡å»ºæµ®ç‚¹æ•°çš„æ­¥éª¤ä¸­ä¼šè¢«æ•…æ„æ¸…é™¤æ‰**ï¼Œå®ƒåœ¨ä¸»æœºç«¯ä¸å‚ä¸åŸå§‹ 32 ä½æµ®ç‚¹æ•°ä½æ¨¡å¼çš„æ„æˆï¼Œä»è€Œä¸å½±å“æœ€ç»ˆçš„æµ®ç‚¹æ•°å€¼ã€‚å®ƒçš„ä½œç”¨ä»…ä»…åœ¨ **è®¾å¤‡ç«¯çš„åŸå­åŠ æ³•æ‰§è¡Œè¿‡ç¨‹** ä¸­ã€‚
*/


//--------------------------------------------------------------------------------------------------
/*
question4:


*/


//--------------------------------------------------------------------------------------------------
/*
question5:


*/
