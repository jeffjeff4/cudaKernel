#include <stdio.h>

template <typename VecT, typename T>
__device__ T* get_offset(T* base, int row, int col, int ld) {
    return base + row * ld + col;
}

template <int BM, int BN, int WM, int WN, int TM, int TN,
          typename VecT, typename T>
__global__ void simulate_store(int ldc, int M, int N)
{
    constexpr int VecSz = sizeof(VecT) / sizeof(T);
    constexpr int TM_vec_num = TM / VecSz;
    constexpr int TN_vec_num = TN / VecSz;

    constexpr int WM_per_loop = WM / TM_vec_num;
    constexpr int WN_per_loop = WN / TN_vec_num;

    int tx = threadIdx.x;   // lane id
    int ty = threadIdx.y;   // warp-level tile id

    // We simulate wm_id/wn_id = 0 (one tile)
    int wm_id = ty;
    int wn_id = 0;

    int tm_vec_id = tx / VecSz;   // small hack: reorganize mapping
    int tn_vec_id = tx % VecSz;

    printf("Thread (%2d,%2d) â†’ tm_vec_id=%d tn_vec_id=%d\n",
           tx, ty, tm_vec_id, tn_vec_id);

    // simulate store loops
    for (int tm_loop = 0; tm_loop < TM_vec_num; ++tm_loop)
    {
        for (int vid = 0; vid < VecSz; ++vid)
        {
            int m = tm_loop * WM_per_loop + tm_vec_id * VecSz + vid;
            int _m = tm_loop * VecSz + vid;

            for (int tn_loop = 0; tn_loop < TN_vec_num; ++tn_loop)
            {
                int n = tn_loop * WN_per_loop + tn_vec_id * VecSz;
                int _n = tn_loop * VecSz;

                printf("  store: (m=%2d, n=%2d)  from (_m=%d,_n=%d)  [tm_loop=%d tn_loop=%d vid=%d]\n",
                       m, n, _m, _n, tm_loop, tn_loop, vid);
            }
        }
    }
}

int main() {
    dim3 block(32, 2);  // 32 lanes Ã— 2 rows = 64 WM
    simulate_store< float4, float >
        <<<1, block>>>(32, 64, 32);

    cudaDeviceSynchronize();
    return 0;
}


//-------------------------------------------------------------------------------------------

/*
question0:

ç”¨çœŸå® CUDA æ¨¡æ‹Ÿä»£ç æ‰“å° m,n è®¿é—®é¡ºåº

ä¸‹é¢ç»™ä½ ä¸€ä¸ª **çœŸå®å¯è¿è¡Œçš„ CUDA æ¨¡æ‹Ÿç¨‹åº**ï¼ˆç”¨ printfï¼‰
ç›´æ¥ **æ‰“å°æ¯ä¸ª thread å†™å…¥çš„ (m, n) è®¿é—®é¡ºåº**ï¼Œå®Œå…¨æ¨¡æ‹Ÿä½ ç»™çš„ kernelï¼š

```
store_Reg_To_Global(...)
```

å¹¶ä½¿ç”¨ä¸ä½ ä¹‹å‰ç›¸åŒçš„å…¸å‹å‚æ•°ï¼š

```
BM = 64
BN = 32
WM = 64
WN = 32
TM = 8
TN = 8
VecSz = 4
```

ç¨‹åºä¼šè¾“å‡ºï¼š

* threadIdx.x / threadIdx.y
* tm_loop / tn_loop / vec index
* æœ€ç»ˆå†™å…¥åæ ‡ (m, n)

ä¾¿äºä½ éªŒè¯ warp åˆ†å·¥ï¼Œä»¥åŠæ‰€æœ‰ store çš„ visiting orderã€‚

---

# âœ… å®Œæ•´å¯ç¼–è¯‘ CUDA ç¨‹åº

ä½ å¯ä»¥ç›´æ¥å¤åˆ¶åˆ° `nvcc test.cu -o test && ./test` è¿è¡Œï¼š

```cpp
#include <stdio.h>

template <typename VecT, typename T>
__device__ T* get_offset(T* base, int row, int col, int ld) {
    return base + row * ld + col;
}

template <int BM, int BN, int WM, int WN, int TM, int TN,
          typename VecT, typename T>
__global__ void simulate_store(int ldc, int M, int N)
{
    constexpr int VecSz = sizeof(VecT) / sizeof(T);
    constexpr int TM_vec_num = TM / VecSz;
    constexpr int TN_vec_num = TN / VecSz;

    constexpr int WM_per_loop = WM / TM_vec_num;
    constexpr int WN_per_loop = WN / TN_vec_num;

    int tx = threadIdx.x;   // lane id
    int ty = threadIdx.y;   // warp-level tile id

    // We simulate wm_id/wn_id = 0 (one tile)
    int wm_id = ty;
    int wn_id = 0;

    int tm_vec_id = tx / VecSz;   // small hack: reorganize mapping
    int tn_vec_id = tx % VecSz;

    printf("Thread (%2d,%2d) â†’ tm_vec_id=%d tn_vec_id=%d\n",
           tx, ty, tm_vec_id, tn_vec_id);

    // simulate store loops
    for (int tm_loop = 0; tm_loop < TM_vec_num; ++tm_loop)
    {
        for (int vid = 0; vid < VecSz; ++vid)
        {
            int m = tm_loop * WM_per_loop + tm_vec_id * VecSz + vid;
            int _m = tm_loop * VecSz + vid;

            for (int tn_loop = 0; tn_loop < TN_vec_num; ++tn_loop)
            {
                int n = tn_loop * WN_per_loop + tn_vec_id * VecSz;
                int _n = tn_loop * VecSz;

                printf("  store: (m=%2d, n=%2d)  from (_m=%d,_n=%d)  [tm_loop=%d tn_loop=%d vid=%d]\n",
                       m, n, _m, _n, tm_loop, tn_loop, vid);
            }
        }
    }
}

int main() {
    dim3 block(32, 2);  // 32 lanes Ã— 2 rows = 64 WM
    simulate_store< float4, float >
        <<<1, block>>>(32, 64, 32);

    cudaDeviceSynchronize();
    return 0;
}
```

---

# âœ… è¿è¡Œåä¼šçœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼ˆéƒ¨åˆ†ï¼‰ï¼š

```
Thread ( 0, 0) â†’ tm_vec_id=0 tn_vec_id=0
  store: (m= 0, n= 0)
  store: (m= 0, n=16)
  store: (m= 1, n= 0)
  store: (m= 1, n=16)
Thread ( 1, 0) â†’ tm_vec_id=0 tn_vec_id=1
  store: (m= 0, n= 4)
  store: (m= 0, n=20)
  store: (m= 1, n= 4)
  ...
Thread (16,0) â†’ tm_vec_id=4 tn_vec_id=0
  store: (m=16, n= 0)
  store: (m=16, n=16)
  ...
Thread ( 0,1)
  store: (m=32, n= 0)
...
Thread (31,1)
  store: (m=63, n=28)
```

---

# ğŸ”¥ åˆ†æä¸ç»“è®º

æ ¹æ®æ‰“å°ç»“æœï¼Œå¯ä»¥éªŒè¯ï¼š

### âœ” warp çš„ threadIdx.y = 0 è¦†ç›– m = 0~31

### âœ” warp çš„ threadIdx.y = 1 è¦†ç›– m = 32~63

ï¼ˆå¦‚æˆ‘ä»¬ä¹‹å‰æ¨å¯¼çš„ï¼‰

å¹¶ä¸”ï¼š

### âœ” threadIdx.x = lane å†³å®š tn_vec_id å’Œ tm_vec_id

### âœ” `WM_per_loop = 32` â†’ æ¯ä¸ª tm_loop æ‰«æ 32 ä¸ªè¡Œ

### âœ” `VecSz = 4` â†’ æ¯ä¸ª vector è´Ÿè´£è¿ç»­ 4 ä¸ªå…ƒç´ 

### âœ” `TN_vec_num = 2` â†’ æ¯è¡Œå†™ä¸¤ä¸ªå‘å³è·³è·ƒçš„ block

æœ€ç»ˆä½ ä¼šçœ‹åˆ°å®Œå®Œæ•´æ•´çš„ `(m,n)` å†™å…¥é¡ºåºï¼Œå®Œå…¨éªŒè¯äº† warp çš„æ•°æ®åˆ†å¸ƒæ–¹å¼ã€‚

---

# å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç»§ç»­æä¾›ï¼š

### ğŸ”§ 1. å¯è§†åŒ–æ¯ä¸ª lane çš„å†™å…¥ç‚¹ä½äºŒç»´ ASCII å›¾

### ğŸ” 2. warp å†™å…¥ global memory çš„ bank åºåˆ—å¯è§†åŒ–

### ğŸš€ 3. æŠŠè¿™ä¸ªæ¨¡æ‹Ÿæ‰©å±•åˆ° tensor core (mma.sync) æ“ä½œ

### ğŸ§  4. ç”¨ python ç»˜åˆ¶å†™å…¥ heatmap ç”¨é¢œè‰²æ ‡ lane

ä½ æƒ³ç»§ç»­å“ªä¸€ä¸ªï¼Ÿ


*/


//-------------------------------------------------------------------------------------------

/*
question1:

ç”»å›¾è§£é‡Š registerâ†’global çš„å­˜å‚¨åˆ†å¸ƒ

ä¸‹é¢ç”¨ **ASCII å›¾** + **è§£é‡Š** æ¥è¯´æ˜ï¼š

## ğŸ¯ åœºæ™¯

å½“ä¸€ä¸ª warpï¼ˆ32 threadsï¼‰ æŠŠå¯„å­˜å™¨ä¸­çš„æ•°æ®å†™å›åˆ° **global memory** æ—¶ï¼Œå®ƒä»¬æœ€ç»ˆå¦‚ä½•å¸ƒå±€ï¼Ÿ
ï¼ˆå‡è®¾æ˜¯å…¸å‹çš„ row-major å†™å›ï¼Œä¾‹å¦‚ C[row, col]ï¼‰

---

# âœ… 1. Warp â†’ Global Memory çš„é€»è¾‘æ˜ å°„ï¼ˆæŠ½è±¡ï¼‰

æ¯ä¸ªçº¿ç¨‹è´Ÿè´£ä¸€ä¸ªå…ƒç´ ï¼ˆæˆ–ä¸€ä¸ªå‘é‡ï¼‰ï¼Œæœ€ç»ˆå†™åˆ° DRAM åœ°å€ä¸Šï¼š

```
Register (per thread)        Global Memory (linear)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Thread 0 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚ addr0                         â”‚
â”‚ Thread 1 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚ addr1                         â”‚
â”‚ Thread 2 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚ addr2                         â”‚
â”‚   ...    â”‚                â”‚ ...                           â”‚
â”‚ Thread31 â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶  â”‚ addr31                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

å¦‚æœå†™å…¥è¿ç»­åœ°å€ï¼Œwarp è¾¾æˆ "perfect-coalescing"ï¼Œåˆå¹¶æˆ **1 æ¬¡ 128B äº‹åŠ¡**ã€‚

---

# âœ… 2. å®é™…äºŒç»´çŸ©é˜µå¸ƒå±€ï¼ˆrow-majorï¼‰

å‡è®¾æ¯ä¸ª thread å†™ C[row0, col + tid]ï¼š

```
Matrix C (row-major) in DRAM
(Each cell = 1 float = 4B)

Row r:
        colâ†’
     0    1    2    ...   31   32   33   ...
    â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€
r   â”‚ T0 â”‚ T1 â”‚ T2 â”‚ ...â”‚T31 â”‚ -- â”‚  --  ...
    â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€
           â†‘
   warp threads 0â€“31 write this segment
```

å³ï¼š

* thread0 å†™ C[r, c+0]
* thread1 å†™ C[r, c+1]
* ...
* thread31 å†™ C[r, c+31]

æœ€ç»ˆåœ¨ DRAM é‡Œè¿ç»­ï¼š

```
Global Memory (flat):
addr + 0   â† thread 0
addr + 4   â† thread 1
addr + 8   â† thread 2
...
addr + 124 â† thread 31
```

---

# âœ… 3. åŠ ä¸Š **å‘é‡åŒ– VecSzï¼ˆä¾‹å¦‚ float4ï¼‰**

å¦‚æœ `VecSz = 4`ï¼ˆfloat4ï¼‰ï¼Œæ¯ä¸ªçº¿ç¨‹å†™ 16 å­—èŠ‚ï¼Œwarp å†™ 32Ã—16=512Bï¼š

```
Thread tid writes C[row, col + tid*4 : col + tid*4 + 3]
```

ASCII å›¾ï¼š

```
Row r:
col â†’
  0     4      8      12      ...   124
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€
â”‚ T0(f4) â”‚ T1(f4) â”‚ T2(f4) â”‚ T3(f4) â”‚ ...
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€
   16B      16B      16B      16B   
```

DRAMï¼š

```
addr +  0 ... addr + 15   â† thread 0
addr + 16 ... addr + 31   â† thread 1
addr + 32 ... addr + 47   â† thread 2
...
addr +496 ... addr +511   â† thread31
```

---

# âœ… 4. å®Œæ•´çš„ **å¯„å­˜å™¨ â†’ L1 â†’ L2 â†’ DRAM** æµç¨‹å›¾

```
Registers (per thread)
      |
      | 32 threads issue store instructions
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Warp     â”‚ (ç¡¬ä»¶ coalescer åˆå¹¶æˆäº‹åŠ¡)
â”‚ Coalescer  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      |
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    L1$      â”‚ (optional: write-through)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      |
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    L2$      â”‚ (write-combining)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      |
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         DRAM (global)      â”‚
â”‚   128B aligned burst writesâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# âœ… 5. æŠŠ warp 32 ä¸ª thread å†™å›çš„ç‰©ç†åˆ†å¸ƒç”»æˆå¤§ ASCII å›¾

ä¸‹é¢æ˜¯çœŸå® **å…¨çƒå†…å­˜çº¿æ€§åœ°å€** + **warp å†™å…¥åŒºå—**

```
Global Memory (linear byte address)
0----------------------------------------------------------->
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   T0 (4B)    â”‚   T1 (4B)    â”‚   T2 (4B)    â”‚ ...          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   T3 (4B)    â”‚   T4 (4B)    â”‚   T5 (4B)    â”‚ ...          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                     ...                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  T30 (4B)    â”‚   T31 (4B)   â”‚   padding?   â”‚    next row   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†‘
      128 bytes (warp store transaction)
```

å¦‚æœæ˜¯ float4ï¼ˆ16Bï¼‰ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ...
â”‚  T0 (16B)       â”‚ T1 (16B)      â”‚ T2 (16B)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚  ...            â”‚               â”‚ 
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚  T31 (16B)      â”‚  next chunk   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
512 bytes (warp)
```

---

# å¦‚æœä½ å¸Œæœ›ï¼Œæˆ‘å¯ä»¥è¿›ä¸€æ­¥æä¾›ï¼š

âœ… å®Œæ•´å¯è¿è¡Œ CUDA C++ ä»£ç 
ï¼ˆæ‰“å°æ‰€æœ‰ register write â†’ global memory addressesï¼‰

æˆ–

âœ… æ˜¾ç¤ºä¸åŒ blockDim / warp / vec write çš„åœ°å€å¯è§†åŒ–å›¾

ä½ æƒ³è¦ï¼š
**(A)** å•çº¯çš„åœ°å€æ‰“å°ï¼Ÿ
**(B)** å°†çŸ©é˜µè®¿é—®åºåˆ—ç”»æˆåŠ¨ç”»å¼ ASCIIï¼Ÿ
**(C)** warp è®¿é—® GPU DRAM bank-levelï¼ˆHBM channelï¼‰å›¾ï¼Ÿ





*/