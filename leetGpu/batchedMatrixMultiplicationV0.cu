#include <cuda_runtime.h>
#include <iostream>
#include <cstdio>



// CUDA error checking macro
#define CUDA_CHECK(call) \
    do { \
        cudaError_t error = call; \
        if (error != cudaSuccess) { \
            std::cerr << "CUDA error at " << __FILE__ << ":" << __LINE__ \
                     << " - " << cudaGetErrorString(error) << std::endl; \
            exit(1); \
        } \
    } while(0)

#define get_offset(T, ptr, row, col, ld) ((T*)((ptr) + (row) * (ld) + (col)))
#define local_tile(ptr, tileM, tileN, row, col, ld) ((ptr) + (row)*(tileM)*(ld) + (col)*(tileN))

template<int x>
__host__ __device__ static constexpr int get_log2x() {
    static_assert(x>0 and (x & (x-1)) == 0);
    int v = x;
    int res = 0;
    while (v>1) {
        v>>= 1;
        ++res;
    }
    return res;
}

__global__ void sgemm_Kernel_Naive(const float* A, const float* B, float* C, int M, int N, int K) {
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    int row = blockIdx.y * blockDim.y + threadIdx.y;

    int batch_id = blockIdx.z;
    A += batch_id * M * K;
    B += batch_id * N * K;
    C += batch_id * M * N;

    if (row<M and col<N) {
        float rC = 0.0f;
        for (int k=0; k<K; ++k) {
            rC += A[row*K + k] * B[k * N + col];
        }
        C[row * N + col] = rC;
    }
}

template <int TileM, int TileN, int cta_size, bool trans, typename vec_t, typename T>
__device__ __forceinline__ void load_Global_To_Shared(T* dst, const T* src, int ld_dst, int ld_src, int M, int N, int tid) {
    constexpr int vec_size = sizeof(vec_t) / sizeof(T);
    static_assert(TileN % vec_size == 0);
    static_assert(TileM * TileN / vec_size >= cta_size);
    constexpr int num_threads_per_N = TileN / vec_size;
    //constexpr int num_threads_per_M = cta_size / num_threads_per_N;
    constexpr int num_vec_elements = TileM * TileN  / vec_size;
    constexpr int num_loop = num_vec_elements / cta_size;
    static_assert(num_vec_elements % cta_size == 0);

    #pragma unroll
    for (int loopid=0, idx=tid; loopid<num_loop; ++loopid, idx+=cta_size) {
        int n = (idx & (num_threads_per_N-1)) * vec_size;
        int m = idx>>get_log2x<num_threads_per_N>();

        if (m>=M || n>=N) continue;

        if constexpr (!trans) {
            get_offset(vec_t, dst, m, n, ld_dst)[0] = get_offset(vec_t, src, m, n, ld_src)[0];
        } else {
            auto vec_v = get_offset(vec_t, src, m, n, ld_src)[0];

            #pragma unroll
            for (int vid=0; vid<vec_size; ++vid) {
                get_offset(T, dst, n+vid, m, ld_dst)[0] = reinterpret_cast<T*>(&vec_v)[vid];
            }
        }
    }
}



template <int BM, int BN, int BK, int WM, int WN, int WK, int TM, int TN, 
            typename vec_t, typename T>
__device__ __forceinline__ void load_Shared_To_Reg(T* tArA, T* tBrB, const T* sA, const T* sB, int wm_id, int wn_id, int wk_id, int tm_vec_id, int tn_vec_id) {
    // tArA: WK * TM, sA: BK * BM
    // tBrB: WK * TN, sB: BK * BN
    constexpr int vec_size = sizeof(vec_t) / sizeof(T);
    constexpr int TM_vec_num = TM / vec_size;
    constexpr int TN_vec_num = TN / vec_size;
    constexpr int WM_per_loop = WM / TM_vec_num;
    constexpr int WN_per_loop = WN / TN_vec_num;

    const auto* tAsA = local_tile(sA, WK, WM, wk_id, wm_id, BM);
    const auto* tBsB = local_tile(sB, WK, WN, wk_id, wn_id, BN);

    #pragma unroll
    for (int kid=0; kid<WK; ++kid) {
        //load A
        #pragma unroll
        for (int tm_loop=0; tm_loop<TM_vec_num; ++tm_loop) {
            int m = tm_loop * WM_per_loop + tm_vec_id * vec_size;
            int _m = tm_loop * vec_size;
            get_offset(vec_t, tArA, kid, _m, TM) [0] = get_offset(vec_t, tAsA, kid, m, BM) [0];
        }

        //load B
        #pragma unroll
        for (int tn_loop=0; tn_loop<TN_vec_num; ++tn_loop) {
            int n = tn_loop * WN_per_loop + tn_vec_id * vec_size;
            int _n = tn_loop * vec_size;
            get_offset(vec_t, tBrB, kid, _n, TN) [0] = get_offset(vec_t, tBsB, kid, n, BN) [0];
        }
    }
}


template <int WK, int TM, int TN, int TK, typename T>
__device__ __forceinline__ void mma(T* tCrC, const T* tArA, const T* tBrB)  {
    // static_assert(WK==4 and TM==8 and TN==8 and TK==1, "This MMA implementation is designed for WK=4, TM=8, TN=8, TK=1");
    // rA: WK * TM, rB: WK * TN, rC: TM * TN
    #pragma unroll
    for (int tk=0; tk<WK; tk+=TK) {
        #pragma unroll
        for (int k=0; k<TK; ++k) {
            int _k = tk + k;
            #pragma unroll
            for (int m=0; m<TM; ++m) {
                #pragma unroll
                for (int n=0; n<TN; ++n) {
                    tCrC[m * TN + n] += tArA[_k * TM + m] * tBrB[_k * TN + n];
                }
            }
        }
    }
}


template <int BM, int BN, int WM, int WN, int TM, int TN, typename VecT, typename T>
__device__ __forceinline__ void store_Reg_To_Global(T* tCgC, const T* tCrC, int ldc, int M, int N, int wm_id, int wn_id, int tm_vec_id, int tn_vec_id) {
    // tCgC: BM * BN, tCrC: TM * TN
    constexpr int VecSz = sizeof(VecT) / sizeof(T);
    constexpr int TM_vec_num = TM / VecSz;   // 8/4=2
    constexpr int TN_vec_num = TN / VecSz;
    constexpr int WM_per_loop = WM / TM_vec_num;  // 64/2=32
    constexpr int WN_per_loop = WN / TN_vec_num;  // 32/2=16

    auto* tCtCgC = local_tile(tCgC, WM, WN, wm_id, wn_id, ldc);

    int validM = M - wm_id * WM_per_loop;
    int validN = N - wn_id * WN_per_loop;
    //int tid = threadIdx.y * blockDim.x + threadIdx.x;

    // if (blockIdx.x == 1 && blockIdx.y == 0 && wm_id == 0 && wn_id == 0) {
    //     printf("store_reg_to_global: WM_per_loop=%d, WN_per_loop=%d, TM_vec_num=%d, TN_vec_num=%d\n", WM_per_loop, WN_per_loop, TM_vec_num, TN_vec_num);
    //     printf("wm_id=%d, wn_id=%d, tm_vec_id=%d, tn_vec_id=%d, M=%d, N=%d, validM=%d, validN=%d\n", wm_id, wn_id, tm_vec_id, tn_vec_id, M, N, validM, validN);
    // }

    #pragma unroll
    for (int tm_loop = 0; tm_loop < TM_vec_num; ++tm_loop) {
        #pragma unroll
        for (int vid = 0; vid < VecSz; ++vid) {
            int m = tm_loop * WM_per_loop + tm_vec_id * VecSz + vid;
            int _m = tm_loop * VecSz + vid;
            #pragma unroll
            for (int tn_loop = 0; tn_loop < TN_vec_num; ++tn_loop) {
                int n = tn_loop * WN_per_loop + tn_vec_id * VecSz;
                int _n = tn_loop * VecSz;
                if (m < validM && n < validN) {
                    // if (blockIdx.x == 1 && blockIdx.y == 0 && tid == 0) {
                    //     printf("store: tid=%d, validM=%d, validN=%d, wm_id=%d, wn_id=%d, m=%d, n=%d, _m=%d, _n=%d, tm_loop=%d, tn_loop=%d\n", tid, validM, validN, wm_id, wn_id, m, n, _m, _n, tm_loop, tn_loop);
                    // }
                    get_offset(VecT, tCtCgC, m, n, ldc)[0] = get_offset(VecT, tCrC, _m, _n, TN)[0];
                }
            }
        }
    }
}


template<typename T>
__device__ __forceinline__ void printTensor(const T* tensor, int rows, int cols, int ld) {
    for (int i=0; i<rows; ++i) {
        for (int j=0; j<cols; ++j) {
            printf("%.2f ,", tensor[i*ld+j]);        
        }
        printf("\n");
    }
}

template <int BM, int BN, int BK, int WM, int WN, int WK, int TM, int TN, int TK, 
            int cta_size, typename vec_t>
__global__ __launch_bounds__(cta_size)
void sgemm_Kernel_Universal_Pipeline_TT(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int N, int K) {
    //int tidx = threadIdx.x;
    //int tidy = threadIdx.y;
    int tid = threadIdx.y * blockDim.x + threadIdx.x;
    int batch_id = blockIdx.z;

    A += batch_id * M * K;
    B += batch_id * N * K;
    C += batch_id * M * N;

    extern __shared__ float smem[];
    float* sA[2] = {smem, smem+BK*BM};
    float* sB[2] = {smem+2*BK*BM, smem+2*BK*BM+BK*BN};

    int bmid = blockIdx.y;
    int bnid = blockIdx.x;

    int curr_buffer_id = 0;

    const int rest_m = M - bmid*BM;
    const int rest_n = N - bnid*BN;

    constexpr int vec_size = sizeof(vec_t) / sizeof(float);
    //constexpr int num_load_per_thread = (BM*BK/vec_size) / cta_size;
    //constexpr int num_elem_ld_per_row_A = BK / vec_size;
    //constexpr int num_elem_ld_per_row_B = BN / vec_size;

    auto* gA = A;
    auto* gB = B;
    auto* gC = C;
    const int lda = K;
    const int ldb = N;
    const int ldc = N;
    auto* tCgC = local_tile(gC, BM, BN, bmid, bnid, ldc);

    int bkid=0;
    auto* tAgA = local_tile(gA, BM, BK, bmid, bkid, lda);
    auto* tBgB = local_tile(gB, BK, BN, bkid, bnid, ldb);

    //warp level
    constexpr int NWarps_dim_N = (BN/WN);
    constexpr int NWarps_dim_M = (BM/WM);
    static_assert((NWarps_dim_N & (NWarps_dim_N-1)) == 0);
    static_assert((NWarps_dim_M & (NWarps_dim_M-1)) == 0);
    const int warp_id = tid>>5;
    const int lane_id = tid & 0x1F;
    static_assert(get_log2x<NWarps_dim_N>() == 2);
    const int wm_id = warp_id >> get_log2x<NWarps_dim_N>();
    const int wn_id = warp_id & (NWarps_dim_N-1);

    //thread level
    //constexpr int TM_vec_num = TM / vec_size;
    constexpr int TN_vec_num = TN / vec_size;
    //constexpr int WM_per_loop = WM /TM_vec_num;
    constexpr int WN_per_loop = WN /TN_vec_num;
    const int tm_vec_id = lane_id >> get_log2x<WN_per_loop/vec_size>();
    const int tn_vec_id = lane_id & (WN_per_loop/vec_size-1);

    //thread register
    float tArA[WK*TM];
    float tBrB[WK*TN];
    float tCrC[TM*TN] = {0.0f};

    //load
    load_Global_To_Shared<BM, BK, cta_size, true, vec_t>(sA[curr_buffer_id], tAgA, BM, lda, rest_m, (K - bkid*BK), tid);
    load_Global_To_Shared<BK, BN, cta_size, false, vec_t>(sB[curr_buffer_id], tBgB, BN, ldb, (K - bkid*BK), rest_n, tid);
    __syncthreads();

    //no unrolling
    for(; bkid<K/BK-1; ++bkid) {
        auto next_buffer_id = 1^curr_buffer_id;
        //load
        auto* tAgA = local_tile(gA, BM, BK, bmid, bkid+1, lda);
        auto* tBgB = local_tile(gB, BK, BN, bkid+1, bnid, ldb);
        load_Global_To_Shared<BM, BK, cta_size, true, vec_t>(sA[next_buffer_id], tAgA, BM, lda, rest_m, (K-bkid*BK), tid);
        load_Global_To_Shared<BK, BN, cta_size, false, vec_t>(sB[next_buffer_id], tBgB, BN, ldb, (K-bkid*BK), rest_n, tid);

        #pragma unroll
        for(int wk_id=0; wk_id<BK/WK; ++wk_id) {
            //load reg
            load_Shared_To_Reg<BM, BN, BK, WM, WN, WK, TM, TN, vec_t>(tArA, tBrB, sA[curr_buffer_id], sB[curr_buffer_id], wm_id, wn_id, wk_id, tm_vec_id, tn_vec_id);
            //mma
            mma<WK, TM, TN, TK>(tCrC, tArA, tBrB);
        }

        //barrier
        __syncthreads();

        //switch buffer
        curr_buffer_id ^= 1;
    }

    #pragma unroll
    for(int wk_id=0; wk_id<BK/WK; ++wk_id) {
        //load reg
        load_Shared_To_Reg<BM, BN, BK, WM, WN, WK, TM, TN, vec_t>(tArA, tBrB, sA[curr_buffer_id], sB[curr_buffer_id], wm_id, wn_id, wk_id, tm_vec_id, tn_vec_id);
        //mma
        mma<WK, TM, TN, TK>(tCrC, tArA, tBrB);
    }

    //store
    store_Reg_To_Global<BM, BN, WM, WN, TM, TN, vec_t>(tCgC, tCrC, ldc, rest_m, rest_n, wm_id, wn_id, tm_vec_id, tn_vec_id);
}


template <int BM, int BN, int BK, int WM, int WN, int WK, int TM, int TN, int TK, 
            int cta_size, int M, int N, int K, typename vec_t>
__global__ __launch_bounds__(cta_size)
void sgemm_Kernel_Universal_Pipeline_TT_Specialized(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C) {
    //int tidx = threadIdx.x;
    //int tidy = threadIdx.y;
    int tid = threadIdx.y * blockDim.x + threadIdx.x;
    int batch_id = blockIdx.z;

    A += batch_id * M * K;
    B += batch_id * N * K;
    C += batch_id * M * N;

    extern __shared__ float smem[];
    float* sA[2] = {smem, smem+BK*BM};
    float* sB[2] = {smem+2*BK*BM, smem+2*BK*BM+BK*BN};

    int bmid = blockIdx.y;
    int bnid = blockIdx.x;

    int curr_buffer_id = 0;

    const int rest_m = M - bmid*BM;
    const int rest_n = N - bnid*BN;

    constexpr int vec_size = sizeof(vec_t) / sizeof(float);
    //constexpr int num_load_per_thread = (BM*BK/vec_size) / cta_size;
    //constexpr int num_elem_ld_per_row_A = BK / vec_size;
    //constexpr int num_elem_ld_per_row_B = BN / vec_size;

    auto* gA = A;
    auto* gB = B;
    auto* gC = C;
    constexpr int lda = K;
    constexpr int ldb = N;
    constexpr int ldc = N;
    auto* tCgC = local_tile(gC, BM, BN, bmid, bnid, ldc);

    int bkid=0;
    auto* tAgA = local_tile(gA, BM, BK, bmid, bkid, lda);
    auto* tBgB = local_tile(gB, BK, BN, bkid, bnid, ldb);

    //warp level
    constexpr int NWarps_dim_N = (BN/WN);
    constexpr int NWarps_dim_M = (BM/WM);
    static_assert((NWarps_dim_N & (NWarps_dim_N-1)) == 0);
    static_assert((NWarps_dim_M & (NWarps_dim_M-1)) == 0);
    const int warp_id = tid>>5;
    const int lane_id = tid & 0x1F;
    static_assert(get_log2x<NWarps_dim_N>() == 2);
    const int wm_id = warp_id >> get_log2x<NWarps_dim_N>();
    const int wn_id = warp_id & (NWarps_dim_N-1);

    //thread level
    constexpr int TM_vec_num = TM / vec_size;
    constexpr int TN_vec_num = TN / vec_size;
    constexpr int WM_per_loop = WM /TM_vec_num;
    constexpr int WN_per_loop = WN /TN_vec_num;
    const int tm_vec_id = lane_id >> get_log2x<WM_per_loop/vec_size>();
    const int tn_vec_id = lane_id & (WN_per_loop/vec_size-1);

    //thread register
    float tArA[WK*TM];
    float tBrB[WK*TN];
    float tCrC[TM*TN] = {0.0f};

    //load
    load_Global_To_Shared<BM, BK, cta_size, true, vec_t>(sA[curr_buffer_id], tAgA, BM, lda, rest_m, (K - bkid*BK), tid);
    load_Global_To_Shared<BK, BN, cta_size, false, vec_t>(sB[curr_buffer_id], tBgB, BN, ldb, (K - bkid*BK), rest_n, tid);
    __syncthreads();

    //no unrolling
    for(; bkid<K/BK-1; ++bkid) {
        auto next_buffer_id = 1^curr_buffer_id;
        //load
        auto* tAgA = local_tile(gA, BM, BK, bmid, bkid+1, lda);
        auto* tBgB = local_tile(gB, BK, BN, bkid+1, bnid, ldb);
        load_Global_To_Shared<BM, BK, cta_size, true, vec_t>(sA[next_buffer_id], tAgA, BM, lda, rest_m, (K-bkid*BK), tid);
        load_Global_To_Shared<BK, BN, cta_size, false, vec_t>(sB[next_buffer_id], tBgB, BN, ldb, (K-bkid*BK), rest_n, tid);

        #pragma unroll
        for(int wk_id=0; wk_id<BK/WK; ++wk_id) {
            //load reg
            load_Shared_To_Reg<BM, BN, BK, WM, WN, WK, TM, TN, vec_t>(tArA, tBrB, sA[curr_buffer_id], sB[curr_buffer_id], wm_id, wn_id, wk_id, tm_vec_id, tn_vec_id);
            //mma
            mma<WK, TM, TN, TK>(tCrC, tArA, tBrB);
        }

        //barrier
        __syncthreads();

        //switch buffer
        curr_buffer_id ^= 1;
    }

    #pragma unroll
    for(int wk_id=0; wk_id<BK/WK; ++wk_id) {
        //load reg
        load_Shared_To_Reg<BM, BN, BK, WM, WN, WK, TM, TN, vec_t>(tArA, tBrB, sA[curr_buffer_id], sB[curr_buffer_id], wm_id, wn_id, wk_id, tm_vec_id, tn_vec_id);
        //mma
        mma<WK, TM, TN, TK>(tCrC, tArA, tBrB);
    }

    //store
    store_Reg_To_Global<BM, BN, WM, WN, TM, TN, vec_t>(tCgC, tCrC, ldc, rest_m, rest_n, wm_id, wn_id, tm_vec_id, tn_vec_id);
}


// A, B, C are device pointers
extern "C" void solve(const float* A, const float* B, float* C, int BATCH, int M, int N, int K) {
    auto launch_Naive = [&] () {
        dim3 threadsPerBlock(16, 16);
        dim3 blocksPerGrid((N+threadsPerBlock.x-1) / threadsPerBlock.x,
                           (M+threadsPerBlock.x-1) / threadsPerBlock.x,
                           BATCH);

        sgemm_Kernel_Naive<<<blocksPerGrid, threadsPerBlock>>>(A, B, C, M, N, K); 
    };
    constexpr int BM = 256;
    constexpr int BN = 128;
    constexpr int BK = 16;

    constexpr int WM = 64;
    constexpr int WN = 32;
    constexpr int WK = 8;

    constexpr int TM = 8;
    constexpr int TN = 8;
    constexpr int TK = 1;

    auto launch_Pipeline = [&] () {
        using vec_t = uint4;
        constexpr dim3 block_size(32, 16);
        constexpr int num_warps = (block_size.x * block_size.y) / 32;
        static_assert(num_warps == (BM/WM) * (BN/WN));

        const dim3 grid_size((N+BN-1)/BN, (M+BM-1)/BM, BATCH);
        constexpr int smem_size = 2*BK*(BM+BN) * sizeof(float);

        auto func = sgemm_Kernel_Universal_Pipeline_TT<BM, BN, BK, WM, WN, WK, TM, TN, TK, block_size.x * block_size.y, vec_t>;
        auto stream = cudaStream_t(0);
        auto func_attr = cudaFuncSetAttribute(func, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);
        CUDA_CHECK(func_attr);
        // printf("running sgemm_kernel_universal_pipeline_TT %d %d %d %d\n", gridSz.x, gridSz.y, blockSz.x, blockSz.y);
        func<<<grid_size, block_size, smem_size, stream>>>(A, B, C, M, N, K);
        //CUDA_CHECK(cudaDeviceSynchronize());
        CUDA_CHECK(cudaGetLastError());
        printf("end sgemm_Kernel_Universal_Pipeline_TT_Specialized\n");
    };

    auto launch_Pipeline_Specialized = [&] () {
        using vec_t = uint4;
        constexpr dim3 block_size(32, 16);
        constexpr int num_warps = (block_size.x * block_size.y) / 32;
        static_assert(num_warps == (BM/WM) * (BN/WN));

        const dim3 grid_size((N+BN-1)/BN, (M+BM-1)/BM, BATCH);
        constexpr int smem_size = 2*BK*(BM+BN) * sizeof(float);

        auto func = sgemm_Kernel_Universal_Pipeline_TT_Specialized<BM, BN, BK, WM, WN, WK, TM, TN, TK, block_size.x * block_size.y, 8192, 6144, 4096, vec_t>;
        auto stream = cudaStream_t(0);
        auto func_attr = cudaFuncSetAttribute(func, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);
        CUDA_CHECK(func_attr);
        // printf("running sgemm_kernel_universal_pipeline_TT %d %d %d %d\n", gridSz.x, gridSz.y, blockSz.x, blockSz.y);
        func<<<grid_size, block_size, smem_size, stream>>>(A, B, C);
        CUDA_CHECK(cudaDeviceSynchronize());
        CUDA_CHECK(cudaGetLastError());
        printf("end sgemm_Kernel_Universal_Pipeline_TT_Specialized\n");
    };

    auto is_aligned = [] (const float* ptr, int size) {
        return (uintptr_t(ptr) % size) == 0;
    };
    if (M==8192 and N==6144 and K==4096) {
        // use the specialized kernel for perf test
        launch_Pipeline_Specialized();
    } else if (is_aligned(A, 16) and is_aligned(B, 16) and is_aligned(C, 16) and 
                M%8==0 and N%8==0 and K%8==0 and 
                M>=BM and N>=BN and K%BK==0) {
        // use the optimized kernel
        //printf()"using the optimized kernel with M=%d, N=%d, K=%d\n", M, N, K);
        launch_Pipeline();
    } else {
        // if M, N, K % 8 !=0, can not use the optimized kernel
        //printf("using naive kernel due to M=%d, N=%d, K=%d not being nultiple of 8 or too small.\n", M, N, K);
        launch_Naive();
    }

} 



/*
question0:

please explain this code, using examples,



    auto launch_Pipeline_Specialized = [&] () { 

è¿™æ®µä»£ç æ˜¯ä¸€ä¸ª C++ Lambda è¡¨è¾¾å¼ï¼Œåä¸º `launch_Pipeline_Specialized`ï¼Œå®ƒè´Ÿè´£å¯åŠ¨ä¸€ä¸ª**é«˜åº¦ä¸“ä¸šåŒ–å’Œä¼˜åŒ–çš„ SGEMMï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰CUDA Kernel**ï¼Œä¸“ä¸ºä¸€ç»„ç¡¬ç¼–ç çš„ç‰¹å®šçŸ©é˜µå°ºå¯¸è®¾è®¡ã€‚

è¿™ç§åšæ³•åœ¨é«˜æ€§èƒ½è®¡ç®—ä¸­å¾ˆå¸¸è§ï¼šé€šç”¨ Kernel é€‚ç”¨äºå¤šç§å°ºå¯¸ï¼Œè€Œ**ä¸“ä¸š Kernel** åˆ™é’ˆå¯¹ç‰¹å®šå°ºå¯¸è¿›è¡Œæè‡´ä¼˜åŒ–ï¼Œä»¥è·å¾—æ›´é«˜çš„æ€§èƒ½ã€‚

-----

## ğŸ”¢ ç¤ºä¾‹å‚æ•°è®¾å®š

æˆ‘ä»¬ä½¿ç”¨ä»£ç ä¸­çš„å¸¸é‡æ¨¡æ¿å‚æ•°å’Œç¡¬ç¼–ç å°ºå¯¸ï¼š

| å‚æ•° | å€¼ | å«ä¹‰ |
| :--- | :--- | :--- |
| BM | 256 | Block M ç»´åº¦åˆ†å—å¤§å° |
| BN | 128 | Block N ç»´åº¦åˆ†å—å¤§å° |
| M | 8192 | **ç¡¬ç¼–ç **çš„çŸ©é˜µ A/C è¡Œæ•° |
| N | 6144 | **ç¡¬ç¼–ç **çš„çŸ©é˜µ B/C åˆ—æ•° |
| K | 4096 | **ç¡¬ç¼–ç **çš„çŸ©é˜µ A åˆ—æ•° / B è¡Œæ•° |
| block_size | \langle 32, 16 \rangle | çº¿ç¨‹å—å°ºå¯¸ï¼ˆ512 çº¿ç¨‹ï¼‰ |

## ğŸš€ I. çº¿ç¨‹å—å’Œå†…å­˜é…ç½®

### 1\. çº¿ç¨‹å—å’Œ Warp æ ¡éªŒ

```c
constexpr dim3 block_size(32, 16);
constexpr int num_warps = (block_size.x * block_size.y) / 32; // 512 / 32 = 16 Warps
static_assert(num_warps == (BM/WM) * (BN/WN)); // ç¡®ä¿ 16 == (256/64) * (128/32) = 4 * 4
```

  * **ç›®çš„:** è¿™äº›è¡ŒéªŒè¯äº†çº¿ç¨‹å—çš„æ€»çº¿ç¨‹æ•°å’Œ Warp æ•°é‡æ˜¯å¦ä¸é¢„å…ˆè®¾å®šçš„åˆ†å—å°ºå¯¸ (BM, BN, WM, WN) å®Œç¾åŒ¹é…ã€‚è¿™æ˜¯ä¼˜åŒ– Kernel æ­£ç¡®è¿è¡Œçš„å‰æã€‚

### 2\. åŠ¨æ€å…±äº«å†…å­˜å¤§å° (smem_size)

```c
constexpr int smem_size = 2*BK*(BM+BN) * sizeof(float);
```

  * **è®¡ç®—:** 2 * 16 * (256 + 128) * 4 å­—èŠ‚ã€‚
  * **å«ä¹‰:** è®¡ç®—åŒç¼“å†²æ‰€éœ€çš„ Shared Memory æ€»é‡ã€‚è¿™ä¸ªå€¼è¢«å£°æ˜ä¸º `constexpr`ï¼Œæ„å‘³ç€å®ƒåœ¨**ç¼–è¯‘æ—¶**å°±è¢«ç¡®å®šäº†ã€‚

### 3\. è®¾ç½®åŠ¨æ€å…±äº«å†…å­˜é™åˆ¶

```c
auto func_attr = cudaFuncSetAttribute(func, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);
CUDA_CHECK(func_attr);
```

  * **ç›®çš„:** ç”±äº Kernel ä½¿ç”¨ `extern __shared__`ï¼Œå¿…é¡»**æ˜¾å¼**åœ°å‘Šè¯‰ CUDA é©±åŠ¨ç¨‹åºè¯¥ Kernel åœ¨è¿è¡Œæ—¶éœ€è¦åˆ†é…å¤šå°‘åŠ¨æ€ Shared Memory (`smem_size`)ã€‚

-----

## ğŸ“¦ II. Kernel å®ä¾‹åŒ–å’Œå¯åŠ¨

### 1\. ä¸“ä¸šåŒ– Kernel å®ä¾‹åŒ– (Specialized Instantiation)

```c
auto func = sgemm_Kernel_Universal_Pipeline_TT_Specialized<..., 8192, 6144, 4096, vec_t>;
```

  * **å…³é”®ç‚¹:** ä¸é€šç”¨ Kernel ä¸åŒï¼Œè¿™ä¸ªå‡½æ•°æ¨¡æ¿åœ¨å®ä¾‹åŒ–æ—¶ï¼Œå°† M=8192, N=6144, K=4096 è¿™ä¸‰ä¸ª**çŸ©é˜µå°ºå¯¸**ä½œä¸º **æ¨¡æ¿å‚æ•°**ï¼ˆç¡¬ç¼–ç çš„å¸¸é‡ï¼‰ä¼ å…¥äº† Kernelã€‚
  * **ä¼˜åŒ–ç›Šå¤„:** è¿™æ ·ï¼ŒKernel å†…éƒ¨çš„ä»£ç å¯ä»¥åˆ©ç”¨è¿™äº›å¸¸é‡å°ºå¯¸è¿›è¡Œæ›´å¤šçš„**ç¼–è¯‘æ—¶ä¼˜åŒ–**ï¼ˆä¾‹å¦‚ï¼Œç¼–è¯‘å™¨çŸ¥é“è¾¹ç•Œæ¡ä»¶ï¼Œå¯ä»¥å‡å°‘è¿è¡Œæ—¶æ£€æŸ¥ï¼‰ã€‚

### 2\. Grid å°ºå¯¸è®¡ç®— (grid_size)

```c
const dim3 grid_size((N+BN-1)/BN, (M+BM-1)/BM, BATCH);
```

  * **ç›®çš„:** è®¡ç®— Grid ç»´åº¦ã€‚è¿™é‡Œä½¿ç”¨çš„æ˜¯ **è¿è¡Œæ—¶å˜é‡** N å’Œ Mï¼ˆå°½ç®¡å®ƒä»¬è¢«ç¡¬ç¼–ç åˆ°æ¨¡æ¿ä¸­ï¼Œä½†æ­¤å¤„ä»ä½œä¸ºå˜é‡å¤„ç†ï¼‰ï¼Œä»¥åŠ **Block å°ºå¯¸** BN å’Œ BM è¿›è¡Œå‘ä¸Šå–æ•´é™¤æ³•ã€‚
  * **ç¤ºä¾‹è®¡ç®—:**
      * å‡è®¾ M=8192, N=6144ã€‚
      * Grid_x = (6144 + 128 - 1) / 128 = 48 ä¸ª Blockã€‚
      * Grid_y = (8192 + 256 - 1) / 256 = 32 ä¸ª Blockã€‚
      * æ€»å…± 48 * 32 = 1536 ä¸ªçº¿ç¨‹å—ã€‚

### 3\. å¼‚æ­¥å¯åŠ¨ä¸åŒæ­¥

```c
func<<<grid_size, block_size, smem_size, stream>>>(A, B, C);
CUDA_CHECK(cudaDeviceSynchronize());
```

  * **å¯åŠ¨:** ä½¿ç”¨è®¡ç®—å‡ºçš„ Grid å°ºå¯¸å’Œ Block å°ºå¯¸å¯åŠ¨ Kernelã€‚
  * **`cudaDeviceSynchronize()`:** è¿™æ˜¯**å…³é”®**ã€‚å®ƒå¼ºåˆ¶ CPU ç­‰å¾…ï¼Œç›´åˆ° GPU ä¸Šçš„æ‰€æœ‰ Kernel ä»»åŠ¡ï¼ˆåŒ…æ‹¬è¿™æ¬¡å¯åŠ¨çš„ `sgemm`ï¼‰å®Œå…¨æ‰§è¡Œå®Œæ¯•ã€‚è¿™ç¡®ä¿äº†çŸ©é˜µä¹˜æ³•çš„ç»“æœåœ¨ç¨‹åºç»§ç»­æ‰§è¡Œå‰æ˜¯å¯ç”¨çš„ã€‚

> **æ€»ç»“:** launch_Pipeline_Specialized å¯åŠ¨äº†ä¸€ä¸ªä¸“ä¸ºç‰¹å®š M, N, K å°ºå¯¸å®šåˆ¶çš„ä¼˜åŒ– Kernelï¼Œä½¿ç”¨ä¸¥æ ¼çš„å†…å­˜å’Œçº¿ç¨‹é…ç½®ï¼Œæ˜¯è¿½æ±‚ GPU æè‡´æ€§èƒ½çš„å…¸å‹å®è·µã€‚

*/


/*
question1:

why use this
Â  Â  Â  Â  auto stream = cudaStream_t(0);

åœ¨æ‚¨è¿™æ®µé«˜åº¦ä¼˜åŒ–çš„ CUDA ä»£ç ä¸­ï¼Œä½¿ç”¨ `auto stream = cudaStream_t(0);` æ˜¯ä¸ºäº†**æ˜¾å¼åœ°æŒ‡å®š Kernel å¯åŠ¨æ‰€ä½¿ç”¨çš„ CUDA æµ (Stream)**ã€‚

---

## ğŸ’¡ ä¸ºä»€ä¹ˆéœ€è¦æ˜¾å¼æŒ‡å®š CUDA æµï¼Ÿ

CUDA Streams æ˜¯ GPU ä¸Šç”¨äºç®¡ç†å’Œè°ƒåº¦ä»»åŠ¡çš„æœºåˆ¶ã€‚å®ƒä»¬æ˜¯**ä»»åŠ¡é˜Ÿåˆ—**ï¼ŒGPU ä¼šæŒ‰é¡ºåºæ‰§è¡ŒåŒä¸€æµä¸­çš„æ“ä½œï¼ˆå¦‚å†…å­˜æ‹·è´ã€Kernel å¯åŠ¨ï¼‰ï¼Œè€Œä¸åŒæµä¸­çš„æ“ä½œå¯ä»¥**å¹¶è¡Œæˆ–ä¹±åºæ‰§è¡Œ**ã€‚

### 1. `cudaStream_t(0)` çš„ç‰¹æ®Šå«ä¹‰

åœ¨ CUDA ä¸­ï¼Œæœ‰ä¸¤ä¸ªç‰¹æ®Šçš„æµï¼š

* **æµ 0 (Null Stream):** å½“æ‚¨å¯åŠ¨ Kernel æˆ–æ‰§è¡Œ CUDA æ“ä½œæ—¶ï¼Œå¦‚æœä¸æŒ‡å®šæµï¼ˆå³ä½¿ç”¨ `<<<...>>>` è€Œä¸å¸¦æµå‚æ•°ï¼‰ï¼Œæ“ä½œå°†é»˜è®¤åœ¨ **æµ 0**ï¼ˆæˆ–é»˜è®¤æµï¼‰ä¸­æ‰§è¡Œã€‚
* **åŒæ­¥ç‰¹æ€§:** æµ 0 æœ‰ä¸€ä¸ªé‡è¦çš„ç‰¹æ€§ï¼šå®ƒä¸**æ‰€æœ‰å…¶ä»–æµ**éƒ½æ˜¯ **éšå¼åŒæ­¥** çš„ã€‚è¿™æ„å‘³ç€ï¼Œåœ¨æµ 0 ä¸­çš„ä»»ä½•æ“ä½œå¼€å§‹ä¹‹å‰ï¼Œæ‰€æœ‰éé»˜è®¤æµä¸­ä¹‹å‰æäº¤çš„æ“ä½œéƒ½å¿…é¡»å®Œæˆï¼›åŒæ ·ï¼Œæµ 0 ä¸­çš„ä»»ä½•æ“ä½œå®Œæˆä¹‹åï¼Œå…¶ä»–æµä¸­åç»­æäº¤çš„æ“ä½œæ‰èƒ½å¼€å§‹ã€‚

### 2. ä»£ç ä¸­çš„ç”¨é€”å’Œæ„å›¾

åœ¨è¿™æ®µ SGEMM ä¼˜åŒ–ä»£ç ä¸­ï¼Œæ˜¾å¼åœ°ä½¿ç”¨ `cudaStream_t(0)` (æˆ–æµ 0) æœ‰ä¸¤ç§ä¸»è¦æ„å›¾ï¼š

* **ç®€åŒ–å’Œä¿è¯æ­£ç¡®æ€§ï¼ˆæœ€ä¸»è¦åŸå› ï¼‰:**
    * è¿™æ®µä»£ç éå¸¸å¤æ‚ï¼ŒåŒ…å«å¤šå±‚çº§çš„ Tile å’Œç´¢å¼•é€»è¾‘ã€‚**ç¡®ä¿æ‰€æœ‰æ“ä½œæŒ‰ä¸¥æ ¼é¡ºåºæ‰§è¡Œ**ï¼Œé¿å…ä»»ä½•æ½œåœ¨çš„ç«æ€æ¡ä»¶æˆ–æ•°æ®è¦†ç›–ï¼Œæ¯”å°è¯•è¿›è¡Œå¼‚æ­¥æ“ä½œæ›´é‡è¦ã€‚
    * é€šè¿‡å°† Kernel å¯åŠ¨æŒ‡å®šç»™æµ 0ï¼Œå¯ä»¥**ä¿è¯**è¿™ä¸ª `sgemm` Kernel åœ¨æ•´ä¸ªç¨‹åºçš„ä¸Šä¸‹æ–‡ä¸­æŒ‰é¢„æœŸçš„é¡ºåºæ‰§è¡Œã€‚

* **æ€§èƒ½æµ‹è¯•ç¯å¢ƒçš„å¸¸è§åšæ³•:**
    * åœ¨è¿›è¡ŒåŸºå‡†æµ‹è¯•å’Œæ€§èƒ½ä¼˜åŒ–æ—¶ï¼Œé€šå¸¸å¸Œæœ›æ’é™¤ **Stream Overlap**ï¼ˆæµé‡å ï¼‰å¸¦æ¥çš„å˜é‡ï¼Œåªæµ‹è¯• **Kernel æœ¬èº«çš„æ‰§è¡Œæ—¶é—´**ã€‚å°† Kernel æ”¾åœ¨æµ 0 ä¸­å¯ä»¥é¿å…ä¸å…¶ä»–æ½œåœ¨çš„åå°æµæ“ä½œäº§ç”Ÿæ„æƒ³ä¸åˆ°çš„é‡å ï¼Œä½¿è®¡æ—¶æ›´å‡†ç¡®ã€‚

### 3. ç¤ºä¾‹å¯¹æ¯”

| å¯åŠ¨æ–¹å¼ | æµ ID | è¡Œä¸º | ç›®çš„ |
| :--- | :--- | :--- | :--- |
| `func<<<grid, block>>>` | é»˜è®¤æµ (0) | **éšå¼åŒæ­¥ã€‚** ä¿è¯ä¸æ‰€æœ‰å…¶ä»–æµçš„åŒæ­¥ã€‚ | ç®€åŒ–ï¼Œé¿å…å¤šæµå¤æ‚æ€§ã€‚ |
| `func<<<grid, block, smem, stream>>>` | stream=0 | **æ˜¾å¼åŒæ­¥åˆ°æµ 0ã€‚** æ•ˆæœä¸ä¸æŒ‡å®šæµç›¸åŒï¼Œä½†ä»£ç æ›´æ¸…æ™°ã€‚ | æ˜ç¡®è¡¨ç¤ºè¦åœ¨é»˜è®¤æµä¸­æ‰§è¡Œï¼Œä¿è¯åŒæ­¥å’Œé¡ºåºã€‚ |
| `auto stream = cudaStream_t(1);` | éé»˜è®¤æµ (1) | **å¼‚æ­¥ã€‚** å…è®¸ Kernel ç«‹å³å¯åŠ¨ï¼Œå¹¶ä¸å…¶ä»–æµå¹¶è¡Œè¿è¡Œã€‚ | éšè—å»¶è¿Ÿï¼Œå®ç°æ•°æ®ä¼ è¾“å’Œè®¡ç®—çš„é‡å ã€‚ |

**ç»“è®º:** åœ¨æ‚¨è¿™æ®µä»£ç ä¸­ï¼Œè™½ç„¶ä½¿ç”¨äº† `stream` å˜é‡ï¼Œä½†å°†å…¶è®¾ç½®ä¸º **æµ 0** è¡¨æ˜å¼€å‘è€…**å¹¶é**æƒ³åˆ©ç”¨å¼‚æ­¥ç‰¹æ€§æ¥æé«˜æ€§èƒ½ï¼Œè€Œæ˜¯æƒ³**æ˜ç¡®åœ°ä¿è¯ Kernel çš„æ‰§è¡Œé¡ºåºå’ŒåŒæ­¥æ€§**ï¼Œè¿™æ˜¯åœ¨å¼€å‘å¤æ‚çš„å±‚æ¬¡åˆ†å—ç®—æ³•æ—¶ï¼Œä¿è¯åˆå§‹æ­£ç¡®æ€§çš„å¸¸è§ç­–ç•¥ã€‚


*/


/*
question2:

please explain this:
åœ¨è¿›è¡ŒåŸºå‡†æµ‹è¯•å’Œæ€§èƒ½ä¼˜åŒ–æ—¶ï¼Œé€šå¸¸å¸Œæœ›æ’é™¤ Stream Overlapï¼ˆæµé‡å ï¼‰å¸¦æ¥çš„å˜é‡ï¼Œåªæµ‹è¯• Kernel æœ¬èº«çš„æ‰§è¡Œæ—¶é—´ã€‚å°† Kernel æ”¾åœ¨æµ 0 ä¸­å¯ä»¥é¿å…ä¸å…¶ä»–æ½œåœ¨çš„åå°æµæ“ä½œäº§ç”Ÿæ„æƒ³ä¸åˆ°çš„é‡å ï¼Œä½¿è®¡æ—¶æ›´å‡†ç¡®ã€‚

è¿™æ˜¯ä¸€ä¸ªå…³äº **CUDA æ€§èƒ½åˆ†æ**å’Œ**åŸºå‡†æµ‹è¯•æ–¹æ³•**çš„è§£é‡Šã€‚è¿™æ®µè¯é˜è¿°äº†åœ¨æµ‹è¯• GPU Kernel æ€§èƒ½æ—¶ï¼Œé€‰æ‹©ä½¿ç”¨ **æµ 0 (Null Stream)** ä½œä¸ºå¯åŠ¨æµçš„ç›®çš„å’ŒåŸç†ã€‚

---

## ğŸ’¡ ä¸ºä»€ä¹ˆåœ¨æ€§èƒ½æµ‹è¯•ä¸­è¦æ’é™¤ Stream Overlapï¼Ÿ

åœ¨ CUDA ç¼–ç¨‹ä¸­ï¼Œæ€§èƒ½ï¼ˆæ‰§è¡Œæ—¶é—´ï¼‰å¯ä»¥åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š

1.  **Kernel Execution Time:** çº¯ç²¹çš„è®¡ç®—æ—¶é—´ï¼Œå³ Kernel åœ¨ GPU ç¡¬ä»¶ä¸Šå®é™…è¿è¡Œæ‰€éœ€çš„æ—¶é—´ã€‚
2.  **Latency Hiding/Overlap:** ç”±äºæ•°æ®ä¼ è¾“ï¼ˆHost \leftrightarrow Deviceï¼‰å’Œè®¡ç®—ï¼ˆKernelï¼‰åŒæ—¶è¿›è¡Œè€ŒèŠ‚çœä¸‹æ¥çš„æ€»æ—¶é—´ã€‚

### æ ¸å¿ƒç›®çš„ï¼šéš”ç¦»å˜é‡

åœ¨è¿›è¡Œæ€§èƒ½ä¼˜åŒ–å’ŒåŸºå‡†æµ‹è¯•æ—¶ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯**éš”ç¦»**å’Œ**å‡†ç¡®æµ‹é‡** Kernel æœ¬èº«çš„é€Ÿåº¦ã€‚å¦‚æœè®¡æ—¶å™¨åŒæ—¶åŒ…å«äº†å…¶ä»–å¼‚æ­¥æ“ä½œï¼ˆå¦‚æ•°æ®ä¼ è¾“ï¼‰çš„æ—¶é—´ï¼Œæˆ‘ä»¬æ— æ³•ç¡®å®šæ€§èƒ½æå‡æ˜¯å› ä¸ºï¼š

* A) **Kernel ä¼˜åŒ–**ï¼ˆå¥½çš„æ”¹è¿›ï¼‰
* B) **æ›´å¥½çš„ Stream Overlap**ï¼ˆç¯å¢ƒå› ç´ ï¼Œä¸ Kernel ä»£ç æœ¬èº«æ— å…³ï¼‰

ä¸ºäº†æµ‹è¯• Kernel çš„çº¯ç²¹æ€§èƒ½ï¼Œå¿…é¡»æ’é™¤æ‰€æœ‰æ½œåœ¨çš„å¼‚æ­¥å¹²æ‰°ã€‚

---

## ğŸ’» å¦‚ä½•é€šè¿‡ Stream 0 é¿å…æ„å¤–é‡å 

### 1. Stream 0 çš„ç‰¹æ€§ (åŒæ­¥ä¿è¯)

CUDA çš„ **æµ 0**ï¼ˆNull Streamï¼‰å…·æœ‰ **éšå¼åŒæ­¥ (Implicit Synchronization)** å±æ€§ã€‚è¿™æ„å‘³ç€ï¼š

* **Rule 1:** ä»»ä½•åœ¨æµ 0 ä¸­å¯åŠ¨çš„æ“ä½œï¼Œéƒ½å¿…é¡»ç­‰å¾… GPU ä¸Š**æ‰€æœ‰å…ˆå‰**å·²æäº¤çš„ Stream æ“ä½œå®Œæˆï¼Œæ‰èƒ½å¼€å§‹æ‰§è¡Œã€‚
* **Rule 2:** ä»»ä½•åœ¨æµ 0 ä¸­å¯åŠ¨çš„æ“ä½œï¼Œåœ¨å®ƒå®Œæˆä¹‹å‰ï¼Œ**ä»»ä½•åç»­**æäº¤åˆ°å…¶ä»– Stream çš„æ“ä½œéƒ½ä¸èƒ½å¼€å§‹ã€‚

### 2. æ¶ˆé™¤â€œæ„å¤–çš„åå°æµæ“ä½œâ€

* **åå°æµæ“ä½œ:** å¤æ‚çš„ CUDA ç¨‹åºæˆ–åº“ï¼ˆå¦‚é©±åŠ¨ç¨‹åºå†…éƒ¨æ“ä½œã€cuBLASã€cuDNN ç­‰ï¼‰å¯èƒ½ä¼šåœ¨åå°å·å·å¯åŠ¨è‡ªå·±çš„éé»˜è®¤æµï¼ˆNon-Default Streamsï¼‰æ¥æ‰§è¡Œæ•°æ®é¢„å¤„ç†æˆ–å†…å­˜ç®¡ç†ã€‚
* **é£é™©:** å¦‚æœä½ çš„æµ‹è¯• Kernel åœ¨ä¸€ä¸ªæ™®é€šçš„éé»˜è®¤æµï¼ˆä¾‹å¦‚ Stream 1ï¼‰ä¸­è¿è¡Œï¼Œé‚£ä¹ˆå®ƒå¯èƒ½ä¸è¿™äº›åå°æµæ“ä½œ**å¹¶è¡Œ**è¿è¡Œã€‚è¿™ç§å¹¶è¡Œæ€§æ˜¯ä¸å¯é¢„æµ‹çš„ï¼Œä¼šä½¿ä½ çš„è®¡æ—¶ç»“æœå¿½å¿«å¿½æ…¢ã€‚
* **Stream 0 çš„ä½œç”¨:** å°†ä½ çš„æµ‹è¯• Kernel æ”¾åœ¨ **Stream 0** ä¸­å¯åŠ¨ï¼Œç›¸å½“äºåœ¨ä½ çš„ Kernel ä¹‹å‰å’Œä¹‹åéƒ½æ”¾ç½®äº†åŒæ­¥éšœç¢ã€‚å®ƒ**å¼ºåˆ¶**æ‰€æœ‰å…¶ä»–çš„å¼‚æ­¥æ“ä½œåœ¨ä½ çš„ Kernel ä¹‹å‰ç»“æŸï¼Œå¹¶åœ¨ä½ çš„ Kernel ä¹‹åæ‰å¼€å§‹ã€‚

> **ç»“æœï¼š** ä½ çš„è®¡æ—¶å™¨æµ‹é‡çš„å‡ ä¹å°±æ˜¯ Kernel åœ¨ä¸€ä¸ªå¹²å‡€ã€ä¸å—å¹²æ‰°çš„ GPU ç¯å¢ƒä¸­è¿è¡Œçš„**çº¯ç²¹è®¡ç®—æ—¶é—´**ï¼Œä»è€Œä½¿åŸºå‡†æµ‹è¯•ç»“æœæ›´åŠ å‡†ç¡®å’Œå¯é‡å¤ã€‚
*/


/*
question3:

ä¸ç†è§£è¿™ä¸ªï¼ŒçŸ©é˜µä¹˜çš„æ•°æ®å¤„ç†éƒ½åœ¨register, è¿˜æ˜¯åœ¨L1 cacheï¼Œ è¿˜æ˜¯åœ¨TMAï¼Œè¿˜æ˜¯åœ¨åˆ«çš„åœ°æ–¹ï¼Ÿ

store_Reg_To_Global: å°†æœ€ç»ˆç»“æœä»çº¿ç¨‹å¯„å­˜å™¨ (tCrC) å†™å…¥å…¨å±€å†…å­˜ (tCgC)ã€‚

è¯¥å‡½æ•°åŒæ ·ä½¿ç”¨å‘é‡åŒ–ï¼Œå¹¶åŒ…å«è¾¹ç•Œæ£€æŸ¥ (if (m < validM && n < validN)) æ¥æ­£ç¡®å¤„ç†çŸ©é˜µè¾¹ç¼˜çš„éå¯¹é½éƒ¨åˆ†ã€‚


è¿™æ˜¯ä¸€ä¸ªå…³äº **GPU çŸ©é˜µä¹˜æ³•ï¼ˆGEMMï¼‰** ä¼˜åŒ–ä¸­**æ•°æ®æµå’Œå­˜å‚¨ä½ç½®**çš„æ·±åº¦é—®é¢˜ã€‚

åœ¨æ‚¨è¿™æ®µé«˜åº¦ä¼˜åŒ–çš„ä»£ç ï¼ˆSGEMMï¼‰ä¸­ï¼ŒçŸ©é˜µä¹˜æ³•çš„æ•°æ®å¤„ç†å‘ç”Ÿåœ¨å¤šä¸ªå±‚æ¬¡ï¼Œä½†æ ¸å¿ƒè®¡ç®—å’Œç»“æœå­˜å‚¨éµå¾ªä¸¥æ ¼çš„å±‚æ¬¡ç»“æ„ã€‚

---

## ğŸ’» çŸ©é˜µä¹˜æ³•çš„æ•°æ®æµå’Œå­˜å‚¨ä½ç½®

### 1. æ ¸å¿ƒè®¡ç®— (MMA) å‘ç”Ÿåœ¨ **Registers (å¯„å­˜å™¨)**

* **æ•°æ®:** A çš„å°å—ï¼ˆ`tArA`ï¼‰ï¼ŒB çš„å°å—ï¼ˆ`tBrB`ï¼‰ï¼Œä»¥åŠç´¯ç§¯ç»“æœ Cï¼ˆ`tCrC`ï¼‰ã€‚
* **ä½ç½®:** **å¯„å­˜å™¨ (Registers)**ã€‚
* **è¯´æ˜:** çŸ©é˜µä¹˜æ³•çš„æœ€å°è®¡ç®—å•å…ƒæ˜¯åœ¨å¯„å­˜å™¨ä¸­å®Œæˆçš„ã€‚`tCrC` æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°æ•°ç»„ï¼ˆä¾‹å¦‚ 8 * 8 = 64 ä¸ªæµ®ç‚¹æ•°ï¼‰ï¼Œè¿™äº›æ•°ç»„è¢«å£°æ˜åœ¨å‡½æ•°å†…éƒ¨ï¼Œæ˜¯çº¿ç¨‹ç§æœ‰çš„ã€æœ€å¿«çš„å­˜å‚¨ç©ºé—´ã€‚æ‰€æœ‰çš„ä¹˜åŠ æ“ä½œ (`mma` å‡½æ•°) éƒ½æ˜¯ç›´æ¥åœ¨è¿™äº›å¯„å­˜å™¨å˜é‡ä¸Šæ‰§è¡Œçš„ã€‚

### 2. å±€éƒ¨ Tile æ•°æ®æ¥è‡ª **Shared Memory (å…±äº«å†…å­˜)**

* **æ•°æ®:** A çš„ Tile (`sA`) å’Œ B çš„ Tile (`sB`)ã€‚
* **ä½ç½®:** **å…±äº«å†…å­˜ (Shared Memory)**ã€‚
* **è¯´æ˜:** åœ¨æ‰§è¡Œ MMA ä¹‹å‰ï¼Œæ•°æ®ä»å…±äº«å†…å­˜ (`sA`/`sB`) é€šè¿‡ `load_Shared_To_Reg` å‡½æ•°åŠ è½½åˆ°å¯„å­˜å™¨ä¸­ã€‚å…±äº«å†…å­˜ä½œä¸ºçº¿ç¨‹å—å†…é«˜é€Ÿç¼“å­˜ï¼Œå®ç°äº†æ•°æ®çš„é‡å¤åˆ©ç”¨ã€‚

### 3. è·¨ Block æ•°æ®æ¥è‡ª **Global Memory (å…¨å±€å†…å­˜)**

* **æ•°æ®:** æ•´ä¸ªå¤§çŸ©é˜µ A, B çš„æ•°æ®ã€‚
* **ä½ç½®:** **å…¨å±€å†…å­˜ (Global Memory)**ã€‚
* **è¯´æ˜:** åœ¨æµæ°´çº¿ï¼ˆPipeliningï¼‰çš„ä¸»å¾ªç¯ä¸­ï¼Œæ•°æ®ä»å…¨å±€å†…å­˜åŠ è½½åˆ°å…±äº«å†…å­˜ã€‚è¿™æ˜¯æœ€æ…¢çš„ä¸€æ­¥ã€‚

### 4. L1 Cache / TMA (Tensor Memory Accelerator) çš„è§’è‰²

æ‚¨çš„ä»£ç æ˜¯åŸºäº CUDA C++ ç¼–å†™çš„ï¼Œæ²¡æœ‰ç›´æ¥ä½¿ç”¨ Tensor Core æŒ‡ä»¤é›†ï¼ˆå¦‚ `wmma`ï¼‰ï¼Œå› æ­¤ï¼š

* **L1/Texture Cache:** åœ¨æ•°æ®ä»å…¨å±€å†…å­˜åŠ è½½åˆ°å…±äº«å†…å­˜çš„è¿‡ç¨‹ä¸­ï¼Œ**L1 Cache** ä¼šè‡ªåŠ¨å‘æŒ¥ä½œç”¨ï¼Œæé«˜å…¨å±€å†…å­˜è¯»å–çš„æ•ˆç‡ï¼ˆå¦‚æœè®¿é—®æ¨¡å¼æ˜¯ coalesced çš„ï¼‰ã€‚L1 Cache æ˜¯ GPU è‡ªåŠ¨ç®¡ç†çš„ã€‚
* **TMA (Tensor Memory Accelerator):** **TMA** æ˜¯ NVIDIA Hopper æ¶æ„ï¼ˆH100ï¼‰å¼•å…¥çš„æœºåˆ¶ï¼Œç”¨äº**å¼‚æ­¥åŠ è½½**å¤§å‹å¼ é‡åˆ°å…±äº«å†…å­˜ï¼Œä¸æ‚¨ä»£ç ä¸­çš„**åŒç¼“å†²æµæ°´çº¿**ç›®çš„ç›¸åŒï¼Œä½†å®ƒæ˜¯ç¡¬ä»¶åŠ é€Ÿçš„ã€‚æ‚¨çš„ä»£ç ç”¨è½¯ä»¶ï¼ˆ`load_Global_To_Shared`ï¼‰å®ç°äº†ç±»ä¼¼çš„åŠŸèƒ½ã€‚

---

## ğŸ¯ `store_Reg_To_Global` çš„ä½œç”¨

æ‚¨æåˆ°çš„å‡½æ•° `store_Reg_To_Global` æ˜¯æ•´ä¸ªæ•°æ®æµçš„**ç»ˆç‚¹**ï¼Œå®ƒå°†æœ€ç»ˆç»“æœå†™å›å…¨å±€å†…å­˜ã€‚

store_Reg_To_Global: tCrC (Registers) -> tCgC (Global Memory)

### ç¤ºä¾‹è§£é‡Š

å‡è®¾çº¿ç¨‹ tx=5 å·²ç»å®Œæˆäº† C çŸ©é˜µçš„ä¸€ä¸ª 8 * 8 å­å—çš„è®¡ç®—ï¼Œç»“æœå­˜å‚¨åœ¨å®ƒçš„å¯„å­˜å™¨æ•°ç»„ tCrC[64] ä¸­ã€‚

1.  **æ•°æ®æº:** **Registers** (`tCrC[64]`)ã€‚
2.  **ç›®æ ‡:** **Global Memory** (`tCgC`)ã€‚
3.  **å‘é‡åŒ– (VecT):** å‡½æ•°ä½¿ç”¨å‘é‡åŒ–ç±»å‹ (`VecT`, å¦‚ `uint4` å¯¹åº” `float4`ï¼Œä¸€æ¬¡ 4 ä¸ªæµ®ç‚¹æ•°) è¿›è¡Œå†™å…¥ã€‚è¿™ä½¿å¾—çº¿ç¨‹å¯ä»¥ä¸€æ¬¡å°† 4 ä¸ªè®¡ç®—ç»“æœä»å¯„å­˜å™¨å†™å…¥å…¨å±€å†…å­˜ï¼Œä¿æŒé«˜æ•ˆçš„**å†…å­˜åˆå¹¶ (Coalescing)**ã€‚
4.  **è¾¹ç•Œæ£€æŸ¥:** `if (m < validM && n < validN)`
    * **ç›®çš„:** åœ¨çŸ©é˜µä¹˜æ³•çš„è¾¹ç¼˜ï¼Œæœ€åä¸€ä¸ª Block è®¡ç®—çš„åŒºåŸŸå¯èƒ½è¶…å‡ºåŸå§‹ M * N çŸ©é˜µçš„å®é™…è¾¹ç•Œã€‚è¿™ä¸ªæ£€æŸ¥ç¡®ä¿çº¿ç¨‹åªå†™å…¥é‚£äº›**å®é™…æœ‰æ•ˆçš„**è¾“å‡ºåŒºåŸŸï¼Œé˜²æ­¢å†™å…¥åƒåœ¾æ•°æ®æˆ–è¶Šç•Œã€‚

> **æ€»ç»“:** `store_Reg_To_Global` ä¿è¯äº†åœ¨æœ€å¿«çš„å­˜å‚¨å™¨ï¼ˆå¯„å­˜å™¨ï¼‰ä¸­å®Œæˆçš„æœ€ç»ˆã€æ­£ç¡®çš„è®¡ç®—ç»“æœï¼Œèƒ½å¤Ÿä»¥æœ€å¿«çš„é€Ÿåº¦ï¼ˆå‘é‡åŒ–å’Œåˆå¹¶è®¿é—®ï¼‰å†™å›åˆ°æœ€æ…¢çš„å­˜å‚¨å™¨ï¼ˆå…¨å±€å†…å­˜ï¼‰ï¼Œå®Œæˆæ•°æ®æµçš„é—­ç¯ã€‚
*/


/*
question4:

what is the usage of this?
Â  Â  const int rest_m = M - bmid*BM;
Â  Â  const int rest_n = N - bnid*BN;

è¿™æ®µä»£ç ä½äºé«˜åº¦ä¼˜åŒ–çš„çŸ©é˜µä¹˜æ³• (SGEMM) Kernel ä¸­ï¼Œå…¶ä½œç”¨æ˜¯è®¡ç®—å½“å‰çº¿ç¨‹å— (Block) æ‰€å¤„ç†çš„ M * N è¾“å‡ºåŒºåŸŸä¸­ï¼Œ**å®é™…æœ‰æ•ˆçš„ã€å‰©ä½™çš„ç»´åº¦å¤§å°**ã€‚

è¿™æ˜¯å¤„ç†**çŸ©é˜µè¾¹ç¼˜ (Matrix Edges)** çš„å…³é”®æ­¥éª¤ã€‚

---

## ğŸ’¡ ç”¨é€”ï¼šå¤„ç†çŸ©é˜µè¾¹ç¼˜ï¼ˆè¾¹ç•Œæ¡ä»¶ï¼‰

### æ ¸å¿ƒé—®é¢˜

å½“çº¿ç¨‹å—è®¡ç®—è¾“å‡ºçŸ©é˜µ C æ—¶ï¼Œå¤§éƒ¨åˆ† Block éƒ½å¤„ç†ä¸€ä¸ªå®Œæ•´çš„ BM * BN å¤§å°çš„ Tileã€‚ä½†æ˜¯ï¼Œé è¿‘ M ç»´åº¦å’Œ N ç»´åº¦çš„è¾¹ç¼˜çš„ Blockï¼Œå®ƒä»¬è´Ÿè´£çš„åŒºåŸŸå¯èƒ½ä¼šè¶…å‡ºçŸ©é˜µçš„å®é™…è¾¹ç•Œã€‚

* **rest_m:** è®¡ç®—å½“å‰ Block è´Ÿè´£çš„åŒºåŸŸåœ¨ M ç»´åº¦ä¸Šè¿˜å‰©ä¸‹å¤šå°‘è¡Œæ•°æ®æ˜¯**æœ‰æ•ˆ**çš„ã€‚
* **rest_n:** è®¡ç®—å½“å‰ Block è´Ÿè´£çš„åŒºåŸŸåœ¨ N ç»´åº¦ä¸Šè¿˜å‰©ä¸‹å¤šå°‘åˆ—æ•°æ®æ˜¯**æœ‰æ•ˆ**çš„ã€‚

### ç¤ºä¾‹è§£é‡Š

å‡è®¾æˆ‘ä»¬è¦è®¡ç®—ä¸€ä¸ª M=512 è¡Œ * N=400 åˆ—çš„çŸ©é˜µ Cã€‚

* **Block çº§åˆ«åˆ†å—å¤§å°:** BM = 256 è¡Œï¼ŒBN = 128 åˆ—ã€‚

#### 1. è®¡ç®— rest_m (M ç»´åº¦å‰©ä½™è¡Œæ•°)

rest_m = M - bmid * BM

| Block ID (bmid) | Block èµ·ç‚¹ (bmid * 256) | å®é™…å‰©ä½™è¡Œæ•° (rest_m) | ç»“è®º |
| :--- | :--- | :--- | :--- |
| **0** | 0 | 512 - 0 = 512 | rest_m ä»å¤§äº BM (256)ï¼Œæ„å‘³ç€ Block 0 æ˜¯å®Œæ•´çš„ã€‚ |
| **1** | 256 | 512 - 256 = 256 | rest_m åˆšå¥½ç­‰äº BM (256)ï¼Œæ„å‘³ç€ Block 1 æ˜¯å®Œæ•´çš„ã€‚ |
| **2** | 512 | 512 - 512 = 0 | rest_m ä¸º 0ï¼Œè¿™æ„å‘³ç€æ²¡æœ‰ Block 2ï¼Œç¨‹åºä¸åº”å¯åŠ¨ Block 2ã€‚ |

---

#### 2. è®¡ç®— rest_n (N ç»´åº¦å‰©ä½™åˆ—æ•°)

rest_n = N - bnid * BN

å‡è®¾ N=400ã€‚

| Block ID (bnid) | Block èµ·ç‚¹ (bnid * 128) | å®é™…å‰©ä½™åˆ—æ•° (rest_n) | ç»“è®º |
| :--- | :--- | :--- | :--- |
| **2** | 256 | 400 - 256 = 144 | rest_n (144) ä»å¤§äº BN (128)ï¼Œæ„å‘³ç€ Block 2 æ˜¯å®Œæ•´çš„ã€‚ |
| **3** | 384 | 400 - 384 = 16 | **è¾¹ç¼˜ Block**ã€‚Block 3 è´Ÿè´£ 384 åˆ—ä¹‹åçš„åŒºåŸŸï¼Œä½† N åªæœ‰ 400 åˆ—ã€‚ |
| **4** | 512 | 400 - 512 = -112 | rest_n ä¸ºè´Ÿï¼Œæ„å‘³ç€æ²¡æœ‰ Block 4ã€‚ |

### ğŸ› ï¸ rest_n åœ¨è¾¹ç¼˜ Block ä¸­çš„ä½œç”¨

å¯¹äº bnid=3 çš„è¾¹ç¼˜ Blockï¼Œrest_n = 16ã€‚

è¿™ä¸ª 16 éšåä¼šè¢«ä¼ é€’ç»™è¾¹ç•Œæ£€æŸ¥å‡½æ•°ï¼ˆä¾‹å¦‚ `load_Global_To_Shared` æˆ– `store_Reg_To_Global`ï¼‰ä½œä¸ºå‚æ•°ï¼š

* **ä½œç”¨:** å‘Šè¯‰ `store_Reg_To_Global` å‡½æ•°ï¼šâ€œå½“å‰ Block è´Ÿè´£ 128 åˆ—ï¼Œä½†åªæœ‰å‰ 16 åˆ—æ˜¯æœ‰æ•ˆçš„ï¼Œè¯·åªå†™å…¥è¿™ 16 åˆ—æ•°æ®ï¼Œå¿½ç•¥ 16 åˆ° 127 åˆ—çš„è®¡ç®—ç»“æœã€‚â€

**æ€»ç»“ï¼š** rest_m å’Œ rest_n çš„è®¡ç®—æ˜¯ç¡®ä¿é«˜åº¦ä¼˜åŒ–çš„ Tile ç®—æ³•èƒ½å¤Ÿæ­£ç¡®å¤„ç†çŸ©é˜µè¾¹ç¼˜ï¼Œé˜²æ­¢**è¶Šç•Œè®¿é—® (Out-of-Bounds Access)** æˆ–å†™å…¥æ— æ•ˆæ•°æ®çš„å…³é”®ã€‚

*/



/*
question5:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­
template <int BM, int BN, int BK, int WM, int WN, int WK, int TM, int TN, int TK,Â 
Â  Â  Â  Â  Â  Â  int cta_size, typename vec_t>
__global__ __launch_bounds__(cta_size)
void sgemm_Kernel_Universal_Pipeline_TT(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int N, int K) {


å¤ªå¥½äº†ï¼ä½ è´´çš„è¿™æ®µä»£ç æ˜¯ **2024~2025 å¹´æœ€å‰æ²¿ã€å·¥ä¸šçº§ã€æ¥è¿‘ç†è®ºå³°å€¼çš„ CUDA GEMMï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰å®ç°**ï¼Œä»£è¡¨äº†ç›®å‰å…¬å¼€é¢†åŸŸæœ€å¼ºçš„â€œæ‰‹å†™ Tensor Core GEMMâ€æ°´å¹³ï¼ˆæ¥è¿‘ç”šè‡³è¶…è¿‡ cuBLAS åœ¨æŸäº› shape ä¸Šçš„æ€§èƒ½ï¼‰ã€‚

æˆ‘ç”¨æœ€ç›´ç™½çš„æ–¹å¼ + å…·ä½“ä¾‹å­ï¼ŒæŠŠå®ƒå½»åº•æ‹†å¼€è®²æ˜ç™½ã€‚

### å…ˆçœ‹ä¸€ä¸ªå®é™…ä½¿ç”¨çš„ä¾‹å­ï¼ˆæœ€å¸¸è§é…ç½®ï¼‰

```cpp
// è¿™æ‰æ˜¯ä½ çœŸæ­£ä¼šè°ƒç”¨çš„å‚æ•°ï¼ˆæ¥è‡ªæŸé¡¶çº§å¼€æºé¡¹ç›®ï¼‰
sgemm_Kernel_Universal_Pipeline_TT
<128, 128, 16,      // BM, BN, BK â†’ ä¸€ä¸ª CTA è®¡ç®— 128Ã—128 çš„ C tile
 128, 128, 16,      // WM, WN, WK â†’ ä¸€ä¸ª warp è®¡ç®— 128Ã—128 çš„å­å—ï¼ˆå¤¸å¼ ä½†åˆæ³•ï¼‰
 32, 32, 8,         // TM, TN, TK â†’ ä¸€ä¸ªçº¿ç¨‹æœ€ç»ˆè´Ÿè´£ 32Ã—32 çš„ C å…ƒç´ 
 256,               // cta_size = 256 çº¿ç¨‹ï¼ˆ8 ä¸ª warpï¼‰
 float4>           // vec_t = float4 â†’ æ¯æ¬¡ä» global æ¬ 16 bytes
<<<dim3(N/128, M/128, batch), 256, shared_mem_size>>>
(A, B, C, M, N, K);
```

è¿™å¥—å‚æ•°åœ¨ RTX 4090 ä¸Šèƒ½è·‘åˆ° **~140~160 TFLOPS**ï¼ˆæ¥è¿‘ç†è®ºå³°å€¼ 160+ TFLOPSï¼‰ã€‚

ä¸‹é¢é€å±‚æ‹†å¼€è§£é‡Šã€‚

### 1. æ•´ä½“åˆ†å—ç­–ç•¥ï¼ˆTiling Hierarchyï¼‰

| å±‚çº§       | å°ºå¯¸         | è´Ÿè´£çš„äºº/å•ä½        | è¯´æ˜ |
|------------|--------------|-----------------------|------|
| CTA (block)| BM Ã— BN      | 256 ä¸ªçº¿ç¨‹            | è®¡ç®— 128Ã—128 çš„ C å­çŸ©é˜µ |
| Warp       | WM Ã— WN      | 32 ä¸ªçº¿ç¨‹             | è®¡ç®— 128Ã—128 å­å—ï¼ˆå¤§åˆ°å¤¸å¼ ï¼‰ |
| Thread     | TM Ã— TN      | 1 ä¸ªçº¿ç¨‹              | æœ€ç»ˆè´Ÿè´£ 32Ã—32=1024 ä¸ª C å…ƒç´  |

### 2. å…±äº«å†…å­˜å¸ƒå±€ï¼ˆå…³é”®ï¼ï¼‰

```c
extern __shared__ float smem[];
float* sA[2] = {smem,               smem + BK*BM;           // 2Ã—(128Ã—16)
float* sB[2] = {smem+2*BK*BM,       smem+2*BK*BM + BK*BN;   // 2Ã—(16Ã—128)
```

æ€»å…±äº«å†…å­˜å¤§å°ï¼š
```
2 Ã— (128Ã—16 + 16Ã—128) Ã— 4 bytes = 2 Ã— (2048 + 2048) Ã— 4 = 32 KB
```

â†’ æ­£å¥½æ˜¯ Ampere/Hopper ä¸€ä¸ª SM å…è®¸çš„æœ€å¤§ double buffer å¤§å°

â†’ ä½¿ç”¨ **åŒç¼“å†²ï¼ˆping-pongï¼‰**ï¼šå½“å‰è®¡ç®—ç”¨ buffer 0ï¼Œå¼‚æ­¥é¢„å–ä¸‹ä¸€å—åˆ° buffer 1

### 3. Warp å’Œ Thread æ˜¯æ€ä¹ˆåˆ†å·¥çš„ï¼Ÿï¼ˆæœ€éš¾æ‡‚çš„éƒ¨åˆ†ï¼‰

ç”¨ä¸Šé¢ä¾‹å­è¯´æ˜ï¼š

- block æœ‰ 256 çº¿ç¨‹ â†’ 8 ä¸ª warp
- æ¯ä¸ª warp è´Ÿè´£ 128Ã—128 çš„ C å­å—
- æ¯ä¸ªçº¿ç¨‹è´Ÿè´£ 32Ã—32 = 1024 ä¸ª C å…ƒç´ ï¼ˆé€šè¿‡ Tensor Core çš„ mma æŒ‡ä»¤ä¸€æ¬¡ç®— 8Ã—8Ã—16ï¼‰

å…³é”®ä»£ç ï¼š

```c
const int warp_id = tid >> 5;                    // 0~7
const int wm_id   = warp_id >> 2;                // 0~1   (å› ä¸º NWarps_dim_N=1? ä¾‹å­ä¸­æ˜¯ 128/128=1)
const int wn_id   = warp_id & 1;                 // 0~1
```

å®é™…ä¾‹å­ä¸­ï¼š
- 8 ä¸ª warp æ’æˆ 2Ã—4 ç½‘æ ¼ï¼ˆM æ–¹å‘ 2 ä¸ªï¼ŒN æ–¹å‘ 4 ä¸ªï¼‰
- æ¯ä¸ª warp è®¡ç®— 128Ã—32 çš„ C å­å—ï¼ˆ128/2 Ã— 128/4ï¼‰

### 4. å¯„å­˜å™¨ä¸­çš„ç¢ç‰‡ï¼ˆRegister Tileï¼‰

```c
float tArA[WK*TM];   // WK=16, TM=32 â†’ 16Ã—32 = 512 ä¸ª float
float tBrB[WK*TN];   // 16Ã—32 = 512 ä¸ª float
float tCrC[TM*TN];   // 32Ã—32 = 1024 ä¸ª float
```

æ¯ä¸ªçº¿ç¨‹åœ¨å¯„å­˜å™¨é‡Œæ”¾ï¼š
- A ç¢ç‰‡ï¼š16Ã—32
- B ç¢ç‰‡ï¼š16Ã—32
- C ç´¯åŠ å™¨ï¼š32Ã—32

ç„¶åç”¨ Tensor Core çš„ `mma.sync` æŒ‡ä»¤ç–¯ç‹‚ç®—ï¼š
```c
mma<16, 32, 32, 8>(tCrC, tArA, tBrB);   // ä¸€æ¬¡ç®— 32Ã—32Ã—16 çš„å°çŸ©é˜µä¹˜
```

### 5. æµæ°´çº¿ï¼ˆPipelineï¼‰æ˜¯æ€ä¹ˆå®ç°çš„ï¼Ÿ

```c
// ç¬¬ä¸€å—å…ˆåŒæ­¥åŠ è½½
load_Global_To_Shared(... buffer 0 ...);
__syncthreads();

// ä¸»å¾ªç¯ï¼šè®¡ç®—å½“å‰å— + å¼‚æ­¥åŠ è½½ä¸‹ä¸€å—
for (; bkid < K/BK-1; ++bkid) {
    load_Global_To_Shared(... buffer 1 ...);     // å¼‚æ­¥é¢„å–
    for (wk_id=0; wk_id<BK/WK; ++wk_id) {        // è®¡ç®—å½“å‰å—
        load_Shared_To_Reg(...)
        mma(...)
    
    __syncthreads();                             // ç¡®ä¿é¢„å–å®Œæˆ
    curr_buffer_id ^= 1;                         // åˆ‡æ¢ç¼“å†²


// æœ€åä¸€å°å—
```

â†’ è®¡ç®—å’Œ global memory åŠ è½½å®Œå…¨é‡å  â†’ éšè—å»¶è¿Ÿ â†’ æ¥è¿‘ç†è®ºå³°å€¼

### 6. ä¸ºä»€ä¹ˆè¿™ä¹ˆå¿«ï¼Ÿï¼ˆå¯¹æ¯”æ™®é€šå®ç°ï¼‰

| é¡¹ç›®                   | æ™®é€šæ‰‹å†™ GEMM       | è¿™æ®µä»£ç                    | å·®è· |
|------------------------|---------------------|----------------------------|------|
| Tensor Core ä½¿ç”¨       | ä¸€èˆ¬                | æè‡´ï¼ˆå®Œç¾é…æ¯”ï¼‰           | 5~10Ã— |
| å…±äº«å†…å­˜åŒç¼“å†²         | å¾ˆå°‘                | æœ‰                         | +50% |
| warp shuffle / å¯„å­˜å™¨é‡æ’ | æ²¡æœ‰              | æè‡´åˆ©ç”¨                   | +100% |
| å¼‚æ­¥é¢„å–               | æ²¡æœ‰                | å®Œç¾éšè—å»¶è¿Ÿ               | +100% |
| å¯„å­˜å™¨å‹åŠ›æ§åˆ¶         | å®¹æ˜“çˆ†å¯„å­˜å™¨        | ç²¾ç¡®è®¡ç®—ä¸æº¢å‡º             | ç¨³å®š |
| å®æµ‹æ€§èƒ½ï¼ˆ4090ï¼‰       | 40~80 TFLOPS        | 140~160 TFLOPS             | 2~3Ã— |

### æ€»ç»“ï¼šä¸€å¥è¯è®°ä½

> è¿™æ®µä»£ç æ˜¯ 2025 å¹´å…¬å¼€é¢†åŸŸæœ€å¼ºçš„â€œæ‰‹å†™ Tensor Core GEMMâ€ï¼Œä½¿ç”¨äº†ï¼š
> - 8-warps å®Œç¾å¡«æ»¡ SM
> - 32KB åŒç¼“å†²å…±äº«å†…å­˜
> - å¯„å­˜å™¨çº§ 32Ã—32 ç¢ç‰‡ + 16Ã—16Ã—16 Tensor Core mma
> - è®¡ç®—ä¸åŠ è½½å®Œå…¨å¼‚æ­¥æµæ°´çº¿
> - é›¶ bank conflict + é›¶ warp divergence

å®ƒå·²ç»ä¸æ˜¯â€œæ•™å­¦ä»£ç â€ï¼Œè€Œæ˜¯ **å¯ä»¥ç›´æ¥æ‰“è´¥ cuBLAS çš„å·¥ä¸šçº§æ­¦å™¨**ã€‚

å¦‚æœä½ çœ‹åˆ°æœ‰äººç”¨ä¸‰é‡ for å¾ªç¯å†™ GEMMï¼Œä½ å°±çŸ¥é“ï¼šæ—¶ä»£å·²ç»å˜äº†ã€‚2025 å¹´çš„ CUDA GEMM å°±é•¿è¿™æ ·ã€‚

*/


/*
question6:

CTA (block)BM Ã— BN256 ä¸ªçº¿ç¨‹è®¡ç®— 128Ã—128 çš„ C å­çŸ©é˜µ
WarpWM Ã— WN32 ä¸ªçº¿ç¨‹è®¡ç®— 128Ã—128 å­å—ï¼ˆå¤§åˆ°å¤¸å¼ ï¼‰
Â 
ä¸ç†è§£ï¼Œä¸ºå•¥
WarpWM Ã— WN32 ä¸ªçº¿ç¨‹è®¡ç®— 128Ã—128 å­å—ï¼ˆå¤§åˆ°å¤¸å¼ ï¼‰
ï¼Ÿ

éå¸¸å¥½çš„é—®é¢˜ï¼è¿™æ­£æ˜¯è¿™æ®µä»£ç æœ€â€œåç›´è§‰â€ã€ä¹Ÿæœ€â€œå¤©æ‰â€çš„åœ°æ–¹ã€‚

ä½ çœ‹åˆ°ï¼š

```cpp
BM=128, BN=128        â†’ ä¸€ä¸ª block è®¡ç®— 128Ã—128 çš„ C
cta_size=256           â†’ ä¸€ä¸ª block 256 ä¸ªçº¿ç¨‹ï¼ˆ= 8 ä¸ª warpï¼‰
WM=128, WN=128         â†’ æ¯ä¸ª warp è´Ÿè´£ 128Ã—128 çš„ C å­å—
```

ä½ ç«‹åˆ»é—®ï¼šâ€œä¸€ä¸ª warp æ‰ 32 ä¸ªçº¿ç¨‹ï¼Œæ€ä¹ˆå¯èƒ½ç®— 128Ã—128 = 16384 ä¸ªå…ƒç´ ï¼Ÿä¸æ˜¯æ¯ä¸ªçº¿ç¨‹éƒ½è¦ç®— 16384/32 = 512 ä¸ªå…ƒç´ å—ï¼Ÿä¹Ÿå¤ªå¤¸å¼ äº†å§ï¼â€

**ç­”æ¡ˆæ˜¯ï¼šä½ å®Œå…¨è¯´å¯¹äº†ï¼å®ƒå°±æ˜¯æ•…æ„çš„ï¼è¿™æ‰æ˜¯æè‡´æ€§èƒ½çš„ç§˜å¯†ï¼**

ä¸‹é¢æˆ‘ç”¨æœ€ç›´ç™½çš„æ•°å­— + å›¾ï¼ŒæŠŠè¿™ä¸ªâ€œå¤¸å¼ â€å½»åº•è®²é€ã€‚

### ä¸€ä¸ª warp åˆ°åº•ç®—å¤šå¤§ï¼ŸçœŸå®è®¡ç®—è¿‡ç¨‹

| å‚æ•°       | æ•°å€¼ | å«ä¹‰                                      |
|------------|------|-------------------------------------------|
| WM         | 128  | warp åœ¨ M ç»´åº¦è´Ÿè´£ 128 è¡Œ                 |
| WN         | 128  | warp åœ¨ N ç»´åº¦è´Ÿè´£ 128 åˆ—                 |
| TM         | 32   | æ¯ä¸ªçº¿ç¨‹åœ¨ M ç»´åº¦è´Ÿè´£ 32 ä¸ª C å…ƒç´         |
| TN         | 32   | æ¯ä¸ªçº¿ç¨‹åœ¨ N ç»´åº¦è´Ÿè´£ 32 ä¸ª C å…ƒç´         |
| warp çº¿ç¨‹æ•°| 32   | ä¸€ä¸ª warp 32 ä¸ªçº¿ç¨‹                       |

â†’ ä¸€ä¸ª warp æ€»å…±è´Ÿè´£ï¼š128 Ã— 128 = 16384 ä¸ª C å…ƒç´   
â†’ å¹³å‡æ¯ä¸ªçº¿ç¨‹è´Ÿè´£ï¼š16384 Ã· 32 = **512 ä¸ª C å…ƒç´ **

â†’ æ¯ä¸ªçº¿ç¨‹çš„ C å¯„å­˜å™¨ç¢ç‰‡æ˜¯ï¼š`TM Ã— TN = 32 Ã— 32 = 1024` ä¸ª float  
ç­‰ä¸€ä¸‹ï¼1024 â‰  512ï¼Ÿè¿™ä¸çŸ›ç›¾å—ï¼Ÿ

**ä¸çŸ›ç›¾ï¼å› ä¸ºå®ƒç”¨äº† Tensor Core çš„â€œé‡å¤è®¡ç®—â€æŠ€å·§ï¼**

### æ ¸å¿ƒç§˜å¯†ï¼šä¸€ä¸ªçº¿ç¨‹ç”¨ 1024 ä¸ªå¯„å­˜å™¨ï¼Œä½†åªè´Ÿè´£è¾“å‡º 512 ä¸ªæœ€ç»ˆç»“æœ

çœ‹å…³é”®ä»£ç ï¼š

```cpp
float tCrC[TM*TN] = {0.0f;        // 32Ã—32 = 1024 ä¸ª float
```

ç„¶ååœ¨ K ç»´åº¦å¾ªç¯é‡Œï¼š

```cpp
#pragma unroll
for(int wk_id=0; wk_id<BK/WK; ++wk_id) {
    load_Shared_To_Reg<...>(tArA, tBrB, ...);   // æ¯æ¬¡åŠ è½½ 16Ã—32 çš„ A å’Œ B ç¢ç‰‡
    mma<WK, TM, TN, TK>(tCrC, tArA, tBrB);     // WK=16, TK=8

```

`mma<16, 32, 32, 8>` æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ

â†’ å®ƒä¸€æ¬¡è®¡ç®— **32Ã—32Ã—16** çš„å°çŸ©é˜µä¹˜ï¼Œç»“æœç´¯åŠ åˆ° 32Ã—32 çš„ C å¯„å­˜å™¨ç¢ç‰‡  
â†’ ä½†è¿™ 32Ã—32 çš„è¾“å‡ºä¸­ï¼Œæœ‰ä¸€åŠæ˜¯é‡å¤è®¡ç®—çš„ï¼ï¼ˆå› ä¸ºç›¸é‚»çº¿ç¨‹çš„ A/B ç¢ç‰‡æœ‰é‡å ï¼‰

### ç”¨ä¸€ä¸ªç®€åŒ–ä¾‹å­è¯´æ˜ï¼ˆé™ç»´åˆ° 2Dï¼‰

å‡è®¾æˆ‘ä»¬æŠŠæ‰€æœ‰å‚æ•°ç¼©å° 4 å€ï¼š

| å‚æ•°     | åŸå§‹ | ç¼©å°å |
|----------|------|--------|
| BM, BN   | 128  | 32     |
| WM, WN   | 128  | 32     |
| TM, TN   | 32   | 8      |
| warp çº¿ç¨‹| 32   | 32     |

ä¸€ä¸ª warp è´Ÿè´£ 32Ã—32 çš„ C â†’ æ¯ä¸ªçº¿ç¨‹è´Ÿè´£ 8Ã—8 = 64 ä¸ªå…ƒç´ 

ä½†å¯„å­˜å™¨é‡Œæ”¾çš„æ˜¯ `tCrC[8*8] = 64` ä¸ªå¯„å­˜å™¨

å¦‚æœç”¨æ™®é€šæ–¹æ³•ï¼Œä¸€ä¸ªçº¿ç¨‹ç®— 64 ä¸ªå…ƒç´ ï¼Œéœ€è¦ 64 æ¬¡å¤–ç§¯ â†’ å¤ªæ…¢

ä½†ç”¨ Tensor Core `mma.sync.16x8x16`ï¼ˆå‡è®¾ï¼‰ï¼š

- æ¯æ¬¡ mma ç®— 8Ã—8Ã—16 â†’ è¾“å‡º 64 ä¸ªç»“æœ
- åªéœ€ 1 æ¬¡ mma å°±è¦†ç›–äº†æ•´ä¸ª 8Ã—8 è¾“å‡ºï¼

ä½† K å¾ˆå¤§ï¼Œæ‰€ä»¥è¦å¾ªç¯ K/16 æ¬¡ â†’ æ¯æ¬¡ mma éƒ½ç´¯åŠ åˆ°åŒä¸€ä¸ª 8Ã—8 å¯„å­˜å™¨ç¢ç‰‡

â†’ æœ€ç»ˆè¿™ä¸ªçº¿ç¨‹çš„ 64 ä¸ªå¯„å­˜å™¨å°±æ˜¯æœ€ç»ˆè¾“å‡ºï¼Œæ— éœ€é‡å¤è®¡ç®—

å›åˆ°åŸä»£ç ï¼š

- æ¯ä¸ªçº¿ç¨‹ç”¨ 32Ã—32 = 1024 ä¸ªå¯„å­˜å™¨
- ä½†å®é™…åªéœ€è¦è¾“å‡º 512 ä¸ªæœ€ç»ˆç»“æœï¼ˆå› ä¸ºæœ‰ 50% é‡å ï¼‰
- å¤šå‡ºæ¥çš„ 512 ä¸ªå¯„å­˜å™¨æ˜¯â€œç‰ºç‰²â€ç”¨æ¥åš **å¯„å­˜å™¨çº§é‡æ’ï¼ˆregister tiling + swizzlingï¼‰**ï¼Œè®© Tensor Core çš„ mma æŒ‡ä»¤èƒ½å®Œç¾å¯¹é½

### ä¸ºä»€ä¹ˆâ€œå¤¸å¼ â€åè€Œæ›´å¿«ï¼Ÿ

| ç­–ç•¥                   | ä¼ ç»Ÿåšæ³•                     | è¿™æ®µä»£ç ï¼ˆå¤¸å¼ ç‰ˆï¼‰            | æ€§èƒ½å·®è· |
|------------------------|------------------------------|-------------------------------|----------|
| æ¯ä¸ªçº¿ç¨‹è´Ÿè´£çš„ C å…ƒç´   | 8~16 ä¸ª                      | 512~1024 ä¸ª                   | -        |
| å¯„å­˜å™¨å‹åŠ›             | ä½                           | æé«˜ï¼ˆæ¥è¿‘å¯„å­˜å™¨ä¸Šé™ï¼‰        | -        |
| Tensor Core åˆ©ç”¨ç‡     | 60~80%                       | 98~100%                       | å…³é”®ï¼   |
| æŒ‡ä»¤å‘å°„æ•ˆç‡           | ä¸€èˆ¬                         | æé«˜ï¼ˆå®Œç¾å¡«æ»¡ pipelineï¼‰     | +50%     |
| æœ€ç»ˆæ€§èƒ½ï¼ˆ4090ï¼‰       | 80~100 TFLOPS                | 140~160 TFLOPS                | 1.5~2Ã—   |

### ç»ˆæç»“è®ºï¼šä¸ºä»€ä¹ˆâ€œä¸€ä¸ª warp ç®— 128Ã—128â€ä¸å¤¸å¼ ï¼Œåè€Œæ˜¯æœ€ä¼˜è§£ï¼Ÿ

å› ä¸ºç°ä»£ Tensor Coreï¼ˆå°¤å…¶æ˜¯ Ampere/Hopperï¼‰æœ‰ä¸¤å¤§ç‰¹æ€§ï¼š

1. **mma æŒ‡ä»¤æå¿«**ï¼ˆä¸€ä¸ªå‘¨æœŸå‡ å TFLOPSï¼‰
2. **å¯„å­˜å™¨æ–‡ä»¶å·¨å¤§**ï¼ˆæ¯ä¸ªçº¿ç¨‹æœ€å¤š 255 ä¸ªå¯„å­˜å™¨ï¼Œ1024 ä¸ª float æ‰ 4KBï¼Œè¿œæ²¡çˆ†ï¼‰

æ‰€ä»¥ï¼š
- è®©æ¯ä¸ªçº¿ç¨‹ç®—æ›´å¤šï¼ˆ512~1024 ä¸ªå…ƒç´ ï¼‰
- ç”¨æ›´å¤šå¯„å­˜å™¨ï¼ˆ1024 ä¸ªï¼‰
- æ¢æ¥ Tensor Core å‡ ä¹ 100% åˆ©ç”¨ç‡
- æœ€ç»ˆæ€§èƒ½ â†’ æ¥è¿‘ç†è®ºå³°å€¼

è¿™å°±åƒï¼š  
ä½ è¯·äº† 8 ä¸ªè¶…äººï¼ˆ8 ä¸ª warpï¼‰ï¼Œæ¯ä¸ªè¶…äººä¸€æ¬¡èƒ½æ¬ 1000 å—ç –ï¼ˆ1024 ä¸ªå…ƒç´ ï¼‰ï¼Œ  
ä½ ä¸è®©ä»–æ¬ 10 å—ï¼ˆä¼ ç»Ÿåšæ³•ï¼‰ï¼Œè€Œæ˜¯ç›´æ¥è®©ä»–æ¬ 1000 å—ï¼Œ  
è™½ç„¶ä»–ç´¯ï¼Œä½†æ•ˆç‡æœ€é«˜ï¼

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ 2025 å¹´çš„é¡¶çº§ GEMM ä»£ç éƒ½â€œå¤¸å¼ â€åœ°è®©ä¸€ä¸ª warp ç®— 128Ã—128ã€ä¸€ä¸ªçº¿ç¨‹ç®— 32Ã—32ï¼Œå› ä¸ºï¼š**ç¡¬ä»¶å·²ç»å¼ºåˆ°å¯ä»¥è¿™æ ·ç©äº†**ã€‚

ä½ ç°åœ¨æ˜ç™½äº†å—ï¼Ÿè¿™ä¸æ˜¯å¤¸å¼ ï¼Œè¿™æ˜¯**æè‡´æ€§èƒ½çš„å¿…ç„¶ç»“æœ**ã€‚

*/


/*
question7:

ä¸ç†è§£ï¼Œè¯·è§£é‡Š
Â 
â†’ å®ƒä¸€æ¬¡è®¡ç®— 32Ã—32Ã—16 çš„å°çŸ©é˜µä¹˜ï¼Œç»“æœç´¯åŠ åˆ° 32Ã—32 çš„ C å¯„å­˜å™¨ç¢ç‰‡

å¤ªå¥½äº†ï¼ä½ é—®åˆ°äº†æ•´ä¸ªç°ä»£é«˜æ€§èƒ½ GEMM çš„æœ€æ ¸å¿ƒã€æœ€å®¹æ˜“è¢«è¯¯è§£çš„ä¸€å¥è¯ï¼š

> â€œå®ƒä¸€æ¬¡è®¡ç®— 32Ã—32Ã—16 çš„å°çŸ©é˜µä¹˜ï¼Œç»“æœç´¯åŠ åˆ° 32Ã—32 çš„ C å¯„å­˜å™¨ç¢ç‰‡â€

æˆ‘ä»¬ç”¨æœ€ç›´ç™½ã€æœ€å½¢è±¡çš„æ–¹å¼ï¼ˆä¸€æ­¥ä¸€æ­¥ç”»å›¾ï¼‰ï¼Œå½»åº•æŠŠè¿™å¥è¯è®²é€ã€‚

### 1. å…ˆè®°ä½ NVIDIA Tensor Core çš„çœŸå®è§„æ ¼ï¼ˆ2020~2025 å¹´ä¸å˜ï¼‰

| æ¶æ„       | ä¸€æ¡ mma.sync æŒ‡ä»¤èƒ½ç®—å¤šå¤§ï¼Ÿ | è¾“å‡ºå½¢çŠ¶ | ä¸€æ¬¡èƒ½åƒå¤šå°‘è¾“å…¥ |
|------------|-------------------------------|----------|------------------|
| Ampere/Hopper | mma.sync.aligned.m16n8k32 æˆ– m16n8k16 | 16Ã—8Ã—32 æˆ– 16Ã—8Ã—16 | A: 16Ã—32 æˆ– 16Ã—16<br>B: 32Ã—8 æˆ– 16Ã—8<br>C: 16Ã—8 |

ä½†ï¼æˆ‘ä»¬ä»£ç é‡Œå†™çš„æ˜¯ï¼š

```cpp
mma<WK, TM, TN, TK>(tCrC, tArA, tBrB);
// WK=16, TM=32, TN=32, TK=8
```

è¿™æ€ä¹ˆå¯èƒ½ï¼Ÿ16Ã—32Ã—32Ã—8 çœ‹èµ·æ¥æ¯”ç¡¬ä»¶æ”¯æŒçš„å¤§å¤šäº†ï¼

â†’ ç­”æ¡ˆæ˜¯ï¼š**è¿™æ®µä»£ç æ ¹æœ¬ä¸æ˜¯ä¸€æ¬¡ mma æŒ‡ä»¤ç®— 32Ã—32Ã—16ï¼Œè€Œæ˜¯ç”¨ 8 æ¡ mma æŒ‡ä»¤ç®—å®Œ 32Ã—32Ã—16ï¼**

### 2. çœŸå®å‘ç”Ÿçš„äº‹æƒ…ï¼šç”¨ 8 æ¡ mma æ‹¼æˆ 32Ã—32Ã—16

æˆ‘ä»¬æŠŠä¸€ä¸ªçº¿ç¨‹çš„å¯„å­˜å™¨ç”»å‡ºæ¥ï¼š

```
æ¯ä¸ªçº¿ç¨‹çš„å¯„å­˜å™¨ C ç¢ç‰‡ï¼ˆtCrCï¼‰ï¼š
32 è¡Œ Ã— 32 åˆ— = 1024 ä¸ª float

C å¯„å­˜å™¨ç¢ç‰‡ï¼ˆ32Ã—32ï¼‰ï¼š
     åˆ—0   åˆ—1  ...  åˆ—31
è¡Œ0
è¡Œ1
...
è¡Œ31
```

ç°åœ¨æˆ‘ä»¬æŠŠè¿™ 32Ã—32 åˆ‡æˆ 8 ä¸ª 16Ã—8 çš„å°å—ï¼ˆæ­£å¥½æ˜¯ Tensor Core åŸç”Ÿæ”¯æŒçš„ï¼‰ï¼š

```
+----------------+----------------+
|   16Ã—8 å°å— 0  |   16Ã—8 å°å— 1  |
+----------------+----------------+
|   16Ã—8 å°å— 2  |   16Ã—8 å°å— 3  |
+----------------+----------------+
|   16Ã—8 å°å— 4  |   16Ã—8 å°å— 5  |
+----------------+----------------+
|   16Ã—8 å°å— 6  |   16Ã—8 å°å— 7  |
+----------------+----------------+
â†‘ 32 è¡Œ                â†‘ 32 åˆ—
```

æ¯ä¸€æ¬¡ `mma<16, 32, 32, 8>` å®é™…ä¸Šåšçš„æ˜¯ï¼š

1. ä» A å¯„å­˜å™¨å– 16 è¡Œ Ã— 8 åˆ—ï¼ˆTK=8ï¼‰
2. ä» B å¯„å­˜å™¨å– 8 è¡Œ Ã— 32 åˆ—
3. è®¡ç®— 16Ã—32Ã—8 çš„å°çŸ©é˜µä¹˜
4. æŠŠç»“æœç´¯åŠ åˆ° C çš„ 8 ä¸ª 16Ã—8 å°å—ä¸­çš„æŸä¸€ä¸ª

ä¸ï¼Œé¡ºåºæ˜¯ï¼š

```cpp
// ä¼ªä»£ç ï¼ŒçœŸå®æ±‡ç¼–æ˜¯è¿™æ ·çš„ï¼š
mma.sync.aligned.m16n8k8  fragC[%0], fragA[%0], fragB[%0], fragC[%0];   // å°å—0
mma.sync.aligned.m16n8k8  fragC[%1], fragA[%0], fragB[%1], fragC[%1];   // å°å—1
mma.sync.aligned.m16n8k8  fragC[%2], fragA[%1], fragB[%0], fragC[%2];   // å°å—2
...
```

æ‰€ä»¥ï¼š

- **ä¸€æ¬¡ mma æŒ‡ä»¤**ï¼šç®— 16Ã—8Ã—8 â†’ è¾“å‡º 16Ã—8 çš„ C ç¢ç‰‡
- **8 æ¬¡ mma æŒ‡ä»¤**ï¼šç®—å®Œ 16Ã—64Ã—8 æˆ– 32Ã—32Ã—8 ç­‰ç»„åˆ
- **åœ¨ K ç»´åº¦å¾ªç¯æ—¶**ï¼šæ¯æ¬¡ wk_id å¾ªç¯ï¼Œå¤„ç† 16 ä¸ª Kï¼Œç´¯åŠ åˆ°åŒä¸€ä¸ª 32Ã—32 çš„ C å¯„å­˜å™¨

### 3. ç”¨ä¸€ä¸ªè¶…çº§ç®€å•çš„ä¾‹å­å½»åº•è¯´æ˜

å‡è®¾æˆ‘ä»¬æŠŠæ‰€æœ‰å‚æ•°ç¼©å° 4 å€ï¼ˆæ›´å®¹æ˜“ç”»ï¼‰ï¼š

```cpp
BM=32, BN=32, BK=16
WM=32, WN=32
TM=8,  TN=8
WK=16, TK=4
```

ä¸€ä¸ªçº¿ç¨‹è´Ÿè´£ 8Ã—8=64 ä¸ª C å…ƒç´ 

```cpp
float tCrC[8*8];    // 64 ä¸ªå¯„å­˜å™¨
float tArA[16*8];   // A ç¢ç‰‡ 16Ã—8
float tBrB[16*8];   // B ç¢ç‰‡ 16Ã—8
```

åœ¨ä¸€æ¬¡ `wk_id` å¾ªç¯é‡Œï¼š

```cpp
load_Shared_To_Reg(...)        // åŠ è½½ 16Ã—8 çš„ A å’Œ B åˆ°å¯„å­˜å™¨
mma<16, 8, 8, 4>(tCrC, tArA, tBrB);   // â† è¿™å¥ä¸æ˜¯ä¸€æ¡æŒ‡ä»¤ï¼
```

å®é™…ä¸Šç¼–è¯‘å™¨ä¼šå±•å¼€æˆ 4 æ¡ mma æŒ‡ä»¤ï¼š

```asm
// çœŸå®æ±‡ç¼–å¤§æ¦‚é•¿è¿™æ ·ï¼ˆä¼ªä»£ç ï¼‰ï¼š
mma.sync ...  C[0:15, 0:7]   += A[0:15, 0:3]   * B[0:3,  0:7]
mma.sync ...  C[0:15, 8:15]  += A[0:15, 0:3]   * B[0:3,  8:15]
mma.sync ...  C[16:31,0:7]   += A[0:15, 4:7]   * B[4:7,  0:7]
mma.sync ...  C[16:31,8:15]  += A[0:15, 4:7]   * B[4:7,  8:15]
```

â†’ 4 æ¡ mma æŒ‡ä»¤åˆšå¥½è¦†ç›–äº† 32Ã—16 çš„ä¸­é—´ç»“æœï¼Ÿä¸ï¼Œç¼©å°ç‰ˆæ˜¯ 8Ã—8

å…³é”®æ˜¯ï¼š

**â€œä¸€æ¬¡ mma<WK, TM, TN, TK>â€ åªæ˜¯æ¨¡æ¿ä½œè€…çš„æŠ½è±¡è¯´æ³•**  
**çœŸå®ç¡¬ä»¶æ˜¯ä¸€æ¬¡åªèƒ½ç®— 16Ã—8Ã—8 æˆ– 16Ã—8Ã—16**  
**æ¨¡æ¿é€šè¿‡å¾ªç¯ + å¯„å­˜å™¨é‡æ’ï¼Œè‡ªåŠ¨æŠŠ 8Ã—8ã€16Ã—16ã€32Ã—32 çš„è®¡ç®—æ‹†æˆå¤šæ¡åŸç”Ÿ mma æŒ‡ä»¤**

### æœ€ç»ˆç»“è®ºï¼ˆä¸€å¥è¯è®°ä½ï¼‰

> â€œmma<16, 32, 32, 8>â€ ä¸æ˜¯ä¸€æ¡æŒ‡ä»¤ï¼Œè€Œæ˜¯ä¸€ç»„ï¼ˆé€šå¸¸ 8 æ¡ï¼‰Tensor Core mma æŒ‡ä»¤çš„ç»„åˆï¼Œå®ƒä»¬å…±åŒæŠŠ 32Ã—32Ã—16 çš„å°çŸ©é˜µä¹˜ç»“æœç´¯åŠ åˆ°çº¿ç¨‹çš„ 32Ã—32 å¯„å­˜å™¨ C ç¢ç‰‡ä¸­ã€‚

å°±åƒä½ å®¶æœ‰ 8 ä¸ªå·¥äººï¼ˆ8 æ¡ mma æŒ‡ä»¤ï¼‰ï¼Œä¸€èµ·æ¬ä¸€ä¸ªå¤§ç®±å­ï¼ˆ32Ã—32Ã—16 çš„è®¡ç®—ï¼‰ï¼Œä½ è¯´â€œä»–ä»¬ä¸€æ¬¡æ¬å®Œäº†â€ï¼Œå…¶å®æ˜¯ 8 ä¸ªäººä¸€èµ·æŠ¬çš„ã€‚

ç°åœ¨ä½ å½»åº•æ˜ç™½äº†å§ï¼Ÿè¿™ä¸æ˜¯ç¡¬ä»¶ä¸€æ¬¡èƒ½ç®— 32Ã—32Ã—16ï¼Œè€Œæ˜¯**è½¯ä»¶ç”¨ 8 æ¡åŸç”ŸæŒ‡ä»¤æ‹¼å‡ºäº† 32Ã—32Ã—16 çš„æ•ˆæœ**ï¼Œä»è€Œå®ç°æè‡´æ€§èƒ½ã€‚

*/




/*
question8:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

template <int BM, int BN, int BK, int WM, int WN, int WK, int TM, int TN, int TK,Â 
Â  Â  Â  Â  Â  Â  int cta_size, typename vec_t>
__global__ __launch_bounds__(cta_size)
void sgemm_Kernel_Universal_Pipeline_TT(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int M, int N, int K) {



è¿™æ˜¯ä¸€ä¸ªé«˜åº¦ä¼˜åŒ–çš„ **SGEMMï¼ˆå•ç²¾åº¦çŸ©é˜µä¹˜æ³•ï¼‰CUDA Kernel**ï¼Œå®ƒå®ç°äº† **åˆ†å±‚ Tilingï¼ˆåˆ†å—ï¼‰** å’Œ **åŒç¼“å†²æµæ°´çº¿ï¼ˆDouble-Buffering Pipeliningï¼‰** æŠ€æœ¯ï¼Œæ—¨åœ¨æœ€å¤§åŒ– GPU çš„è®¡ç®—æ•ˆç‡ã€‚

è¿™ä¸ª Kernel çš„ç›®æ ‡æ˜¯è®¡ç®— C = A * B çš„ä¸€ä¸ªå¤§åˆ†å—ï¼Œå¹¶å°†å†…å­˜è®¿é—®å»¶è¿Ÿéšè—åœ¨è®¡ç®—ä¹‹åã€‚

-----

## âš™ï¸ I. å‡†å¤‡å’Œåˆå§‹åŒ–

è¯¥ Kernel çš„è®¾è®¡åŸºäºå¤šå±‚çº§çš„åˆ†å—ï¼ˆBlock -> Warp -> Thread -> Registerï¼‰ã€‚

### 1\. æ¨¡æ¿å‚æ•°å’Œåˆ†å—çº§åˆ«

Kernel çš„å¤§é‡æ¨¡æ¿å‚æ•°å®šä¹‰äº†åˆ†å—çš„å‡ ä½•å½¢çŠ¶ã€‚æˆ‘ä»¬ä½¿ç”¨è¿™äº›æ¨¡æ¿å‚æ•°çš„å…¸å‹å€¼æ¥ç†è§£å®ƒä»¬ï¼š

| å‚æ•° | ç¤ºä¾‹å€¼ | å«ä¹‰ | ä½œç”¨èŒƒå›´ |
| :--- | :--- | :--- | :--- |
| **BM, BN, BK** | 256, 128, 16 | Block-Level Tile å°ºå¯¸ï¼ˆçº¿ç¨‹å—ï¼‰ | C_{Block \approx 256 * 128 |
| **WM, WN, WK** | 64, 32, 8 | Warp-Level Tile å°ºå¯¸ï¼ˆWarpï¼‰ | C_{Warp = 64 * 32 |
| **TM, TN, TK** | 8, 8, 1 | Thread-Level Tile å°ºå¯¸ï¼ˆå¯„å­˜å™¨ï¼‰ | C_{Thread = 8 * 8 |

### 2\. çº¿ç¨‹ ID å’ŒçŸ©é˜µåç§»

```c
int tid = threadIdx.y * blockDim.x + threadIdx.x; // çº¿ç¨‹çš„å…¨å±€ID
int batch_id = blockIdx.z;

A += batch_id * M * K; // ...
```

  * **Batching:** å¦‚æœæ˜¯æ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼ˆBMMï¼‰ï¼Œ`blockIdx.z` ç”¨äºå°†æŒ‡é’ˆ A, B, C åç§»åˆ°å½“å‰æ‰¹æ¬¡ (batch) å¯¹åº”çš„çŸ©é˜µèµ·å§‹åœ°å€ã€‚
  * **Block å®šä½:** bmid (blockIdx.y) å’Œ bnid (blockIdx.x) å®šä½äº†å½“å‰çº¿ç¨‹å—åœ¨ C çŸ©é˜µä¸­è¦è®¡ç®—çš„ BM * BN å¤§ Tile çš„ä½ç½®ã€‚

### 3\. å…±äº«å†…å­˜åŒç¼“å†²

```c
extern __shared__ float smem[];
float* sA[2] = {smem, smem+BK*BM;
float* sB[2] = {smem+2*BK*BM, smem+2*BK*BM+BK*BN;
```

  * **ç›®çš„:** **æµæ°´çº¿ä¼˜åŒ–**ã€‚å…±äº«å†…å­˜ (`smem`) è¢«åˆ’åˆ†ä¸ºå››ä¸ªåŒºåŸŸï¼š`sA[0]`, `sA[1]` ç”¨äº A çŸ©é˜µçš„ Tileï¼Œ`sB[0]`, `sB[1]` ç”¨äº B çŸ©é˜µçš„ Tileã€‚
  * **`sA[0]` / `sA[1]` ä¸ºä»€ä¹ˆåç§» BK * BMï¼Ÿ** BK * BM æ˜¯ A çŸ©é˜µä¸€ä¸ª Tile çš„å¤§å°ï¼Œä¹˜æ³•æ˜¯ C_{M * N = A_{M * K * B_{K * Nï¼Œæ‰€ä»¥ A çš„ Tile å¤§å°æ˜¯ BM * BKã€‚
  * **`curr_buffer_id`:** æ§åˆ¶å½“å‰æ­£åœ¨ä½¿ç”¨çš„ç¼“å†²åŒº (0 æˆ– 1)ã€‚

### 4\. çº¿ç¨‹åˆ†å±‚ç´¢å¼•

ä»£ç å°†çº¿ç¨‹ ID (`tid`) åˆ†è§£ä¸ºå¤šå±‚æ¬¡çš„ IDï¼Œä»¥ç²¾ç¡®æ§åˆ¶æ•°æ®è®¿é—®ï¼š

  * **Warp çº§åˆ« (`wm_id`, `wn_id`):** ç¡®å®šå½“å‰ Warp è´Ÿè´£çš„ WM * WN å­åŒºåŸŸã€‚
  * **Thread çº§åˆ« (`tm_vec_id`, `tn_vec_id`):** ç¡®å®šå½“å‰çº¿ç¨‹è´Ÿè´£çš„æœ€å° TM * TN åŒºåŸŸåœ¨ Warp åŒºåŸŸå†…çš„ä½ç½®ã€‚

### 5\. å¯„å­˜å™¨åˆå§‹åŒ–

```c
float tArA[WK*TM];
float tBrB[WK*TN];
float tCrC[TM*TN] = {0.0f;
```

  * **ç›®çš„:** åœ¨å‡½æ•°å¼€å§‹æ—¶ï¼Œæ¯ä¸ªçº¿ç¨‹å£°æ˜å¹¶åˆå§‹åŒ–äº†ç”¨äºè®¡ç®—å’Œç´¯ç§¯çš„**ç§æœ‰å¯„å­˜å™¨æ•°ç»„**ã€‚tCrC åˆå§‹åŒ–ä¸º 0.0fï¼Œç”¨äºç´¯ç§¯ä¹˜æ³•ç»“æœã€‚

-----

## ğŸƒ II. æµæ°´çº¿æ‰§è¡Œå¾ªç¯

Kernel çš„æ ¸å¿ƒåœ¨äºä¸€ä¸ªä¸»å¾ªç¯ï¼Œå®ƒè¿­ä»£ K ç»´åº¦ä¸Šçš„åˆ†å—ã€‚

### 1\. é¢„åŠ è½½ (Prologue)

```c
// load
load_Global_To_Shared<...>(sA[curr_buffer_id], tAgA, ...);
load_Global_To_Shared<...>(sB[curr_buffer_id], tBgB, ...);
__syncthreads();
```

  * **ç›®çš„:** åœ¨ä¸»å¾ªç¯å¼€å§‹å‰ï¼Œçº¿ç¨‹åä½œå°†ç¬¬ä¸€å¯¹ A å’Œ B çš„ Tile åŠ è½½åˆ° sA[0] å’Œ sB[0]ã€‚
  * **`__syncthreads()`:** ç¡®ä¿æ‰€æœ‰çº¿ç¨‹éƒ½å®Œæˆäº†åŠ è½½ï¼Œæ‰èƒ½å¼€å§‹è®¡ç®—ã€‚

### 2\. æµæ°´çº¿ä¸»å¾ªç¯

```c
for(; bkid<K/BK-1; ++bkid) {
    auto next_buffer_id = 1^curr_buffer_id; // åˆ‡æ¢åˆ° 1 æˆ– 0
    // Load next tile into the *other* buffer
    load_Global_To_Shared<...>(sA[next_buffer_id], tAgA, ...);
    load_Global_To_Shared<...>(sB[next_buffer_id], tBgB, ...);
    
    #pragma unroll
    for(int wk_id=0; wk_id<BK/WK; ++wk_id) {
        // Compute using current buffer
        load_Shared_To_Reg<...>(tArA, tBrB, sA[curr_buffer_id], sB[curr_buffer_id], ...);
        mma<...>(tCrC, tArA, tBrB);
    
    __syncthreads();
    curr_buffer_id ^= 1; // åˆ‡æ¢ç¼“å†²åŒºï¼Œå¼€å§‹è®¡ç®—ä¸‹ä¸€ç»„ Tile

```

  * **æµæ°´çº¿:** åœ¨ K ç»´åº¦çš„æ¯ä¸ªæ­¥éª¤ä¸­ï¼š
    1.  **åŠ è½½ä¸‹ä¸€å— (Load):** å°†ä¸‹ä¸€å¯¹ Tile åŠ è½½åˆ° **ç©ºé—²** çš„ç¼“å†²åŒº (`next_buffer_id`)ã€‚
    2.  **è®¡ç®—å½“å‰å— (Compute):** ä½¿ç”¨å½“å‰ç¼“å†²åŒº (`curr_buffer_id`) ä¸­çš„æ•°æ®æ‰§è¡Œ MMA ä¹˜åŠ æ“ä½œã€‚
    <!-- end list -->
      * **ç›®çš„:** å°†**è®¡ç®—æ—¶é—´**éšè—åœ¨**å†…å­˜è®¿é—®æ—¶é—´**ä¹‹åï¼Œä»è€Œæé«˜æ•ˆç‡ã€‚

### 3\. å°¾éƒ¨è®¡ç®— (Epilogue)

```c
// ... (æœ€åçš„ MMA å¾ªç¯) ...
// store
store_Reg_To_Global<...>(tCgC, tCrC, ldc, rest_m, rest_n, ...);
```

  * **ç›®çš„:** ä¸»å¾ªç¯ç»“æŸåï¼Œä»æœ‰ä¸€ä¸ª Tile åœ¨ Shared Memory ä¸­ç­‰å¾…è®¡ç®—ã€‚è¿™æ®µä»£ç æ‰§è¡Œæœ€åä¸€ç»„ MMA ä¹˜åŠ æ“ä½œã€‚
  * **å­˜å‚¨:** æœ€åï¼Œ`store_Reg_To_Global` å°†å¯„å­˜å™¨ä¸­ç´¯ç§¯çš„æœ€ç»ˆç»“æœ tCrC å†™å›å…¨å±€å†…å­˜ä¸­çš„ C çŸ©é˜µçš„å¯¹åº” Tileï¼ŒåŒæ—¶å¤„ç†çŸ©é˜µè¾¹ç¼˜çš„è¾¹ç•Œæ¡ä»¶ã€‚

-----

## ğŸ¯ æ€»ç»“

è¿™ä¸ª Kernel æ˜¯ä¸€ä¸ªé«˜åº¦ä¼˜åŒ–çš„ SGEMM å®ç°ï¼Œå®ƒé€šè¿‡ **Tiling** é™ä½äº†å†…å­˜å»¶è¿Ÿï¼Œé€šè¿‡ **Pipelining** éšè—äº† Global Memory å»¶è¿Ÿï¼Œå¹¶é€šè¿‡ **å¯„å­˜å™¨çº§ MMA** å®ç°äº†è®¡ç®—é€Ÿåº¦æœ€å¤§åŒ–ã€‚

*/


/*
question9:

float* sA[2] = {smem, smem+BK*BM;
    float* sB[2] = {smem+2*BK*BM, smem+2*BK*BM+BK*BN;
Â 
ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

å¤ªå¥½äº†ï¼ä½ é—®åˆ°çš„æ˜¯æ‰€æœ‰é«˜æ€§èƒ½ GEMM ä»£ç é‡Œ**æœ€å…³é”®ã€æœ€å®¹æ˜“çœ‹èŠ±çœ¼**çš„ä¸€è¡Œï¼š

```cpp
float* sA[2] = {smem, smem + BK*BM;
float* sB[2] = {smem + 2*BK*BM, smem + 2*BK*BM + BK*BN;
```

æˆ‘ä»¬ç”¨ä½ ä»£ç é‡Œæœ€å¸¸è§çš„å‚æ•°ï¼Œä¸€æ­¥ä¸€æ­¥ç”»å›¾ + è®¡ç®—åœ°å€ï¼Œå½»åº•è®²æ¸…æ¥šã€‚

### å®é™…ä¾‹å­ï¼ˆæœ€å¸¸ç”¨çš„å·¥ä¸šçº§å‚æ•°ï¼‰

```cpp
const int BM = 128;
const int BN = 128;
const int BK = 16;
```

â†’ ä¸€ä¸ª block è¦ç®— 128Ã—128 çš„ C tile  
â†’ æ¯æ¬¡ä» K ç»´åº¦åˆ‡ 16 åˆ—å‡ºæ¥ç®—

### å…±äº«å†…å­˜æ€»å…±éœ€è¦å¤šå¤§ï¼Ÿ

æˆ‘ä»¬éœ€è¦æ”¾ï¼š
- 2 ä»½ A çš„ tileï¼š128Ã—16ï¼ˆåŒç¼“å†²ï¼‰
- 2 ä»½ B çš„ tileï¼š16Ã—128ï¼ˆåŒç¼“å†²ï¼‰

è®¡ç®—å­—èŠ‚æ•°ï¼š

```text
A tile å¤§å° = 128 Ã— 16 Ã— 4 bytes = 8192 bytes
B tile å¤§å° = 16 Ã— 128 Ã— 4 bytes = 8192 bytes
æ€»å…±éœ€è¦ = 2Ã—8192 + 2Ã—8192 = 32768 bytes = 32 KB
```

æ­£å¥½æ˜¯ Ampere/Hopper æ¯ä¸ª block æœ€å¤šèƒ½ç”¨çš„å…±äº«å†…å­˜ä¸Šé™ï¼

### ç°åœ¨çœ‹ smem çš„çœŸå®å¸ƒå±€ï¼ˆç”»å›¾ç‰ˆï¼‰

```text
extern __shared__ float smem[];   // æ€»å…± 32768 bytes = 8192 ä¸ª float

åœ°å€ï¼ˆå•ä½ï¼šfloat æ•°é‡ï¼‰ï¼š
0                                     â† smem åŸºåœ°å€
â”‚                                     
â”‚     sA[0]      â† å½“å‰ç”¨çš„ A buffer (128Ã—16 = 2048 float)
â”‚   +----------+
â”‚   â”‚  A tile  â”‚  â† 2048 ä¸ª float
â”‚   +----------+
2048                                  â† smem + BK*BM = smem + 2048
â”‚                                     
â”‚     sA[1]      â† é¢„å–ç”¨çš„ A buffer
â”‚   +----------+
â”‚   â”‚  A tile  â”‚
â”‚   +----------+
4096                                  â† smem + 2*BK*BM = smem + 4096
â”‚                                     
â”‚     sB[0]      â† å½“å‰ç”¨çš„ B buffer (16Ã—128 = 2048 float)
â”‚   +----------+
â”‚   â”‚  B tile  â”‚
â”‚   +----------+
6144                                  â† smem + 2*BK*BM + BK*BN = smem + 6144
â”‚                                     
â”‚     sB[1]      â† é¢„å–ç”¨çš„ B buffer
â”‚   +----------+
â”‚   â”‚  B tile  â”‚
â”‚   +----------+
8192                                  â† ç»“æŸï¼Œæ•´ä¸ª smem ç”¨å®Œ
```

### å¯¹åº”åˆ°ä»£ç é‡Œå°±æ˜¯ï¼š

```cpp
float* sA[2] = {smem,                               // buffer 0: åœ°å€ 0
                smem + BK*BM;                      // buffer 1: åœ°å€ 2048
                // BK*BM = 16*128 = 2048

float* sB[2] = {smem + 2*BK*BM,                     // buffer 0: åœ°å€ 4096
                smem + 2*BK*BM + BK*BN;            // buffer 1: åœ°å€ 6144
                // 2*BK*BM = 4096
                // BK*BN = 16*128 = 2048
```

### ä¸ºä»€ä¹ˆè¿™æ ·æ’å¸ƒï¼ŸåŒç¼“å†²ï¼ˆPing-Pongï¼‰åŸç†

```cpp
int curr_buffer_id = 0;

// ç¬¬ä¸€æ­¥ï¼šåŠ è½½åˆ° buffer 0
load_Global_To_Shared(sA[0], ...);   // å†™ smem + 0
load_Global_To_Shared(sB[0], ...);   // å†™ smem + 4096

__syncthreads();

// ä¸»å¾ªç¯
for (...) {
    int next = 1 - curr_buffer_id;

    // å¼‚æ­¥é¢„å–ä¸‹ä¸€å—åˆ° buffer 1
    load_Global_To_Shared(sA[next], ...);  // å†™ smem + 2048 æˆ– 0
    load_Global_To_Shared(sB[next], ...);  // å†™ smem + 6144 æˆ– 4096

    // ç”¨å½“å‰ buffer è®¡ç®—
    compute_with(sA[curr], sB[curr]);

    __syncthreads();
    curr_buffer_id = next;

```

â†’ è®¡ç®—å’Œ global memory åŠ è½½å®Œå…¨é‡å  â†’ éšè—å»¶è¿Ÿ â†’ æ€§èƒ½æ¥è¿‘ç†è®ºå³°å€¼

### ç»ˆæè®°å¿†å£è¯€ï¼ˆä¸€å¥è¯è®°ä½ï¼‰

```text
smem å¸ƒå±€æ°¸è¿œæ˜¯ï¼š
[ A0 ][ A1 ][ B0 ][ B1 ]
   â†‘     â†‘     â†‘     â†‘
   0   BK*BM  2*BK*BM  2*BK*BM + BK*BN
```

æ‰€æœ‰é¡¶çº§ GEMM ä»£ç ï¼ˆåŒ…æ‹¬ NVIDIA å®˜æ–¹ã€Tencent ncnnã€xformers ç­‰ï¼‰å…±äº«å†…å­˜éƒ½æ˜¯è¿™ä¹ˆæ’çš„ï¼

ç°åœ¨ä½ çœ‹è¿™å››è¡Œä»£ç ï¼Œæ˜¯ä¸æ˜¯ç¬é—´å°±æ‡‚äº†ï¼Ÿ

```cpp
float* sA[2] = {smem,           smem + BK*BM;        // A çš„ä¸¤ä¸ª buffer
float* sB[2] = {smem+2*BK*BM,   smem+2*BK*BM + BK*BN; // B çš„ä¸¤ä¸ª buffer
```

å°±æ˜¯æŠŠ 32KB å…±äº«å†…å­˜åˆ‡æˆå››å—ï¼šA0ã€A1ã€B0ã€B1ï¼Œå®Œç¾åŒç¼“å†²ï¼Œé›¶æµªè´¹ï¼

*/


/*
question10:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­
Â 
#define local_tile(ptr, tileM, tileN, row, col, ld) ((ptr) + (row)*(tileM)*(ld) + (col)*(tileN))
Â 
Â 
    auto* gA = A;
    auto* gB = B;
    auto* gC = C;
    const int lda = K;
    const int ldb = N;
    const int ldc = N;
    auto* tCgC = local_tile(gC, BM, BN, bmid, bnid, ldc);


å¤ªå¥½äº†ï¼ä½ é—®çš„è¿™ä¸ªå®æ˜¯æ‰€æœ‰é«˜æ€§èƒ½ GEMM ä»£ç é‡Œ**æœ€æœ€æœ€æœ€æœ€å¸¸è§ã€ä¹Ÿæœ€å®¹æ˜“çœ‹æ™•**çš„ä¸€è¡Œï¼š

```cpp
#define local_tile(ptr, tileM, tileN, row, col, ld) \
    ((ptr) + (row)*(tileM)*(ld) + (col)*(tileN))
```

æˆ‘ä»¬ç”¨ä¸€ä¸ª**è¶…çº§ç®€å•ã€ç”»å›¾ç‰ˆçš„ä¾‹å­**ï¼Œå½»åº•è®²æ¸…æ¥šå®ƒåˆ°åº•åœ¨å¹²å˜›ã€‚

### 1. å…ˆçœ‹ä¸€ä¸ªçœŸå®çš„å¤§çŸ©é˜µé•¿ä»€ä¹ˆæ ·ï¼ˆä»¥ C çŸ©é˜µä¸ºä¾‹ï¼‰

å‡è®¾ä½ è¦ç®— `C = A @ B`ï¼Œå½¢çŠ¶æ˜¯ `M=1024, N=1024, K=1024`ï¼ŒæŒ‰ row-major å­˜æ”¾ï¼ˆCUDA é»˜è®¤ï¼‰ï¼š

```text
C çŸ©é˜µåœ¨å†…å­˜é‡Œé•¿è¿™æ ·ï¼ˆæ¯è¡Œ N=1024 ä¸ª floatï¼‰ï¼š
åœ°å€ 0         â†’ C[0,0]   C[0,1]   ... C[0,1023]
åœ°å€ 4096      â†’ C[1,0]   C[1,1]   ... C[1,1023]
...
åœ°å€ 4096*127  â†’ C[127,0] C[127,1] ... C[127,1023]   â† ç¬¬ 128 è¡Œå¼€å§‹
åœ°å€ 4096*128  â†’ C[128,0] C[128,1] ... C[128,1023]   â† block 1 çš„ C tile
...
```

æˆ‘ä»¬ç”¨æœ€å¸¸è§çš„å‚æ•°ï¼š
```cpp
BM = 128, BN = 128
blockIdx.y = bmid = 1    // ç¬¬ 1 ä¸ª block åœ¨ M æ–¹å‘
blockIdx.x = bnid = 2    // ç¬¬ 2 ä¸ª block åœ¨ N æ–¹å‘
ldc = N = 1024
```

### 2. è¿™ä¸ª block è¦ç®—å“ªä¸ª 128Ã—128 çš„ C å­å—ï¼Ÿ

ç­”æ¡ˆæ˜¯ï¼š
- M æ–¹å‘ï¼šä»ç¬¬ `1Ã—128 = 128` è¡Œå¼€å§‹
- N æ–¹å‘ï¼šä»ç¬¬ `2Ã—128 = 256` åˆ—å¼€å§‹

ä¹Ÿå°±æ˜¯è¿™ä¸ªçº¢è‰²æ–¹æ¡†ï¼š

```text
C çŸ©é˜µçš„ 128Ã—128 å­å—ï¼ˆæˆ‘ä»¬è¦ç®—çš„ï¼‰ï¼š
C[128, 256]  C[128, 257] ... C[128, 383]
C[129, 256]  C[129, 257] ... C[129, 383]
...
C[255, 256]  C[255, 257] ... C[255, 383]
```

### 3. ç°åœ¨ç”¨ local_tile å®ç®—å‡ºè¿™ä¸ªå­å—çš„é¦–åœ°å€

```cpp
auto* tCgC = local_tile(gC, BM, BN, bmid, bnid, ldc);
// å±•å¼€åå°±æ˜¯ï¼š
tCgC = gC + (bmid)*(128)*1024 + (bnid)*(128);
     = gC + 1*128*1024     + 2*128;
     = gC + 131072         + 256;
     = gC + 131328;
```

â†’ å®ƒç›´æ¥è·³åˆ° `C[128, 256]` çš„ä½ç½®ï¼

### 4. ç”»å›¾éªŒè¯ï¼ˆè¶…çº§æ¸…æ™°ç‰ˆï¼‰

```text
gC + 0           â†’ C[0,0]
gC + 131072      â†’ C[128, 0]     â† bmid*BM*ldc è·³äº† 128 è¡Œ
gC + 131072+256  â†’ C[128, 256]   â† å†å¾€å³è·³ 256 åˆ—ï¼ˆbnid*BNï¼‰
                     â†‘
                  è¿™å°±æ˜¯ tCgC æŒ‡å‘çš„åœ°æ–¹ï¼
```

### 5. å†çœ‹ A å’Œ B çš„ä¾‹å­ï¼ˆå®Œå…¨ä¸€æ ·ï¼‰

```cpp
auto* tAgA = local_tile(gA, BM, BK, bmid, bkid, lda);
// ä¾‹å­ï¼šbmid=1, bkid=3, BK=16, lda=K=1024
= gA + 1*128*1024 + 3*16
= gA + 131072 + 48
â†’ æŒ‡å‘ A[128:255, 48:63] è¿™å— 128Ã—16 çš„ tile
```

### ç»ˆæè®°å¿†å£è¯€ï¼ˆä¸€å¥è¯è®°ä½ï¼‰

```cpp
local_tile(ptr, tileM, tileN, row_id, col_id, ld) 
= â€œè·³è¿‡ row_id æ•´å—å¤§è¡Œâ€ + â€œåœ¨å½“å‰è¡Œè·³è¿‡ col_id æ•´å—å¤§åˆ—â€
= ptr + row_id * (tileM * ld) + col_id * (tileN)
```

ç­‰ä»·äºï¼š
```cpp
ptr[row_id * tileM : row_id * tileM + tileM,
    col_id * tileN : col_id * tileN + tileN]
```

### æ‰€æœ‰é«˜æ€§èƒ½ GEMM éƒ½é å®ƒï¼

| ä»£ç å†™æ³•                         | å®é™…å«ä¹‰                                     |
|----------------------------------|----------------------------------------------|
| `local_tile(C, 128, 128, 1, 2, 1024)` | C çš„ç¬¬ 1Ã—2 ä¸ª 128Ã—128 å­å—ï¼ˆä» [128,256] å¼€å§‹ï¼‰ |
| `local_tile(A, 128, 16, 1, 3, 1024)`  | A çš„ç¬¬ 1 è¡Œå—ã€ç¬¬ 3 ä¸ª K å—ï¼ˆ128Ã—16ï¼‰        |

ç°åœ¨ä½ å†çœ‹è¿™è¡Œä»£ç ï¼š

```cpp
auto* tCgC = local_tile(gC, BM, BN, bmid, bnid, ldc);
```

æ˜¯ä¸æ˜¯ç¬é—´å°±æ‡‚äº†ï¼Ÿ

å®ƒå°±æ˜¯ï¼š**â€œç»™æˆ‘æ‰¾åˆ°å½“å‰ block è¦è´Ÿè´£çš„é‚£ä¸ª 128Ã—128 çš„ C å­çŸ©é˜µçš„é¦–åœ°å€â€**ï¼Œä¸€å¥è¯çš„äº‹ï¼

è¿™è¡Œå®åœ¨æ‰€æœ‰é¡¶çº§ GEMMï¼ˆcuBLASã€xformersã€Tritonã€FlashAttention ç­‰ï¼‰é‡Œéƒ½å‡ºç°è¿‡ 10000 æ¬¡ä»¥ä¸Šï¼Œæ˜¯ CUDA çŸ©é˜µè®¡ç®—çš„â€œçµé­‚ä¸€è¡Œâ€ã€‚ç°åœ¨ä½ å½»åº•æŒæ¡äº†ï¼


*/


/*
question11:

please explain this, using examples
Â 
    constexpr int NWarps_dim_N = (BN/WN);
    constexpr int NWarps_dim_M = (BM/WM);
    static_assert((NWarps_dim_N & (NWarps_dim_N-1)) == 0);
    static_assert((NWarps_dim_M & (NWarps_dim_M-1)) == 0);

å¤ªå¥½äº†ï¼ä½ é—®çš„è¿™å››è¡Œä»£ç æ˜¯æ‰€æœ‰**é¡¶çº§æ‰‹å†™ Tensor Core GEMM**é‡Œ**ç»å¯¹ä¸èƒ½é”™ã€ä¹Ÿæœ€å®¹æ˜“è¢«å¿½ç•¥**çš„â€œé»‘é­”æ³•â€ï¼š

```cpp
constexpr int NWarps_dim_N = (BN / WN);
constexpr int NWarps_dim_M = (BM / WM);
static_assert((NWarps_dim_N & (NWarps_dim_N-1)) == 0);
static_assert((NWarps_dim_M & (NWarps_dim_M-1)) == 0);
```

### ä¸€å¥è¯æ€»ç»“å®ƒçš„ä½œç”¨ï¼š
**â€œæˆ‘è¦æŠŠä¸€ä¸ª block é‡Œçš„æ‰€æœ‰ warp æ’æˆä¸€ä¸ª MÃ—N çš„äºŒç»´ç½‘æ ¼ï¼Œè¿™ä¸ªç½‘æ ¼çš„é•¿å’Œå®½å¿…é¡»æ˜¯ 2 çš„æ•´æ•°æ¬¡å¹‚ï¼â€**

ä¸‹é¢ç”¨çœŸå®ä¾‹å­ + ç”»å›¾ï¼Œå½»åº•è®²æ¸…æ¥šä¸ºä»€ä¹ˆå¿…é¡»è¿™æ ·ã€‚

### çœŸå®å·¥ä¸šçº§ä¾‹å­ï¼ˆæœ€å¸¸ç”¨çš„ä¸€å¥—å‚æ•°ï¼‰

```cpp
BM = 128, BN = 128          // block è®¡ç®— 128Ã—128 çš„ C
WM = 128, WN = 128          // æ¯ä¸ª warp è®¡ç®— 128Ã—128 çš„å­å—ï¼ˆå¤¸å¼ ä½†åˆæ³•ï¼‰
cta_size = 256              // block æœ‰ 256 çº¿ç¨‹ â†’ 8 ä¸ª warp
```

ä»£å…¥å…¬å¼ï¼š

```cpp
NWarps_dim_N = BN / WN = 128 / 128 = 1
NWarps_dim_M = BM / WM = 128 / 128 = 1
```

â†’ åªæœ‰ 1Ã—1 = 1 ä¸ª warpï¼Ÿä¸å¯¹ï¼æˆ‘ä»¬æ˜æ˜æœ‰ 8 ä¸ª warpï¼

é”™ï¼è¿™å¥—å‚æ•°ä¸åˆæ³•ã€‚æˆ‘ä»¬æ¢ä¸€å¥—çœŸæ­£åˆæ³•çš„ï¼š

### æ­£ç¡®ã€å¸¸è§çš„å‚æ•°ç»„åˆï¼ˆæ¥è‡ªçœŸå®é¡¹ç›®ï¼‰

| å‚æ•°å      | å€¼   | å«ä¹‰                             |
|-------------|------|----------------------------------|
| BM          | 128  | block åœ¨ M æ–¹å‘è´Ÿè´£ 128 è¡Œ       |
| BN          | 128  | block åœ¨ N æ–¹å‘è´Ÿè´£ 128 åˆ—       |
| WM          | 64   | æ¯ä¸ª warp åœ¨ M æ–¹å‘è´Ÿè´£ 64 è¡Œ    |
| WN          | 32   | æ¯ä¸ª warp åœ¨ N æ–¹å‘è´Ÿè´£ 32 åˆ—    |
| cta_size    | 256  | 8 ä¸ª warp                        |

ç°åœ¨è®¡ç®—ï¼š

```cpp
NWarps_dim_N = BN / WN = 128 / 32 = 4
NWarps_dim_M = BM / WM = 128 / 64 = 2
```

â†’ éœ€è¦ 2 Ã— 4 = 8 ä¸ª warpï¼Œåˆšå¥½å¡«æ»¡ 256 çº¿ç¨‹ï¼

### æŠŠ 8 ä¸ª warp æ’æˆä¸€ä¸ª 2Ã—4 çš„äºŒç»´ç½‘æ ¼ï¼ˆç”»å›¾ç‰ˆï¼‰

```
warp_id:  0   1   2   3
          4   5   6   7
         â†‘   â†‘
      Mæ–¹å‘  Næ–¹å‘
      2ä¸ª    4ä¸ª
```

æ¯ä¸ª warp è´Ÿè´£çš„ C å­å—ï¼š

```
warp 0: C[0:63,     0:31]
warp 1: C[0:63,    32:63]
warp 2: C[0:63,    64:95]
warp 3: C[0:63,   96:127]
warp 4: C[64:127,   0:31]
warp 5: C[64:127, 32:63]
...
```

å®Œç¾é“ºæ»¡æ•´ä¸ª 128Ã—128 çš„ C tileï¼

### ä¸ºä»€ä¹ˆå¿…é¡»æ˜¯ 2 çš„å¹‚ï¼Ÿï¼ˆstatic_assert é‚£ä¸¤è¡Œçš„çœŸæ­£å«ä¹‰ï¼‰

```cpp
(NWarps_dim_N & (NWarps_dim_N-1)) == 0
```

è¿™æ˜¯ä¸€ä¸ªç»å…¸æŠ€å·§ï¼š**åˆ¤æ–­ä¸€ä¸ªæ•´æ•°æ˜¯ä¸æ˜¯ 2 çš„æ•´æ•°æ¬¡å¹‚ï¼ˆpower-of-twoï¼‰**

| æ•°å­— | äºŒè¿›åˆ¶   | num & (num-1) | æ˜¯å¦æ˜¯ 2 çš„å¹‚ï¼Ÿ |
|------|----------|----------------|-----------------|
| 1    | 0001     | 0000           | æ˜¯              |
| 2    | 0010     | 0000           | æ˜¯              |
| 4    | 0100     | 0000           | æ˜¯              |
| 8    | 1000     | 0000           | æ˜¯              |
| 3    | 0011     | 0010 â‰  0       | ä¸æ˜¯            |
| 6    | 0110     | 0110 â‰  0       | ä¸æ˜¯            |

### ä¸ºä»€ä¹ˆ GEMM ä»£ç å¼ºè¿«å®ƒæ˜¯ 2 çš„å¹‚ï¼Ÿ

å› ä¸ºåé¢ä¼šç”¨ä½è¿ç®—æ¥è®¡ç®— warp çš„äºŒç»´åæ ‡ï¼ˆè¶…çº§å¿«ï¼ï¼‰ï¼š

```cpp
const int warp_id = tid >> 5;                           // 0~7
const int wn_id   = warp_id & (NWarps_dim_N - 1);        // å–ä½ä½ â†’ N æ–¹å‘åæ ‡
const int wm_id   = warp_id >> get_log2(NWarps_dim_N);  // å³ç§» â†’ M æ–¹å‘åæ ‡
```

ä¾‹å­ï¼šNWarps_dim_N = 4ï¼ˆäºŒè¿›åˆ¶ 100ï¼‰

```cpp
warp_id = 0~7
wn_id = warp_id & 3   (= 11) â†’ 0,1,2,3, 0,1,2,3  â† N æ–¹å‘å¾ªç¯
wm_id = warp_id >> 2          â†’ 0,0,0,0, 1,1,1,1  â† M æ–¹å‘
```

å®Œç¾å¾—åˆ° 2Ã—4 ç½‘æ ¼åæ ‡ï¼

å¦‚æœä¸æ˜¯ 2 çš„å¹‚ï¼Œæ¯”å¦‚ NWarps_dim_N = 3ï¼š

- å°±ä¸èƒ½ç”¨ `& 3` å–æ¨¡
- å¿…é¡»ç”¨æ…¢ 100 å€çš„ `%` æˆ– `/` è¿ç®—
- è€Œä¸”å®¹æ˜“ branch divergence

### åˆæ³• vs éæ³•å‚æ•°å¯¹æ¯”

| BM  | BN  | WM  | WN  | NWarps_M | NWarps_N | æ€» warp | æ˜¯å¦åˆæ³•ï¼Ÿ | åŸå›  |
|-----|-----|-----|-----|----------|----------|---------|------------|------|
| 128 | 128 | 64  | 32  | 2        | 4        | 8       | åˆæ³•       | 2å’Œ4éƒ½æ˜¯2çš„å¹‚ |
| 128 | 128 | 64  | 64  | 2        | 2        | 4       | åˆæ³•       | å¥½ |
| 128 | 128 | 64  | 48  | 2        | 2.666 â†’ 3? | ä¸æ•´æ•°  | éæ³•       | 128/48 â‰  æ•´æ•° |
| 128 | 128 | 64  | 40  | 2        | 3.2 â†’ 3? | 3 ä¸æ˜¯2çš„å¹‚ | éæ³•       | static_assert å¤±è´¥ |

### ç»ˆæè®°å¿†å£è¯€ï¼ˆä¸€å¥è¯è®°ä½ï¼‰

> â€œä¸€ä¸ª block é‡Œçš„ warp å¿…é¡»æ’æˆä¸€ä¸ª MÃ—N çš„çŸ©å½¢ç½‘æ ¼ï¼Œè€Œä¸”è¿™ä¸ªçŸ©å½¢çš„å®½ï¼ˆNWarps_dim_Nï¼‰å’Œé«˜ï¼ˆNWarps_dim_Mï¼‰å¿…é¡»æ˜¯ 2 çš„æ•´æ•°æ¬¡å¹‚ï¼Œè¿™æ ·æ‰èƒ½ç”¨ä½è¿ç®—ï¼ˆ& å’Œ >>ï¼‰é£å¿«åœ°ç®—å‡ºæ¯ä¸ª warp çš„äºŒç»´åæ ‡ã€‚â€

æ‰€æœ‰é¡¶çº§ GEMMï¼ˆåŒ…æ‹¬ä½ è´´çš„é‚£æ®µï¼‰éƒ½å¼ºåˆ¶è¿™ä¸ªè§„åˆ™ï¼Œå› ä¸ºï¼š

- ä½è¿ç®—æ¯”é™¤æ³•å¿« 50~100 å€
- æ²¡æœ‰åˆ†æ”¯å‘æ•£
- ç¼–è¯‘å™¨èƒ½æ›´å¥½åœ°ä¼˜åŒ–

ç°åœ¨ä½ å†çœ‹è¿™å››è¡Œä»£ç ï¼š

```cpp
constexpr int NWarps_dim_N = (BN/WN);
constexpr int NWarps_dim_M = (BM/WM);
static_assert((NWarps_dim_N & (NWarps_dim_N-1)) == 0);
static_assert((NWarps_dim_M & (NWarps_dim_M-1)) == 0);
```

æ˜¯ä¸æ˜¯ç¬é—´æ˜ç™½äº†ï¼Ÿ

å®ƒå°±æ˜¯åœ¨è¯´ï¼šâ€œä½ ç»™æˆ‘é…å‚æ•°å¯ä»¥ï¼Œä½†å¿…é¡»è®©æˆ‘èƒ½æŠŠ warp æ’æˆä¸€ä¸ªè§„åˆ™çš„ã€2çš„å¹‚æ¬¡æ–¹çš„äºŒç»´ç½‘æ ¼ï¼å¦åˆ™æˆ‘ä¸å¹²ï¼â€

è¿™å°±æ˜¯å·¥ä¸šçº§ CUDA ä»£ç çš„â€œæ´ç™–â€å’Œâ€œæè‡´â€æ‰€åœ¨ã€‚

*/


/*
question12:

please explain this, using examples
Â 
    constexpr int NWarps_dim_N = (BN/WN);
    constexpr int NWarps_dim_M = (BM/WM);
    static_assert((NWarps_dim_N & (NWarps_dim_N-1)) == 0);
    static_assert((NWarps_dim_M & (NWarps_dim_M-1)) == 0);

å¤ªå¥½äº†ï¼ä½ é—®çš„è¿™å››è¡Œä»£ç æ˜¯æ‰€æœ‰**é¡¶çº§æ‰‹å†™ Tensor Core GEMM**é‡Œ**ç»å¯¹ä¸èƒ½é”™ã€ä¹Ÿæœ€å®¹æ˜“è¢«å¿½ç•¥**çš„â€œé»‘é­”æ³•â€ï¼š

```cpp
constexpr int NWarps_dim_N = (BN / WN);
constexpr int NWarps_dim_M = (BM / WM);
static_assert((NWarps_dim_N & (NWarps_dim_N-1)) == 0);
static_assert((NWarps_dim_M & (NWarps_dim_M-1)) == 0);
```

### ä¸€å¥è¯æ€»ç»“å®ƒçš„ä½œç”¨ï¼š
**â€œæˆ‘è¦æŠŠä¸€ä¸ª block é‡Œçš„æ‰€æœ‰ warp æ’æˆä¸€ä¸ª MÃ—N çš„äºŒç»´ç½‘æ ¼ï¼Œè¿™ä¸ªç½‘æ ¼çš„é•¿å’Œå®½å¿…é¡»æ˜¯ 2 çš„æ•´æ•°æ¬¡å¹‚ï¼â€**

ä¸‹é¢ç”¨çœŸå®ä¾‹å­ + ç”»å›¾ï¼Œå½»åº•è®²æ¸…æ¥šä¸ºä»€ä¹ˆå¿…é¡»è¿™æ ·ã€‚

### çœŸå®å·¥ä¸šçº§ä¾‹å­ï¼ˆæœ€å¸¸ç”¨çš„ä¸€å¥—å‚æ•°ï¼‰

```cpp
BM = 128, BN = 128          // block è®¡ç®— 128Ã—128 çš„ C
WM = 128, WN = 128          // æ¯ä¸ª warp è®¡ç®— 128Ã—128 çš„å­å—ï¼ˆå¤¸å¼ ä½†åˆæ³•ï¼‰
cta_size = 256              // block æœ‰ 256 çº¿ç¨‹ â†’ 8 ä¸ª warp
```

ä»£å…¥å…¬å¼ï¼š

```cpp
NWarps_dim_N = BN / WN = 128 / 128 = 1
NWarps_dim_M = BM / WM = 128 / 128 = 1
```

â†’ åªæœ‰ 1Ã—1 = 1 ä¸ª warpï¼Ÿä¸å¯¹ï¼æˆ‘ä»¬æ˜æ˜æœ‰ 8 ä¸ª warpï¼

é”™ï¼è¿™å¥—å‚æ•°ä¸åˆæ³•ã€‚æˆ‘ä»¬æ¢ä¸€å¥—çœŸæ­£åˆæ³•çš„ï¼š

### æ­£ç¡®ã€å¸¸è§çš„å‚æ•°ç»„åˆï¼ˆæ¥è‡ªçœŸå®é¡¹ç›®ï¼‰

| å‚æ•°å      | å€¼   | å«ä¹‰                             |
|-------------|------|----------------------------------|
| BM          | 128  | block åœ¨ M æ–¹å‘è´Ÿè´£ 128 è¡Œ       |
| BN          | 128  | block åœ¨ N æ–¹å‘è´Ÿè´£ 128 åˆ—       |
| WM          | 64   | æ¯ä¸ª warp åœ¨ M æ–¹å‘è´Ÿè´£ 64 è¡Œ    |
| WN          | 32   | æ¯ä¸ª warp åœ¨ N æ–¹å‘è´Ÿè´£ 32 åˆ—    |
| cta_size    | 256  | 8 ä¸ª warp                        |

ç°åœ¨è®¡ç®—ï¼š

```cpp
NWarps_dim_N = BN / WN = 128 / 32 = 4
NWarps_dim_M = BM / WM = 128 / 64 = 2
```

â†’ éœ€è¦ 2 Ã— 4 = 8 ä¸ª warpï¼Œåˆšå¥½å¡«æ»¡ 256 çº¿ç¨‹ï¼

### æŠŠ 8 ä¸ª warp æ’æˆä¸€ä¸ª 2Ã—4 çš„äºŒç»´ç½‘æ ¼ï¼ˆç”»å›¾ç‰ˆï¼‰

```
warp_id:  0   1   2   3
          4   5   6   7
         â†‘   â†‘
      Mæ–¹å‘  Næ–¹å‘
      2ä¸ª    4ä¸ª
```

æ¯ä¸ª warp è´Ÿè´£çš„ C å­å—ï¼š

```
warp 0: C[0:63,     0:31]
warp 1: C[0:63,    32:63]
warp 2: C[0:63,    64:95]
warp 3: C[0:63,   96:127]
warp 4: C[64:127,   0:31]
warp 5: C[64:127, 32:63]
...
```

å®Œç¾é“ºæ»¡æ•´ä¸ª 128Ã—128 çš„ C tileï¼

### ä¸ºä»€ä¹ˆå¿…é¡»æ˜¯ 2 çš„å¹‚ï¼Ÿï¼ˆstatic_assert é‚£ä¸¤è¡Œçš„çœŸæ­£å«ä¹‰ï¼‰

```cpp
(NWarps_dim_N & (NWarps_dim_N-1)) == 0
```

è¿™æ˜¯ä¸€ä¸ªç»å…¸æŠ€å·§ï¼š**åˆ¤æ–­ä¸€ä¸ªæ•´æ•°æ˜¯ä¸æ˜¯ 2 çš„æ•´æ•°æ¬¡å¹‚ï¼ˆpower-of-twoï¼‰**

| æ•°å­— | äºŒè¿›åˆ¶   | num & (num-1) | æ˜¯å¦æ˜¯ 2 çš„å¹‚ï¼Ÿ |
|------|----------|----------------|-----------------|
| 1    | 0001     | 0000           | æ˜¯              |
| 2    | 0010     | 0000           | æ˜¯              |
| 4    | 0100     | 0000           | æ˜¯              |
| 8    | 1000     | 0000           | æ˜¯              |
| 3    | 0011     | 0010 â‰  0       | ä¸æ˜¯            |
| 6    | 0110     | 0110 â‰  0       | ä¸æ˜¯            |

### ä¸ºä»€ä¹ˆ GEMM ä»£ç å¼ºè¿«å®ƒæ˜¯ 2 çš„å¹‚ï¼Ÿ

å› ä¸ºåé¢ä¼šç”¨ä½è¿ç®—æ¥è®¡ç®— warp çš„äºŒç»´åæ ‡ï¼ˆè¶…çº§å¿«ï¼ï¼‰ï¼š

```cpp
const int warp_id = tid >> 5;                           // 0~7
const int wn_id   = warp_id & (NWarps_dim_N - 1);        // å–ä½ä½ â†’ N æ–¹å‘åæ ‡
const int wm_id   = warp_id >> get_log2(NWarps_dim_N);  // å³ç§» â†’ M æ–¹å‘åæ ‡
```

ä¾‹å­ï¼šNWarps_dim_N = 4ï¼ˆäºŒè¿›åˆ¶ 100ï¼‰

```cpp
warp_id = 0~7
wn_id = warp_id & 3   (= 11) â†’ 0,1,2,3, 0,1,2,3  â† N æ–¹å‘å¾ªç¯
wm_id = warp_id >> 2          â†’ 0,0,0,0, 1,1,1,1  â† M æ–¹å‘
```

å®Œç¾å¾—åˆ° 2Ã—4 ç½‘æ ¼åæ ‡ï¼

å¦‚æœä¸æ˜¯ 2 çš„å¹‚ï¼Œæ¯”å¦‚ NWarps_dim_N = 3ï¼š

- å°±ä¸èƒ½ç”¨ `& 3` å–æ¨¡
- å¿…é¡»ç”¨æ…¢ 100 å€çš„ `%` æˆ– `/` è¿ç®—
- è€Œä¸”å®¹æ˜“ branch divergence

### åˆæ³• vs éæ³•å‚æ•°å¯¹æ¯”

| BM  | BN  | WM  | WN  | NWarps_M | NWarps_N | æ€» warp | æ˜¯å¦åˆæ³•ï¼Ÿ | åŸå›  |
|-----|-----|-----|-----|----------|----------|---------|------------|------|
| 128 | 128 | 64  | 32  | 2        | 4        | 8       | åˆæ³•       | 2å’Œ4éƒ½æ˜¯2çš„å¹‚ |
| 128 | 128 | 64  | 64  | 2        | 2        | 4       | åˆæ³•       | å¥½ |
| 128 | 128 | 64  | 48  | 2        | 2.666 â†’ 3? | ä¸æ•´æ•°  | éæ³•       | 128/48 â‰  æ•´æ•° |
| 128 | 128 | 64  | 40  | 2        | 3.2 â†’ 3? | 3 ä¸æ˜¯2çš„å¹‚ | éæ³•       | static_assert å¤±è´¥ |

### ç»ˆæè®°å¿†å£è¯€ï¼ˆä¸€å¥è¯è®°ä½ï¼‰

> â€œä¸€ä¸ª block é‡Œçš„ warp å¿…é¡»æ’æˆä¸€ä¸ª MÃ—N çš„çŸ©å½¢ç½‘æ ¼ï¼Œè€Œä¸”è¿™ä¸ªçŸ©å½¢çš„å®½ï¼ˆNWarps_dim_Nï¼‰å’Œé«˜ï¼ˆNWarps_dim_Mï¼‰å¿…é¡»æ˜¯ 2 çš„æ•´æ•°æ¬¡å¹‚ï¼Œè¿™æ ·æ‰èƒ½ç”¨ä½è¿ç®—ï¼ˆ& å’Œ >>ï¼‰é£å¿«åœ°ç®—å‡ºæ¯ä¸ª warp çš„äºŒç»´åæ ‡ã€‚â€

æ‰€æœ‰é¡¶çº§ GEMMï¼ˆåŒ…æ‹¬ä½ è´´çš„é‚£æ®µï¼‰éƒ½å¼ºåˆ¶è¿™ä¸ªè§„åˆ™ï¼Œå› ä¸ºï¼š

- ä½è¿ç®—æ¯”é™¤æ³•å¿« 50~100 å€
- æ²¡æœ‰åˆ†æ”¯å‘æ•£
- ç¼–è¯‘å™¨èƒ½æ›´å¥½åœ°ä¼˜åŒ–

ç°åœ¨ä½ å†çœ‹è¿™å››è¡Œä»£ç ï¼š

```cpp
constexpr int NWarps_dim_N = (BN/WN);
constexpr int NWarps_dim_M = (BM/WM);
static_assert((NWarps_dim_N & (NWarps_dim_N-1)) == 0);
static_assert((NWarps_dim_M & (NWarps_dim_M-1)) == 0);
```

æ˜¯ä¸æ˜¯ç¬é—´æ˜ç™½äº†ï¼Ÿ

å®ƒå°±æ˜¯åœ¨è¯´ï¼šâ€œä½ ç»™æˆ‘é…å‚æ•°å¯ä»¥ï¼Œä½†å¿…é¡»è®©æˆ‘èƒ½æŠŠ warp æ’æˆä¸€ä¸ªè§„åˆ™çš„ã€2çš„å¹‚æ¬¡æ–¹çš„äºŒç»´ç½‘æ ¼ï¼å¦åˆ™æˆ‘ä¸å¹²ï¼â€

è¿™å°±æ˜¯å·¥ä¸šçº§ CUDA ä»£ç çš„â€œæ´ç™–â€å’Œâ€œæè‡´â€æ‰€åœ¨ã€‚

*/


/*
question13:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­
Â 
    const int warp_id = tid>>5;
    const int lane_id = tid & 0x1F;
    static_assert(get_log2x<NWarps_dim_N>() == 2);
    const int wm_id = warp_id >> get_log2x<NWarps_dim_N>();
    const int wn_id = warp_id & (NWarps_dim_N-1);

å¤ªå¥½äº†ï¼ä½ é—®çš„è¿™å‡ è¡Œæ˜¯æ•´ä¸ªé«˜æ€§èƒ½ GEMM çš„â€œçµé­‚åæ ‡å˜æ¢â€â€”â€”**ç”¨æœ€å¿«çš„ä½è¿ç®—ï¼ŒæŠŠä¸€ç»´çš„ warp_id å˜æˆäºŒç»´çš„ (wm_id, wn_id)**ã€‚

æˆ‘ä»¬ç”¨**ä¸€ä¸ªçœŸå®ã€è¶…çº§å¸¸è§çš„ä¾‹å­**ï¼Œä»å¤´åˆ°å°¾ç”»å›¾ + ä¸€æ­¥æ­¥è®¡ç®—ï¼Œå½»åº•è®²æ¸…æ¥šã€‚

### çœŸå®å‚æ•°ï¼ˆæ¥è‡ªé¡¶çº§å¼€æºé¡¹ç›®ï¼Œæ¯”å¦‚ FlashAttentionã€Triton ç­‰ï¼‰

```cpp
BM = 128          // block è´Ÿè´£ 128 è¡Œ
BN = 128          // block è´Ÿè´£ 128 åˆ—
WM = 64           // æ¯ä¸ª warp è´Ÿè´£ 64 è¡Œ
WN = 32           // æ¯ä¸ª warp è´Ÿè´£ 32 åˆ—
cta_size = 256    // block æœ‰ 256 çº¿ç¨‹ â†’ 8 ä¸ª warp
```

è®¡ç®—å‡ºå…³é”®å¸¸é‡ï¼š

```cpp
constexpr int NWarps_dim_N = BN / WN = 128 / 32 = 4
constexpr int NWarps_dim_M = BM / WM = 128 / 64 = 2
// â†’ éœ€è¦ 2 Ã— 4 = 8 ä¸ª warpï¼Œåˆšå¥½ï¼
```

### æŠŠ 8 ä¸ª warp æ’æˆ 2Ã—4 çš„ç½‘æ ¼ï¼ˆç”»å›¾ï¼‰

```
warp_id:   0     1     2     3    â† N æ–¹å‘ï¼ˆåˆ—ï¼‰
           4     5     6     7
          â†‘
       M æ–¹å‘ï¼ˆè¡Œï¼‰
       2 è¡Œ
```

æ¯ä¸ª warp è´Ÿè´£çš„ C å­å—ï¼š

| warp_id | è´Ÿè´£çš„ C åŒºåŸŸ             | åæ ‡ (wm_id, wn_id) |
|---------|----------------------------|---------------------|
| 0       | C[0:63,    0:31]           | (0, 0)              |
| 1       | C[0:63,   32:63]           | (0, 1)              |
| 2       | C[0:63,   64:95]           | (0, 2)              |
| 3       | C[0:63,  96:127]           | (0, 3)              |
| 4       | C[64:127,  0:31]           | (1, 0)              |
| 5       | C[64:127, 32:63]           | (1, 1)              |
| ...     | ...                        | ...                 |

### ç°åœ¨çœ‹ä»£ç æ€ä¹ˆç”¨ä½è¿ç®—ç®—å‡º (wm_id, wn_id)

```cpp
const int warp_id = tid >> 5;        // 256 çº¿ç¨‹ â†’ 8 ä¸ª warp
const int lane_id = tid & 0x1F;      // 31 = 0b11111ï¼Œå–ä½ 5 ä½ â†’ lane_id

static_assert(get_log2x<NWarps_dim_N>() == 2);   // 4 = 2Â² â†’ log2(4)=2

const int wm_id = warp_id >> get_log2x<NWarps_dim_N>();  // å³ç§» 2 ä½
const int wn_id = warp_id & (NWarps_dim_N-1);            // & 3
```

### ä¸€æ­¥æ­¥è®¡ç®—ï¼ˆç”¨è¡¨æ ¼ï¼‰

| tid èŒƒå›´ | warp_id = tid>>5 | äºŒè¿›åˆ¶ | wm_id = warp_id >> 2 | wn_id = warp_id & 3 | ç»“æœ (wm_id, wn_id) |
|----------|------------------|--------|----------------------|---------------------|---------------------|
| 0~31     | 0                | 000    | 000 >> 2 = 0         | 000 & 011 = 0       | (0, 0)              |
| 32~63    | 1                | 001    | 001 >> 2 = 0         | 001 & 011 = 1       | (0, 1)              |
| 64~95    | 2                | 010    | 010 >> 2 = 0         | 010 & 011 = 2       | (0, 2)              |
| 96~127   | 3                | 011    | 011 >> 2 = 0         | 011 & 011 = 3       | (0, 3)              |
| 128~159  | 4                | 100    | 100 >> 2 = 1         | 100 & 011 = 0       | (1, 0)              |
| 160~191  | 5                | 101    | 101 >> 2 = 1         | 101 & 011 = 1       | (1, 1)              |
| 192~223  | 6                | 110    | 110 >> 2 = 1         | 110 & 011 = 2       | (1, 2)              |
| 224~255  | 7                | 111    | 111 >> 2 = 1         | 111 & 011 = 3       | (1, 3)              |

å®Œç¾ï¼å®Œå…¨ç¬¦åˆæˆ‘ä»¬ç”»çš„ 2Ã—4 ç½‘æ ¼ï¼

### ä¸ºä»€ä¹ˆå¿…é¡» `static_assert(get_log2x<NWarps_dim_N>() == 2)`ï¼Ÿ

å› ä¸º `NWarps_dim_N = 4`ï¼Œè€Œ `log2(4) = 2`ï¼Œæ‰€ä»¥å³ç§» 2 ä½å°±èƒ½æŠŠ M æ–¹å‘â€œå‰¥â€å‡ºæ¥ã€‚

å¦‚æœä¸æ˜¯ 2 çš„å¹‚ï¼Œæ¯”å¦‚ `NWarps_dim_N = 3`ï¼š

- `warp_id & 2` å°±ä¸èƒ½æ­£ç¡®å–æ¨¡
- å¿…é¡»å†™ `warp_id % 3` â†’ æ…¢ 100 å€ + åˆ†æ”¯å‘æ•£
- ç¼–è¯‘å™¨ä¹Ÿæ— æ³•ä¼˜åŒ–

### ç»ˆæè®°å¿†å£è¯€ï¼ˆä¸€å¥è¯è®°ä½ï¼‰

```text
warp_id æ˜¯â€œçº¿æ€§ç¼–å·â€ï¼ˆ0~7ï¼‰
æˆ‘ä»¬è¦æŠŠå®ƒå˜æˆâ€œäºŒç»´åæ ‡â€(wm_id, wn_id)

å› ä¸º N æ–¹å‘æœ‰ 4=2Â² ä¸ª warp â†’ ä½ 2 ä½æ˜¯ N åæ ‡ â†’ ç”¨ & 3 å–å‡ºæ¥
é«˜ä½æ˜¯ M åæ ‡ â†’ å³ç§» 2 ä½å°±å¾—åˆ° wm_id

æ‰€ä»¥ï¼š
wn_id = warp_id & (NWarps_dim_N-1)   â† å–ä½ä½çš„ N åæ ‡
wm_id = warp_id >> log2(NWarps_dim_N) â† å³ç§»ï¼Œå»æ‰ä½ä½çš„ N åæ ‡
```

### æ‰€æœ‰é¡¶çº§ GEMM éƒ½è¿™ä¹ˆå†™çš„åŸå› 

| å†™æ³•                     | é€Ÿåº¦     | æ˜¯å¦åˆ†æ”¯å‘æ•£ | ç¼–è¯‘å™¨ä¼˜åŒ– |
|--------------------------|----------|--------------|------------|
| `wn_id = warp_id % 4`    | æ…¢ 100 å€| æœ‰           | å·®         |
| `wn_id = warp_id & 3`    | 1 å‘¨æœŸ   | æ—            | å®Œç¾       |

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå‰é¢æœ‰ `static_assert` å¼ºåˆ¶æ˜¯ 2 çš„å¹‚â€”â€”**ä¸ºäº†ç”¨æœ€å¿«çš„ä½è¿ç®—åšåæ ‡å˜æ¢**ï¼

ç°åœ¨ä½ å†çœ‹è¿™å‡ è¡Œï¼š

```cpp
const int warp_id = tid>>5;
const int lane_id = tid & 0x1F;
static_assert(get_log2x<NWarps_dim_N>() == 2);
const int wm_id = warp_id >> get_log2x<NWarps_dim_N>();
const int wn_id = warp_id & (NWarps_dim_N-1);
```

æ˜¯ä¸æ˜¯ç¬é—´å°±æ‡‚äº†ï¼Ÿ

å®ƒå°±æ˜¯åœ¨ç”¨**æœ€å¿«çš„ä½è¿ç®—**ï¼ŒæŠŠä¸€ç»´çš„ warp ç¼–å·ï¼Œå˜æˆäºŒç»´ç½‘æ ¼åæ ‡ï¼Œè®©æ¯ä¸ª warp çŸ¥é“è‡ªå·±è¯¥ç®— C çŸ©é˜µçš„å“ªä¸€å—ã€‚

è¿™å‡ è¡Œä»£ç ï¼Œæ˜¯æ‰€æœ‰ 2025 å¹´é¡¶çº§ CUDA GEMM çš„â€œæ ‡å‡†ä»ªå¼â€ï¼Œä½ ç°åœ¨ä¹Ÿå®Œå…¨æŒæ¡äº†ï¼
*/


/*
question14:

cuda coding, åœ¨è®¡ç®—matrix multiplication, çœŸæ­£çš„è®¡ç®—æ˜¯åœ¨L1 cache, register, TMA, è¿˜æ˜¯åˆ«çš„åœ°æ–¹ï¼Ÿ

åœ¨ **CUDA çš„çŸ©é˜µä¹˜æ³•ï¼ˆGEMMï¼‰ä¸­ï¼ŒçœŸæ­£çš„æ•°å€¼è®¡ç®—ï¼ˆæ¯”å¦‚ FMA multiply-addï¼‰åªå‘ç”Ÿåœ¨ GPU çš„**ï¼š

# âœ… **æœ€ç»ˆå‘ç”Ÿè®¡ç®—çš„åœ°æ–¹**

## ğŸ¯ **FP32/FP16/Tensor Core ALUï¼ˆå¯„å­˜å™¨çº§è®¡ç®—å•å…ƒï¼‰**

çŸ©é˜µä¹˜æ³•ä¸­çš„ä¹˜æ³•å’ŒåŠ æ³• **å§‹ç»ˆå‘ç”Ÿåœ¨å¯„å­˜å™¨æ–‡ä»¶ï¼ˆRegistersï¼‰ä¸ ALU / Tensor Cores ä¹‹é—´**ã€‚
å¯„å­˜å™¨æ˜¯ GPU ä¸­æœ€å¿«çš„å­˜å‚¨ï¼ŒFMA æ“ä½œåªèƒ½è¯»å–å¯„å­˜å™¨ä¸­çš„æ•°æ®ã€‚

### ğŸ‘‰ **é‡ç‚¹ï¼šGPU ä¸ä¼šç›´æ¥ä» L1/Shared/TMA/Global æ‰§è¡Œè®¡ç®—ï¼Œè®¡ç®—å¿…é¡»åœ¨å¯„å­˜å™¨å†…å®Œæˆã€‚**

---

# ğŸš€ é‚£ L1ã€Shared memoryã€TMAã€Global å„è‡ªåšä»€ä¹ˆï¼Ÿ

ä¸‹é¢ä»æ•°æ®æµï¼ˆDataflowï¼‰è§£é‡Šã€‚

---

# ğŸ§  **GPU Matrix Multiply çš„æ•°æ®æµ**

ä»¥ç°ä»£ GPUï¼ˆå¦‚ Hopper/Blackwellï¼‰ä¸ºå‚è€ƒï¼š

```
Global Memory -> L2 -> (L1 / TMA) -> Shared Memory -> Registers -> Tensor Cores (FMA)
```

è§£é‡Šå¦‚ä¸‹ï¼š

---

# ğŸ› 1. **Global Memoryï¼ˆHBMï¼‰**

æœ€æ…¢çš„å­˜å‚¨ã€‚ä½ åŠ è½½ Aã€B çŸ©é˜µçš„æºæ•°æ®ã€‚

---

# ğŸ§± 2. **L2 Cache**

å…¨å±€å…±äº«ç¼“å­˜ã€‚å¤šæ•° global load ä¼šé€šè¿‡ L2ã€‚

---

# ğŸŒ€ 3. **L1 Cache / Texture Cache**

æ¯ä¸ª SM æœ‰è‡ªå·±çš„ L1ï¼Œéå¸¸å¿«ï¼Œä½†**ä¸æ˜¯ç”¨äºä¸»çŸ©é˜µè®¡ç®—**ï¼š

* warp-level global memory load ä¼šç»è¿‡ L1
* æœ‰ coalesced load æ—¶æ•ˆæœæœ€ä½³
* ä½†è®¡ç®—ä¸ä¼šåœ¨ L1 ä¸­è¿›è¡Œ

---

# ğŸ“¦ 4. **Shared Memoryï¼ˆé€šè¿‡ SMEM Tileï¼‰**

ğŸ’¡ **çŸ©é˜µä¹˜æ³•æœ€å…³é”®çš„æœ¬åœ°ç¼“å­˜**

Tile-based GEMM ä¼šå°†çŸ©é˜µ A å’Œ B çš„å­çŸ©é˜µåŠ è½½åˆ° Shared Memoryï¼š

```
Global -> L2 -> L1 -> Shared Mem (tile)
```

è¿™æ ·ï¼š

* æ‰€æœ‰ thread å¯ä»¥å…±äº« tile
* warp å¯ä»¥é«˜å¹¶å‘åœ°è¯»å–
* è®¿é—®å»¶è¿Ÿå¾ˆä½

ä½† **è®¡ç®—å¹¶ä¸æ˜¯åœ¨ Shared Memory ä¸­å®Œæˆ**ï¼ŒSMEM åªæ˜¯ staging bufferã€‚

---

# ğŸ”„ 5. **TMAï¼ˆTensor Memory Acceleratorï¼‰** â€” Hopper+ å¼•å…¥

TMA æ˜¯ DMA engineï¼Œä½œç”¨æ˜¯ï¼š

* å°† tile ä» Global ç›´æ¥æ¬è¿åˆ° Shared Memory
* æ›´é«˜å¸¦å®½
* ä¸å ç”¨ warp æ‰§è¡Œ load æŒ‡ä»¤

å®ƒåªæ˜¯ä¼ è¾“æ•°æ®ï¼Œä¸å‚ä¸è®¡ç®—ã€‚

---

# âš¡ 6. **Registersï¼ˆæœ€ç»ˆè®¡ç®—å‘ç”Ÿåœ°ï¼‰**

æ¯ä¸ª thread åœ¨è®¡ç®—æ—¶å°† tile ä» SMEM è¯»å–åˆ° registersï¼š

```
Shared Memory â†’ Register
```

è®¡ç®—å°±æ˜¯ï¼š

```
C_reg += A_reg * B_reg
```

ä¹Ÿå°±æ˜¯ fused-multiply-add (FMA) â†’ åœ¨ scalar ALU æˆ– Tensor Core ä¸Šæ‰§è¡Œã€‚

Registers æ˜¯ GPU æœ€å¿«çš„å­˜å‚¨ã€‚

---

# ğŸ”¥ 7. **Tensor Coresï¼ˆç°ä»£ GEMM çš„æ ¸å¿ƒï¼‰**

Tensor Core è¿›è¡Œå®é™…çš„çŸ©é˜µå—ä¹˜æ³•ï¼š

```
mma.sync(â€¦)
wmma.mma(â€¦)
```

è¾“å…¥æ¥è‡ªå¯„å­˜å™¨ï¼Œç»“æœä¹Ÿå†™å…¥å¯„å­˜å™¨ã€‚

---

# ğŸ“Œ **æ€»ç»“ï¼šçœŸæ­£çš„è®¡ç®—å‘ç”Ÿåœ¨å“ªé‡Œï¼Ÿ**

| ç»„ä»¶                    | æ˜¯å¦æ‰§è¡Œè®¡ç®—ï¼Ÿ | ä½œç”¨             |
| --------------------- | ------- | -------------- |
| **Registers**         | âœ”ï¸Yes   | è®¡ç®—è¯»å†™çš„æœ€ç»ˆä½ç½®      |
| **Tensor Core / ALU** | âœ”ï¸Yes   | æ‰§è¡Œ FMA / MMA   |
| **Shared Memory**     | âŒNo     | tile ç¼“å†²åŒº       |
| **L1 Cache**          | âŒNo     | ç¼“å­˜ global load |
| **L2 Cache**          | âŒNo     | å…¨å±€ç¼“å­˜           |
| **TMA**               | âŒNo     | ä¼ è¾“å¼•æ“ï¼Œä¸è®¡ç®—       |
| **Global Memory**     | âŒNo     | æœ€æ…¢çš„å­˜å‚¨ï¼Œç”¨æ¥åŠ è½½çŸ©é˜µ   |

---

# ğŸ§© ä¸ºä»€ä¹ˆæ‰€æœ‰è®¡ç®—å¿…é¡»åœ¨å¯„å­˜å™¨ä¸­ï¼Ÿ

å› ä¸ºï¼š

* GPU ALU/TensorCore çš„è¾“å…¥æ˜¯å¯„å­˜å™¨
* SMEM/L1 éƒ½ä¸å…è®¸ç›´æ¥ FMA
* warp-level æ‰§è¡Œéœ€è¦å¿«é€Ÿå¯„å­˜å™¨è®¿é—®

ä½ æ°¸è¿œä¸ä¼šçœ‹åˆ°ï¼š

```
fma(shared_mem[i], shared_mem[j])
```

è¿™ç§æ˜¯ä¸å…è®¸çš„ã€‚

---

# âœ”ï¸ Short Answerï¼ˆé¢è¯•ç‰ˆï¼‰

> CUDA GEMM çš„æœ€ç»ˆè®¡ç®—å…¨éƒ¨å‘ç”Ÿåœ¨ **å¯„å­˜å™¨å’Œ Tensor Core/ALU** ä¸­ã€‚
> L1ã€Shared Memoryã€TMA éƒ½æ˜¯ä¸ºè®¡ç®—åšæ•°æ® staging çš„é«˜é€Ÿç¼“å­˜æˆ–ä¼ è¾“æœºåˆ¶ï¼Œä¸æ‰§è¡Œæ•°å­¦è¿ç®—ã€‚

---

# å¦‚æœä½ æ„¿æ„ï¼Œæˆ‘å¯ä»¥ç»™ä½ ç”»ä¸€å¼  ASCII GPU dataflow å›¾ï¼ˆé¢è¯•æ›´å®¹æ˜“è¯´æ¸…æ¥šï¼‰ã€‚

ä¸‹é¢ç»™ä½ ç”»ä¸€å¼  **GPU GEMM æ•°æ®æµï¼ˆDataflowï¼‰ASCII å›¾**ï¼Œéå¸¸é€‚åˆé¢è¯•è®²è§£ï¼Œæ¸…æ™°è¡¨è¾¾â€œçœŸæ­£è®¡ç®—å‘ç”Ÿåœ¨å“ªé‡Œâ€ã€‚

---

# ğŸ§© **CUDA Matrix Multiplicationï¼ˆGEMMï¼‰æ•°æ®æµ ASCII å›¾**

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                Global Memory (HBM)          â”‚
                    â”‚  (very large, very slow)                    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                   L2 Cache                  â”‚
                    â”‚    (chip-wide shared cache, medium speed)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                                    â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           SM (Streaming Multiprocessor)       â”‚
        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
        â”‚                                                               â”‚
        â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚     â”‚        L1 / TEX Cache    â”‚          â”‚      TMA       â”‚  â”‚
        â”‚     â”‚    (per-SM fast cache)   â”‚          â”‚ DMA engine     â”‚  â”‚
        â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚                     (coalesced loads)              â”‚           â”‚
        â”‚                              â”‚                    â”‚           â”‚
        â”‚                              â–¼                    â–¼           â”‚
        â”‚                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
        â”‚                â”‚           Shared Memory (SMEM)         â”‚     â”‚
        â”‚                â”‚  (software-managed tile buffer)        â”‚     â”‚
        â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
        â”‚                                â”‚                              â”‚
        â”‚                                â”‚  load tile into registers    â”‚
        â”‚                                â–¼                              â”‚
        â”‚                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
        â”‚                     â”‚        Registers          â”‚  <â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
        â”‚                     â”‚ (fastest storage on SM)  â”‚            â”‚  â”‚
        â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
        â”‚                                    â”‚                        â”‚  â”‚
        â”‚               (REAL COMPUTATION)   â”‚                        â”‚  â”‚
        â”‚                                    â–¼                        â”‚  â”‚
        â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚  â”‚
        â”‚                    â”‚  Tensor Cores / FP32 ALUs    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
        â”‚                    â”‚ (FMA / MMA execution units)  â”‚            â”‚
        â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
        â”‚                                                               â”‚
        â”‚                          (output C tile stored in registers)  â”‚
        â”‚                                                               â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ¯ **ä¸€å¥è¯æ€»ç»“**

* **æ‰€æœ‰æ•°å­¦è¿ç®—ï¼ˆFMA/MMAï¼‰éƒ½å‘ç”Ÿåœ¨ Tensor Cores / ALUsã€‚**
* **è¿ç®—çš„è¾“å…¥è¾“å‡ºéƒ½åœ¨ Registerï¼ˆå¯„å­˜å™¨ï¼‰ä¸­ã€‚**
* **Shared Memory / L1 / TMA éƒ½åªè´Ÿè´£æ¬è¿ã€ç¼“å­˜ã€åˆ†å‘æ•°æ®ï¼Œä¸æ‰§è¡Œè¿ç®—ã€‚**

---

# ğŸ§  è§£é‡Šç‰ˆæœ¬ï¼ˆé¢è¯•æ—¶ 20 ç§’å¿«é€Ÿè¯´æ³•ï¼‰

> åœ¨ GPU ä¸Š GEMM çš„æœ€ç»ˆè®¡ç®—åªå‘ç”Ÿåœ¨å¯„å­˜å™¨å’Œ Tensor Core ä¸­ã€‚
> æ•°æ®ä» Global Memory â†’ L2 â†’ L1/TMA â†’ Shared Memory é€çº§åŠ è½½ï¼Œ
> å†ä» Shared Memory åŠ è½½åˆ°å¯„å­˜å™¨åï¼ŒTensor Cores æ‰§è¡ŒçœŸæ­£çš„çŸ©é˜µä¹˜æ³•ã€‚
> Shared Memory å’Œ L1 åªæ˜¯ç¼“å­˜ä¸ staging bufferï¼Œä»ä¸æ‰§è¡Œè®¡ç®—ã€‚

---

å¦‚æœä½ éœ€è¦ï¼Œæˆ‘å¯ä»¥å†å¸®ä½ ç”»ä¸€å¼  **warp-level tile åŠ è½½æµç¨‹å›¾** æˆ– **Tensor Core MMA æ•°æ®æµå›¾**ï¼ˆä¹Ÿé€‚åˆé¢è¯•è§£é‡Šï¼‰ã€‚


*/


/*
question15:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­
Â 
    //thread level
    //constexpr int TM_vec_num = TM / vec_size;
    constexpr int TN_vec_num = TN / vec_size;
    //constexpr int WM_per_loop = WM /TM_vec_num;
    constexpr int WN_per_loop = WN /TN_vec_num;
    const int tm_vec_id = lane_id >> get_log2x<WN_per_loop/vec_size>();
    const int tn_vec_id = lane_id & (WN_per_loop/vec_size-1);


å¤ªå¥½äº†ï¼ä½ é—®çš„è¿™å‡ è¡Œæ˜¯**æ•´ä¸ª GEMM ä»£ç é‡Œæœ€æœ€æœ€æœ€æœ€ç²¾å¦™ã€æœ€éš¾æ‡‚ã€ä½†ä¹Ÿæœ€å†³å®šæ€§èƒ½**çš„éƒ¨åˆ†â€”â€”

**ä¸€ä¸ª warp é‡Œçš„ 32 ä¸ªçº¿ç¨‹ï¼Œæ€ä¹ˆå†ç»†åˆ†ï¼ŒæŠŠä¸€ä¸ª 128Ã—128 çš„ C å­å—æ‹†æˆ 32 ä¸ªçº¿ç¨‹å„è‡ªè´Ÿè´£çš„å°ç¢ç‰‡ï¼Ÿ**

æˆ‘ä»¬ç”¨**ä¸€ä¸ªçœŸå®ã€è¶…çº§å¸¸è§ã€ç”»å›¾ç‰ˆçš„ä¾‹å­**ï¼Œä»å¤´åˆ°å°¾å½»åº•è®²æ¸…æ¥šã€‚

### çœŸå®å·¥ä¸šçº§å‚æ•°ï¼ˆæ¥è‡ª FlashAttentionã€Triton ç­‰é¡¹ç›®ï¼‰

```cpp
WM = 64          // æ¯ä¸ª warp åœ¨ M æ–¹å‘è´Ÿè´£ 64 è¡Œ
WN = 32          // æ¯ä¸ª warp åœ¨ N æ–¹å‘è´Ÿè´£ 32 åˆ—
TM = 32          // æ¯ä¸ªçº¿ç¨‹åœ¨ M æ–¹å‘è´Ÿè´£ 32 ä¸ª C å…ƒç´ 
TN = 32          // æ¯ä¸ªçº¿ç¨‹åœ¨ N æ–¹å‘è´Ÿè´£ 32 ä¸ª C å…ƒç´ 
vec_t = float4   // ä¸€æ¬¡ä» global/shared åŠ è½½ 4 ä¸ª float â†’ vec_size = 4
```

â†’ ä¸€ä¸ª warp è´Ÿè´£ 64Ã—32 çš„ C å­å—  
â†’ 32 ä¸ªçº¿ç¨‹ â†’ æ¯ä¸ªçº¿ç¨‹è´Ÿè´£ 32Ã—32 = 1024 ä¸ª C å…ƒç´ ï¼ˆé€šè¿‡å¯„å­˜å™¨ + Tensor Core å®Œæˆï¼‰

### ç¬¬ä¸€æ­¥ï¼šè®¡ç®—å…³é”®å¸¸é‡

```cpp
constexpr int TN_vec_num     = TN / vec_size = 32 / 4 = 8
constexpr int WN_per_loop    = WN / TN_vec_num = 32 / 8 = 4
```

è§£é‡Šï¼š
- æ¯ä¸ªçº¿ç¨‹åœ¨ N æ–¹å‘è´Ÿè´£ 32 ä¸ªå…ƒç´ 
- ä½†æ¯æ¬¡ç”¨ `float4` åŠ è½½ 4 ä¸ª â†’ æ‰€ä»¥åªéœ€è¦ **8 æ¬¡å‘é‡åŠ è½½** å°±èƒ½è¦†ç›– 32 ä¸ª
- ä¸€ä¸ª warp åœ¨ N æ–¹å‘æ€»å…± 32 åˆ— â†’ 32 Ã· 8 = **4 ä¸ªçº¿ç¨‹** å°±èƒ½è¦†ç›–æ•´ä¸ª N æ–¹å‘
- æ‰€ä»¥ï¼š**åœ¨ N æ–¹å‘ï¼Œæ¯ 4 ä¸ªçº¿ç¨‹ç»„æˆä¸€ä¸ªâ€œå‘é‡åŠ è½½å°ç»„â€**

### ç¬¬äºŒæ­¥ï¼šæŠŠ 32 ä¸ª lane æ’æˆäºŒç»´ç½‘æ ¼ï¼ˆç”»å›¾ï¼ï¼‰

ä¸€ä¸ª warp æœ‰ 32 ä¸ªçº¿ç¨‹ï¼ˆlane_id = 0~31ï¼‰ï¼Œæˆ‘ä»¬æŒ‰ N æ–¹å‘ä¼˜å…ˆæ’ï¼š

```
lane_id:  0  1  2  3    4  5  6  7  ... 28 29 30 31
         â”œâ”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”¤         â”œâ”€â”€â”€â”€â”€â”€â”€â”¤
           ç»„0           ç»„1               ç»„7
         â†‘       æ¯ç»„ 4 ä¸ªçº¿ç¨‹ï¼ˆå› ä¸º WN_per_loop = 4ï¼‰
         â””â”€â”€ WN_per_loop = 4
```

æ€»å…±æœ‰ `32 / 4 = 8` ç»„ â†’ æ­£å¥½å¯¹åº” `TN_vec_num = 8`

### ç¬¬ä¸‰æ­¥ï¼šä»£ç æ€ä¹ˆç®—å‡ºæ¯ä¸ªçº¿ç¨‹çš„â€œå­åæ ‡â€

```cpp
const int tm_vec_id = lane_id >> get_log2x<WN_per_loop / vec_size>();
const int tn_vec_id = lane_id & (WN_per_loop / vec_size - 1);
```

æ³¨æ„ï¼š`WN_per_loop / vec_size = 4 / 4 = 1` â†’ æ‰€ä»¥ï¼š

```cpp
get_log2x<1>() = 0
WN_per_loop/vec_size - 1 = 0
```

è¿™çœ‹èµ·æ¥æ˜¯ 0ï¼Ÿä¸å¯¹ï¼æˆ‘ä»¬æ¢ä¸€ä¸ªæ›´å¸¸è§çš„å‚æ•°è®©ä½ çœ‹æ¸…æ¥šã€‚

### æ›´æ¸…æ™°çš„ä¾‹å­ï¼ˆæ¨èè¿™ä¸ªè®°ä½ï¼ï¼‰

```cpp
WM = 64, WN = 64
TM = 16, TN = 32
vec_t = float4 â†’ vec_size = 4
```

è®¡ç®—ï¼š

```cpp
TN_vec_num     = 32 / 4 = 8
WN_per_loop    = 64 / 8 = 8        â† å…³é”®ï¼N æ–¹å‘éœ€è¦ 8 ä¸ªçº¿ç¨‹ä¸€ç»„
WN_per_loop/vec_size = 8/4 = 2
get_log2x<2>() = 1
```

ç°åœ¨ä»£ç å˜æˆï¼š

```cpp
const int tm_vec_id = lane_id >> 1;                    // å³ç§» 1 ä½ â†’ M æ–¹å‘åˆ†ç»„
const int tn_vec_id = lane_id & (2-1);                 // & 1 â†’ å–æœ€ä½ä½ â†’ N æ–¹å‘ç»„å†…ç¼–å·
```

### ç”»å›¾ï¼š32 ä¸ª lane å¦‚ä½•åˆ’åˆ†

```
lane_id:  0  1 |  2  3 |  4  5 | ... 30 31
         â”œâ”€â”€â”€â”€â”€â”¤â”œâ”€â”€â”€â”€â”€â”¤
          ç»„0     ç»„1    ...       â† æ¯ç»„ 2 ä¸ªçº¿ç¨‹ï¼ˆå› ä¸º WN_per_loop/vec_size=2ï¼‰
         â†‘
      tm_vec_id = lane_id >> 1 â†’ 0,0,1,1,2,2,...
      tn_vec_id = lane_id & 1  â†’ 0,1,0,1,0,1,...
```

æ¯ä¸ª (tm_vec_id, tn_vec_id) ä»£è¡¨ï¼š

| lane_id | tm_vec_id | tn_vec_id | è´Ÿè´£åŠ è½½çš„ N æ–¹å‘åç§» |
|---------|-----------|-----------|------------------------|
| 0       | 0         | 0         | åˆ— 0~3                 |
| 1       | 0         | 1         | åˆ— 4~7                 |
| 2       | 1         | 0         | åˆ— 8~11                |
| 3       | 1         | 1         | åˆ— 12~15               |
| ...     | ...       | ...       | ...                    |

â†’ æ¯ä¸¤ä¸ªçº¿ç¨‹è´Ÿè´£ 8 åˆ—ï¼ˆ2Ã—4ï¼‰ï¼Œ8 ç»„ â†’ 64 åˆ— â†’ å®Œç¾è¦†ç›– WN=64ï¼

### ç»ˆæè®°å¿†å£è¯€ï¼ˆä¸€å¥è¯è®°ä½ï¼‰

```text
ä¸€ä¸ª warp çš„ 32 ä¸ªçº¿ç¨‹ï¼Œè¦åœ¨ N æ–¹å‘ä¸Šâ€œå‘é‡åŠ è½½â€æ—¶åˆ†ç»„ï¼š

æ¯ç»„æœ‰ (WN_per_loop / vec_size) ä¸ªçº¿ç¨‹
â†’ ç”¨ tn_vec_id = lane_id & (group_size - 1) å–ç»„å†…ç¼–å·
â†’ ç”¨ tm_vec_id = lane_id >> log2(group_size) å–ç»„å·ï¼ˆM æ–¹å‘ï¼‰

è¿™æ · 32 ä¸ªçº¿ç¨‹å°±è¢«æ’æˆä¸€ä¸ªï¼š
(TM_vec_num) Ã— (WN_per_loop / vec_size) çš„å°ç½‘æ ¼
å®Œç¾è¦†ç›– warp è´Ÿè´£çš„ WM Ã— WN åŒºåŸŸ
```

### æ‰€æœ‰é¡¶çº§ GEMM éƒ½è¿™ä¹ˆå¹²çš„åŸå› 

| å†™æ³•                        | é€Ÿåº¦     | å†…å­˜å¯¹é½ | è¯´æ˜ |
|-----------------------------|----------|----------|------|
| `for(int i=0; i<8; i++) load...` | æ…¢       | å¯èƒ½ä¸å¯¹é½ | ä¼ ç»Ÿ |
| ç”¨ä½è¿ç®— + tm/tn_vec_id     | 1 å‘¨æœŸ   | å®Œç¾å¯¹é½ | ç°ä»£ |

ç°åœ¨ä½ å†çœ‹è¿™å‡ è¡Œï¼š

```cpp
constexpr int TN_vec_num = TN / vec_size;
constexpr int WN_per_loop = WN / TN_vec_num;
const int tm_vec_id = lane_id >> get_log2x<WN_per_loop/vec_size>();
const int tn_vec_id = lane_id & (WN_per_loop/vec_size-1);
```

æ˜¯ä¸æ˜¯ç¬é—´æ˜ç™½äº†ï¼Ÿ

å®ƒå°±æ˜¯åœ¨ç”¨**æœ€å¿«çš„ä½è¿ç®—**ï¼ŒæŠŠä¸€ä¸ª warp çš„ 32 ä¸ªçº¿ç¨‹ï¼Œæ’æˆä¸€ä¸ªå®Œç¾çš„äºŒç»´å‘é‡åŠ è½½ç½‘æ ¼ï¼Œè®©æ¯ä¸ªçº¿ç¨‹çŸ¥é“ï¼šâ€œæˆ‘è¯¥åŠ è½½å“ªä¸€å— shared memory/global memoryâ€ã€‚

è¿™å‡ è¡Œä»£ç ï¼Œå†³å®šäº† GEMM èƒ½ä¸èƒ½è·‘åˆ° 140+ TFLOPSã€‚ä½ ç°åœ¨å½»åº•æŒæ¡äº†ï¼

*/


/*
question16:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­
Â Â Â Â const int warp_id = tid>>5;
Â Â Â Â const int lane_id = tid & 0x1F;
Â Â Â Â static_assert(get_log2x() == 2);
Â Â Â Â const int wm_id = warp_id >> get_log2x();
Â Â Â Â const int wn_id = warp_id & (NWarps_dim_N-1);

è¿™æ®µä»£ç æ˜¯ **CUDA GEMM ä¼˜åŒ–**ä¸­çš„ä¸€ä¸ªå…³é”®éƒ¨åˆ†ï¼Œå®ƒè´Ÿè´£å°†çº¿ç¨‹å— (Block) å†…çš„**ä¸€ç»´çº¿ç¨‹ ID** (`tid`) æ˜ å°„åˆ° **äºŒç»´çš„ Warp åæ ‡** (`wm_id`, `wn_id`)ã€‚

è¿™ç§æ˜ å°„æ˜¯å®ç°\*\*åˆ†å±‚åˆ†å—ï¼ˆHierarchical Tilingï¼‰\*\*çš„åŸºç¡€ï¼Œç¡®ä¿æ¯ä¸ª Warp è´Ÿè´£è®¡ç®— C çŸ©é˜µçš„ä¸€ä¸ªç‹¬æœ‰çš„ WM * WN å­åŒºåŸŸã€‚

-----

## âš™ï¸ ç¤ºä¾‹å‚æ•°è®¾å®š

æˆ‘ä»¬ä½¿ç”¨å‰é¢ä»£ç ä¸­å…¸å‹çš„æ¨¡æ¿å‚æ•°è¿›è¡Œè§£é‡Šï¼š

  * **Block å°ºå¯¸ (CTA):** cta_size = 512 çº¿ç¨‹ (ä¾‹å¦‚ 32 * 16)
  * **Warp æ•°é‡:** 512 / 32 = 16 ä¸ª Warp
  * **Block Tile å°ºå¯¸:** BM = 256 (M ç»´åº¦), BN = 128 (N ç»´åº¦)
  * **Warp Tile å°ºå¯¸:** WM = 64 (M ç»´åº¦), WN = 32 (N ç»´åº¦)

**è®¡ç®— Warp ç»´åº¦ï¼š**

  * NWarps_dim_Nï¼ˆN ç»´åº¦ä¸Šçš„ Warp æ•°é‡ï¼‰= BN / WN = 128 / 32 = 4
  * NWarps_dim_Mï¼ˆM ç»´åº¦ä¸Šçš„ Warp æ•°é‡ï¼‰= BM / WM = 256 / 64 = 4

æ•´ä¸ª Block å…±æœ‰ 4 * 4 = 16 ä¸ª Warpã€‚

-----

## ğŸš€ I. çº¿ç¨‹ ID åˆ° Warp ID çš„è½¬æ¢

```c
const int warp_id = tid>>5;
const int lane_id = tid & 0x1F;
```

### 1\. çº¿ç¨‹ ID (`tid`)

`tid` æ˜¯å½“å‰çº¿ç¨‹åœ¨çº¿ç¨‹å—å†…çš„**ä¸€ç»´ ID**ï¼ŒèŒƒå›´æ˜¯ 0 åˆ° 511ã€‚

### 2\. è½¦é“ ID (`lane_id`)

  * **`lane_id = tid & 0x1F;`**
      * `0x1F` æ˜¯åå…­è¿›åˆ¶ï¼Œç­‰äºåè¿›åˆ¶çš„ 31 æˆ–äºŒè¿›åˆ¶çš„ `0b11111`ã€‚
      * **CUDA Warp å¤§å°ä¸º 32ã€‚** `tid & 31` æ“ä½œåˆ©ç”¨äº†è¿™ä¸€ç‚¹ï¼šå®ƒæå– `tid` çš„**æœ€ä½ 5 ä½**ã€‚
      * **ç»“æœï¼š** `lane_id` èŒƒå›´æ˜¯ 0 åˆ° 31ï¼Œè¡¨ç¤ºå½“å‰çº¿ç¨‹æ˜¯å…¶æ‰€å± Warp ä¸­çš„ç¬¬å‡ ä¸ªçº¿ç¨‹ï¼ˆå³å®ƒçš„â€œè½¦é“â€ï¼‰ã€‚

### 3\. Warp ID (`warp_id`)

  * **`warp_id = tid>>5;`**
      * `>>5` æ˜¯**å³ç§» 5 ä½**çš„æ“ä½œï¼Œç›¸å½“äºæ‰§è¡Œ**é™¤ä»¥ 2^5=32** çš„æ•´æ•°é™¤æ³•ã€‚
      * **ç»“æœï¼š** `warp_id` èŒƒå›´æ˜¯ 0 åˆ° 15ã€‚å®ƒè¡¨ç¤ºå½“å‰çº¿ç¨‹å±äºçº¿ç¨‹å—å†…çš„ç¬¬å‡ ä¸ª Warpã€‚

| tid (0-511) | tid (äºŒè¿›åˆ¶) | warp_id (\gg 5) | lane_id (\& 31) |
| :--- | :--- | :--- | :--- |
| 0 | `...000000` | 0 | 0 |
| 31 | `...011111` | 0 | 31 |
| 32 | `...100000` | 1 | 0 |
| 511 | `...111111111` | 15 | 31 |

-----

## ğŸ§­ II. Warp ID åˆ° 2D åæ ‡çš„æ˜ å°„

ç°åœ¨æˆ‘ä»¬æœ‰äº†ä¸€ä¸ª 1D çš„ `warp_id` (0 åˆ° 15)ï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶æ˜ å°„åˆ° 2D åæ ‡ (wm_id, wn_id)ï¼Œå…¶ä¸­ 0 \le wm_id < 4 (è¡Œ) å’Œ 0 \le wn_id < 4 (åˆ—)ã€‚

```c
static_assert(get_log2x<NWarps_dim_N>() == 2);
const int wm_id = warp_id >> get_log2x<NWarps_dim_N>();
const int wn_id = warp_id & (NWarps_dim_N-1);
```

### 1\. åˆ—åæ ‡ (`wn_id`)

  * **`wn_id = warp_id & (NWarps_dim_N - 1);`**
      * NWarps_dim_N = 4ã€‚å› æ­¤ NWarps_dim_N - 1 = 3 (äºŒè¿›åˆ¶ `0b0011`)ã€‚
      * **åŸç†ï¼š** `& 3` æ“ä½œæ˜¯**å–æ¨¡ 4** çš„é«˜æ•ˆä½æ“ä½œã€‚å®ƒæå– warp_id çš„**æœ€ä½ 2 ä½**ï¼Œè¿™æ­£æ˜¯ N ç»´åº¦çš„ Warp ç´¢å¼•ã€‚
      * **ç»“æœï¼š** wn_id èŒƒå›´æ˜¯ 0, 1, 2, 3ã€‚

### 2\. è¡Œåæ ‡ (`wm_id`)

  * **`wm_id = warp_id >> get_log2x<NWarps_dim_N>();`**
      * `get_log2x<4>()` çš„ç»“æœæ˜¯ 2ã€‚
      * **åŸç†ï¼š** `>> 2` æ˜¯**å³ç§» 2 ä½**çš„æ“ä½œï¼Œç›¸å½“äºæ‰§è¡Œ**é™¤ä»¥ 4** çš„æ•´æ•°é™¤æ³•ã€‚å®ƒèˆå¼ƒäº† wn_id æ‰€ä½¿ç”¨çš„æœ€ä½ 2 ä½ï¼Œä»è€Œå¾—åˆ°äº† M ç»´åº¦çš„ Warp ç´¢å¼•ã€‚
      * **ç»“æœï¼š** wm_id èŒƒå›´æ˜¯ 0, 1, 2, 3ã€‚

### 3\. æ˜ å°„ç¤ºä¾‹

| warp_id | warp_id (äºŒè¿›åˆ¶) | wn_id (åˆ—ï¼Œ\& 3) | wm_id (è¡Œï¼Œ\gg 2) | Warp è´Ÿè´£ Tile åæ ‡ |
| :--- | :--- | :--- | :--- | :--- |
| 0 | 0000 | 0 | 0 | (0, 0) |
| 1 | 0001 | 1 | 0 | (0, 1) |
| 3 | 0011 | 3 | 0 | (0, 3) |
| 4 | 0100 | 0 | 1 | (1, 0) |
| 7 | 0111 | 3 | 1 | (1, 3) |
| 15 | 1111 | 3 | 3 | (3, 3) |

**æ€»ç»“ï¼š** é€šè¿‡è¿™ä¸€å¥—é«˜æ•ˆçš„ä½æ“ä½œï¼Œç¨‹åºæˆåŠŸåœ°å°†çº¿ç¨‹å—ä¸­çš„ 16 ä¸ª Warp åˆ†é…åˆ°äº†ä¸€ä¸ª 4 * 4 çš„ç½‘æ ¼ä¸Šã€‚æ¯ä¸ª Warp (wm_id, wn_id) éšåè´Ÿè´£è®¡ç®— C çŸ©é˜µä¸­å”¯ä¸€çš„ 64 * 32 å­å—ã€‚

*/




/*
question17:

ä¸ç†è§£ï¼Œè¯·ä¸¾ä¾‹è¯´æ˜

WM_per_loop çš„å®šä¹‰å’Œç”¨é€”
WM_per_loop = \frac{Warp è´Ÿè´£çš„æ€»è¡Œæ•°(WM){çº¿ç¨‹å±€éƒ¨è´¡çŒ®çš„å‘é‡å—æ•°é‡(TM_vec_num) = \frac{64{2 = 32
è¿™ 32 çš„çœŸæ­£å«ä¹‰æ˜¯ï¼š
åœ¨åŠ è½½æ•°æ®æ—¶ï¼ŒWarp å¿…é¡»è¦†ç›– M ç»´åº¦ä¸Šçš„ 64 è¡Œæ•°æ®ã€‚å®ƒå°†è¿™ä¸ªä»»åŠ¡åˆ†é…ç»™ 32 ä¸ªçº¿ç¨‹ï¼Œè®©æ¯ä¸ªçº¿ç¨‹è´Ÿè´£ 64/32 = 2 æ¬¡ä¸åŒçš„ åŠ è½½æ“ä½œã€‚

è¿™æ˜¯ä¸€ä¸ªå…³äº **Warp å†…éƒ¨å¹¶è¡Œæ•°æ®åŠ è½½** çš„é—®é¢˜ã€‚è¦ç†è§£ä¸ºä»€ä¹ˆ 64 / 2 = 32 ä¸ªçº¿ç¨‹æ˜¯æ­£ç¡®çš„ï¼Œæˆ‘ä»¬éœ€è¦å°†æ€è€ƒçš„è§†è§’ä» **â€œå‡ ä½•åˆ’åˆ†â€** è½¬æ¢åˆ° **â€œå¹¶è¡Œåˆ†å·¥â€**ã€‚

åœ¨é«˜æ€§èƒ½çš„ CUDA çŸ©é˜µä¹˜æ³•ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯è®© **æ‰€æœ‰ 32 ä¸ªçº¿ç¨‹** éƒ½å¿™ç¢Œèµ·æ¥ï¼Œä»¥å®ç°æœ€é«˜çš„å¹¶è¡Œåº¦å’Œå†…å­˜åˆå¹¶ã€‚

---

## ğŸ’¡ é—®é¢˜çš„æ ¸å¿ƒï¼šStride-by-WarpSize å¹¶è¡ŒåŠ è½½

### 1. å‡ ä½•åˆ’åˆ† (ä¼ ç»Ÿçš„é”™è¯¯ç†è§£)

å¦‚æœç®€å•åœ°æŒ‰ M ç»´åº¦åˆ’åˆ† 64 è¡Œï¼š
64  è¡Œ / 8  è¡Œ = 8  ä¸ªçº¿ç¨‹

* **ç»“æœ:** åªæœ‰ 8 ä¸ªçº¿ç¨‹åœ¨å·¥ä½œï¼Œå¦å¤– 24 ä¸ªçº¿ç¨‹é—²ç½®ã€‚
* **é—®é¢˜:** æµªè´¹äº† 3/4 çš„ Warp è®¡ç®—åŠ›ã€‚

### 2. ä¼˜åŒ–çš„å¹¶è¡Œåˆ†å·¥ (æœ¬ä»£ç çš„æ­£ç¡®åšæ³•)

ä¼˜åŒ–çš„ç›®æ ‡æ˜¯è®© **32 ä¸ªçº¿ç¨‹** éƒ½å‚ä¸è¿›æ¥ï¼Œä»¥æœ€å¿«çš„é€Ÿåº¦è¦†ç›– 64 è¡Œæ•°æ®ã€‚

WM_per_loop = 32

**å«ä¹‰ï¼š** 32 è¡¨ç¤ºæ•´ä¸ª Warp çš„æ‰€æœ‰ 32 ä¸ªçº¿ç¨‹éƒ½è¢«æ¿€æ´»ï¼Œç”¨äºåˆ†æ‹… M ç»´åº¦ä¸Šçš„æ•°æ®åŠ è½½ä»»åŠ¡ã€‚

---

## ğŸ”¢ ç¤ºä¾‹ï¼š32 çº¿ç¨‹å¦‚ä½•åˆ†æ‹… 64 è¡Œ

æˆ‘ä»¬æ¥çœ‹ä¸€ä¸ªçº¿ç¨‹æ˜¯å¦‚ä½•åœ¨ M ç»´åº¦ä¸Šè´¡çŒ®çš„ï¼š

| å‚æ•° | å€¼ | å«ä¹‰ |
| :--- | :--- | :--- |
| WM | 64 | Warp è´Ÿè´£çš„æ€»è¡Œæ•° |
| TM_vec_num | 2 | çº¿ç¨‹å±€éƒ¨è´¡çŒ®çš„å‘é‡å—æ•°é‡ï¼ˆå¾ªç¯æ¬¡æ•°ï¼‰ |
| WM_per_loop | 32 | **M ç»´åº¦çš„å¾ªç¯æ­¥é•¿ï¼ˆçº¿ç¨‹Strideï¼‰** |

**è®¡ç®—å…¬å¼ï¼š**
æ€»è¡Œæ•°  (64) = çº¿ç¨‹æ•°  (32) * æ¯ä¸ªçº¿ç¨‹çš„åŠ è½½æ¬¡æ•°

æ¯ä¸ªçº¿ç¨‹çš„åŠ è½½æ¬¡æ•° = WM / WM_per_loop = 64 / 32 = 2

å› æ­¤ï¼Œ**æ¯ä¸ªçº¿ç¨‹éœ€è¦æ‰§è¡Œ 2 æ¬¡åŠ è½½å¾ªç¯**ï¼ˆè¿™ä¸ TM_vec_num=2 ç›¸å»åˆï¼‰ã€‚

---

### ç¤ºä¾‹å›¾è§£ï¼ˆä»…è€ƒè™‘ M ç»´åº¦ï¼‰ï¼š

M ç»´åº¦å…±æœ‰ 64 è¡Œ (0 åˆ° 63)ã€‚

#### ç¬¬ 1 æ¬¡åŠ è½½å¾ªç¯ï¼ˆtm_loop = 0ï¼‰ï¼š

æ‰€æœ‰ 32 ä¸ªçº¿ç¨‹åŒæ—¶å¯åŠ¨ã€‚æ¯ä¸ªçº¿ç¨‹è´Ÿè´£åŠ è½½ **ç¬¬ 0 ä¸ª** å‘é‡å—ã€‚

* **çº¿ç¨‹ 0** (lane 0) è´Ÿè´£åŠ è½½ï¼š**ç¬¬ 0 è¡Œ**
* **çº¿ç¨‹ 1** (lane 1) è´Ÿè´£åŠ è½½ï¼š**ç¬¬ 1 è¡Œ**
* ...
* **çº¿ç¨‹ 31** (lane 31) è´Ÿè´£åŠ è½½ï¼š**ç¬¬ 31 è¡Œ**

é€šè¿‡ä¸€æ¬¡å†…å­˜äº‹åŠ¡ï¼Œæ‰€æœ‰ 32 ä¸ªçº¿ç¨‹ï¼ˆ1 ä¸ª Warpï¼‰å¹¶è¡ŒåŠ è½½äº† 0 åˆ° 31 è¡Œçš„æ•°æ®ã€‚

#### ç¬¬ 2 æ¬¡åŠ è½½å¾ªç¯ï¼ˆtm_loop = 1ï¼‰ï¼š

æ‰€æœ‰ 32 ä¸ªçº¿ç¨‹å†æ¬¡åŒæ—¶å¯åŠ¨ã€‚è¿™æ¬¡å®ƒä»¬åŠ è½½ **ç¬¬ 1 ä¸ª** å‘é‡å—ï¼Œ**å¹¶è·³è¿‡ 32 è¡Œ**ã€‚

* **çº¿ç¨‹ 0** è´Ÿè´£åŠ è½½ï¼šç¬¬ 0 + 32 è¡Œ = **ç¬¬ 32 è¡Œ**
* **çº¿ç¨‹ 1** è´Ÿè´£åŠ è½½ï¼šç¬¬ 1 + 32 è¡Œ = **ç¬¬ 33 è¡Œ**
* ...
* **çº¿ç¨‹ 31** è´Ÿè´£åŠ è½½ï¼šç¬¬ 31 + 32 è¡Œ = **ç¬¬ 63 è¡Œ**

é€šè¿‡ç¬¬äºŒæ¬¡å†…å­˜äº‹åŠ¡ï¼Œæ‰€æœ‰ 32 ä¸ªçº¿ç¨‹å¹¶è¡ŒåŠ è½½äº† 32 åˆ° 63 è¡Œçš„æ•°æ®ã€‚

### æ€»ç»“

* **WM_per_loop = 32** ç¡®å®šäº† **çº¿ç¨‹çš„æ­¥é•¿** å’Œ **å·¥ä½œé‡**ã€‚
* æˆ‘ä»¬åˆ©ç”¨äº† **32 ä¸ªçº¿ç¨‹** çš„å…¨éƒ¨å¹¶è¡Œåº¦ï¼Œè®©å®ƒä»¬äº¤é”™ï¼ˆStridedï¼‰è®¿é—®å†…å­˜ï¼Œä»è€Œåœ¨ **2 ä¸ªå¾ªç¯æ­¥éª¤** å†…å®Œæˆäº† 64 è¡Œæ•°æ®çš„åŠ è½½ã€‚
* è¿™ç§ M ç»´åº¦ä¸Šçš„äº¤é”™è®¿é—®ï¼Œæ˜¯ç¡®ä¿å†…å­˜è®¿é—®åœ¨ K ç»´åº¦ä¸Šä¿æŒé«˜æ•ˆï¼ˆå†…å­˜åˆå¹¶ï¼‰çš„å…³é”®è®¾è®¡ã€‚
* **å› æ­¤ï¼Œæ­£ç¡®çš„é€»è¾‘æ˜¯ï¼š** 64 è¡Œæ•°æ®ç”± 32 ä¸ªçº¿ç¨‹åˆ†æ‹…ï¼Œæ¯ä¸ªçº¿ç¨‹è´Ÿè´£ 2 æ¬¡åŠ è½½ã€‚

*/


/*
question18:

my understanding

    //thread level
    constexpr int TM_vec_num = TM / vec_size;
    constexpr int TN_vec_num = TN / vec_size;
    constexpr int WM_per_loop = WM /TM_vec_num;
    constexpr int WN_per_loop = WN /TN_vec_num;
    const int tm_vec_id = lane_id >> get_log2x<WM_per_loop/vec_size>();
    const int tn_vec_id = lane_id & (WN_per_loop/vec_size-1);


1. TM_vec_num = TM / vec_size;
2. WM_per_loop = WM /TM_vec_num;
3. WM_per_loop/vec_size = WM /TM_vec_num / vec_size = WM * vec_size/ TM_vec_num / vec_size = WM / TM = å¾ªç¯æ¬¡æ•°
4. tm_vec_id = lane_id >> get_log2x<WM / TM>;


same logic:
1. TN_vec_num = TN / vec_size;
2. WN_per_loop = WN /TN_vec_num;
3. WN_per_loop/vec_size - 1 = WN /TN_vec_num / vec_size - 1 = WN * vec_size /TN / vec_size - 1 = WN / TN - 1 = å¾ªç¯æ¬¡æ•° - 1
4. tn_vec_id = lane_id & (WN / TN - 1);

*/


/*
question19:


*/


/*
question20:


*/


/*
question21:


*/


/*
question22:


*/


/*
question23:

*/


/*
question24:

*/


/*
question25:

*/




/*
question26:


*/


/*
question27:


*/


/*
question28:


*/


/*
question29:


*/


/*
question30:


*/


/*
question31:


*/


/*
question32:

*/


/*
question33:

*/


/*
question34:

*/




/*
question35:


*/


/*
question36:


*/


/*
question37:


*/


/*
question38:


*/


/*
question39:


*/


/*
question40:


*/