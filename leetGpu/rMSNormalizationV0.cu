
#include <cuda_runtime.h>
#include <math.h>
#include <vector>

//method0
///*
__global__ void PowerSum(const float* input, int N, float* blockSum) {
    int tid = threadIdx.x;
    int idx = tid + (blockDim.x * blockIdx.x);
    __shared__ float acc[256];
    float x_pow = 0.0f;
    if (idx == 0) acc[0] = 0.0f;
    __syncthreads();
    for (int i = idx; i < N; i+= blockDim.x*gridDim.x) {
        float x = input[i];
        x_pow += x * x;
    }
    acc[threadIdx.x] = x_pow;
    __syncthreads();
    // Using reduction to accumulate sum into acc[0]
    for (int stride = blockDim.x/2; stride >= warpSize; stride /= 2) {
        if (tid < stride) acc[tid] += acc[tid + stride];
        __syncthreads();
        }
    // warp shuffle 

    if (tid < warpSize) {
        float val = acc[tid];
        unsigned mask = __activemask();
        val += __shfl_down_sync(mask, val, 16);
        val += __shfl_down_sync(mask, val, 8);
        val += __shfl_down_sync(mask, val, 4);
        val += __shfl_down_sync(mask, val, 2);
        val += __shfl_down_sync(mask, val, 1);
        if (tid == 0) blockSum[blockIdx.x] = val;
    }

    // ensure single block has all accumulated sums:
   
}
__global__ void RMSNorm(const float* input, float gamma, float beta, float* output, int N, float eps, float rmsn) {
    int idx = threadIdx.x + (blockDim.x * blockIdx.x);
    for (int k = idx; k < N; k+= blockDim.x*gridDim.x) {
        float x_hat = input[k] / rmsn;
        output[k] = gamma * x_hat + beta;
    }
}

// input, output are device pointers
extern "C" void solve(const float* input, float gamma, float beta, 
                     float* output, int N, float eps) {
                        int threadsPerBlock = 256;
                        int blocksPerGrid = (threadsPerBlock + N - 1) / threadsPerBlock;
                        float* blockSum_d = nullptr;
                        cudaMalloc(&blockSum_d, blocksPerGrid*sizeof(float));
                        PowerSum<<<blocksPerGrid, threadsPerBlock>>>(input, N, blockSum_d);
                        cudaDeviceSynchronize();

                        std::vector<float> blockSum_h(blocksPerGrid);
                        cudaMemcpy(blockSum_h.data(), blockSum_d, blocksPerGrid*sizeof(float), cudaMemcpyDeviceToHost);
                        float rmsn = 0.0f;
                        for (int j = 0; j < blocksPerGrid; j++) {
                            rmsn += blockSum_h[j];
                        }
                        rmsn = sqrtf((rmsn/N) + eps);
                        RMSNorm<<<blocksPerGrid, threadsPerBlock>>>(input, gamma, beta, output, N, eps, rmsn);

}
//*/


//method1
//wrong
/*
#define WARP_SIZE       32
#define THEADPERBLOCK   (WARP_SIZE*WARP_SIZE)
#define STRIDE_LENGTH   8

__device__ __forceinline__ float warp_sum(float val) {
    // å¾—åˆ°å¤„äºæ´»è·ƒçŠ¶æ€çš„çº¿ç¨‹æ©ç 
    unsigned m = __activemask();
    val += __shfl_down_sync(m , val , 16);
    val += __shfl_down_sync(m , val , 8);
    val += __shfl_down_sync(m , val , 4);
    val += __shfl_down_sync(m , val , 2);
    val += __shfl_down_sync(m , val , 1);
    return val;
}

__global__ void PowerSum(const float* input, int N, float* output) {
    int tid = threadIdx.x;
    int idx = tid + (blockDim.x * blockIdx.x);

    float x_pow = 0.0f;
    for (int i = idx; i < N; i+= blockDim.x*gridDim.x) {
        float x = input[i];
        x_pow += x * x;
    }
    __syncthreads();

    // 1.åœ¨æ¯ä¸ª warpå†…è§„çº¦æ±‚å’Œï¼Œå¹¶å°†å…¶éƒ¨åˆ†æ±‚å’Œç»“æœå­˜å‚¨åˆ°å…±äº«å†…å­˜ä¸­
    __shared__ float shared_partial_sum[WARP_SIZE];
    int warp = tid >> 5;    // å½“å‰çº¿ç¨‹æ‰€åœ¨çš„ warpåœ¨æ•´ä¸ª warpæ•°ç»„ä¸­çš„ä¸‹æ ‡
    int lane = tid & 31;    // å½“å‰çº¿ç¨‹åœ¨å½“å‰ warpå†…çš„ä¸‹æ ‡

    float sum_val = 0;
    int wsum = warp_sum(sum_val);
    if (lane == 0) {
        shared_partial_sum[warp] = wsum;
    }
    __syncthreads();

    // 2.å°†æ¯ä¸ªå—å†…æ‰€æœ‰ warpå·²å¾—åˆ°çš„éƒ¨åˆ†æ±‚å’Œç»“æœå†è¿›è¡Œè§„çº¦æ±‚å’Œ
    if (warp == 0) {
        float partial_sum_val = shared_partial_sum[lane];
        shared_partial_sum[0] = warp_sum(partial_sum_val);
    }
     __syncthreads();

    // 3.åˆ©ç”¨åŸå­åŠ æ“ä½œ,å¯¹æ‰€æœ‰å—å†…çš„ shared_partial_sum[0]æ±‚å’Œ
    if (tid == 0) {
        atomicAdd(output, shared_partial_sum[0]);
    }

}

__global__ void RMSNorm(const float* input, float gamma, float beta, float* output, int N, float eps, float rmsn) {
    int idx = threadIdx.x + (blockDim.x * blockIdx.x);
    for (int k = idx; k < N; k+= blockDim.x*gridDim.x) {
        float x_hat = input[k] / rmsn;
        output[k] = gamma * x_hat + beta;
    }
}

// input, output are device pointers
extern "C" void solve(const float* input, float gamma, float beta, 
                     float* output, int N, float eps) {
                        int threadsPerBlock = 256;
                        int blocksPerGrid = (threadsPerBlock + N - 1) / threadsPerBlock;
                        float* blockSum_d = nullptr;
                        cudaMalloc(&blockSum_d, blocksPerGrid*sizeof(float));

                        cudaMemset(output, 0.0f, sizeof(float));
                        PowerSum<<<blocksPerGrid, threadsPerBlock>>>(input, N, output);
                        cudaDeviceSynchronize();

                        float rmsn = *output;
                        cudaMemset(output, 0.0f, sizeof(float));
                        RMSNorm<<<blocksPerGrid, threadsPerBlock>>>(input, gamma, beta, output, N, eps, rmsn);

}
//*/



//--------------------------------------------------------------------------------------------------
/*
question0:
ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

method0

è¿™æ®µä»£ç å®ç°äº†ä¸€ä¸ªè®¡ç®— **RMS å½’ä¸€åŒ–ï¼ˆRoot Mean Square Normalizationï¼‰** çš„æµç¨‹ã€‚å®ƒåˆ©ç”¨ CUDA å¹¶è¡Œè®¡ç®—å‡æ–¹æ ¹ (RMS) çš„å¹³æ–¹å’Œéƒ¨åˆ†ï¼Œç„¶åç”± CPU è®¡ç®—æœ€ç»ˆçš„ RMS å€¼ï¼Œæœ€åå†ç”± GPU å®Œæˆå½’ä¸€åŒ–å˜æ¢ã€‚

-----

## âš™ï¸ I. æ ¸å¿ƒåŸç†ï¼šRMS å½’ä¸€åŒ–

RMS å½’ä¸€åŒ–çš„åŸºæœ¬å…¬å¼æ˜¯ï¼š

RMS(x) = sqrt1/N * sum_(=1)^N x_i^2 + epsilon

Output_i = gamma * x_i / RMS(x) + beta

è¿™æ®µä»£ç çš„æµç¨‹åˆ†è§£ä¸ºï¼š

1.  **GPU (`PowerSum`):** å¹¶è¡Œè®¡ç®— sum x_i^2 çš„æ€»å’Œï¼ŒæŒ‰ Block åˆ†ç‰‡å­˜å‚¨ã€‚
2.  **CPU (`solve`):** æ”¶é›†æ‰€æœ‰ Block çš„ sum x_i^2 æ€»å’Œï¼Œè®¡ç®—æœ€ç»ˆçš„ RMS(x) å€¼ã€‚
3.  **GPU (`RMSNorm`):** ä½¿ç”¨è®¡ç®—å‡ºçš„ RMS(x) å®Œæˆæœ€ç»ˆçš„å½’ä¸€åŒ–å’Œç¼©æ”¾ (gamma, \beta) å˜æ¢ã€‚

-----

## ğŸš€ II. Kernel 1: å¹³æ–¹å’Œè®¡ç®— (`PowerSum`)

è¿™ä¸ª Kernel è´Ÿè´£è®¡ç®—æ‰€æœ‰è¾“å…¥å…ƒç´  x çš„å¹³æ–¹å’Œ sum x^2ï¼Œå¹¶å°†å…¶å½’çº¦åˆ°æ¯ä¸ª Block çš„ç»“æœä¸­ã€‚

### 1\. å±€éƒ¨å¹³æ–¹å’Œè®¡ç®— (Grid-Stride Loop)

c
// ...
for (int i = idx; i < N; i+= blockDim.x*gridDim.x) 
    float x = input[i];
    x_pow += x * x; // å±€éƒ¨ç´¯ç§¯å¹³æ–¹å’Œ

acc[threadIdx.x] = x_pow;
__syncthreads();


  * **çº¿ç¨‹åˆ†å·¥:** ä½¿ç”¨ **Grid-Stride Loop** æ¨¡å¼ã€‚æ¯ä¸ªçº¿ç¨‹ idx è´Ÿè´£å¤„ç† N ä¸ªæ•°æ®ä¸­ï¼Œä»¥ (blockDim.x * gridDim.x) ä¸ºæ­¥é•¿çš„ä¸€ç³»åˆ—å…ƒç´ ã€‚
  * **ç›®çš„:** ä¿è¯æ•´ä¸ªæ•°æ®é›† N çš„æ¯ä¸ªå…ƒç´ éƒ½è¢«ä¸€ä¸ªçº¿ç¨‹å¤„ç†åˆ°ã€‚
  * **ç»“æœ:** æ¯ä¸ªçº¿ç¨‹ tx çš„å±€éƒ¨å¹³æ–¹å’Œ x_pow è¢«å†™å…¥ Shared Memory æ•°ç»„ acc[tx] ä¸­ã€‚

### 2\. å—å†…å½’çº¦ (ä¸¤çº§å½’çº¦)

**A. Shared Memory å½’çº¦ (ç²—ç²’åº¦):**

c
for (int stride = blockDim.x/2; stride >= warpSize; stride /= 2) 
    if (tid < stride) acc[tid] += acc[tid + stride];
    __syncthreads();



  * **ç›®çš„:** å°† 256 ä¸ªå±€éƒ¨å’Œå½’çº¦åˆ° acc[0] åˆ° acc[31] (ç¬¬ä¸€ä¸ª Warp çš„åŒºåŸŸ) ä¸­ã€‚
  * **æ­¥é•¿:** å½’çº¦åˆ° warpSize=32 å¤„åœæ­¢ã€‚

**B. Warp Shuffle å½’çº¦ (ç»†ç²’åº¦):**

c
if (tid < warpSize) 
    // ... Shuffle down reduction ...
    val += __shfl_down_sync(mask, val, 1);
    if (tid == 0) blockSum[blockIdx.x] = val; // çº¿ç¨‹ 0 å†™å…¥å…¨å±€ç»“æœ



  * **ç›®çš„:** ä½¿ç”¨æœ€å¿«çš„ **Warp Shuffle æŒ‡ä»¤** å°†å‰©ä¸‹çš„ 32 ä¸ªå€¼å½’çº¦æˆä¸€ä¸ªæ€»å’Œ valã€‚
  * **ç»“æœ:** æœ€ç»ˆçš„æ€»å’Œå­˜å‚¨åœ¨çº¿ç¨‹ 0 çš„ val å˜é‡ä¸­ï¼Œç„¶åè¢«å†™å…¥å…¨å±€æ•°ç»„ blockSum[blockIdx.x]ã€‚

-----

## ğŸ’» III. Host ç«¯è®¡ç®— RMS (solve å‡½æ•°)

ä¸»æœºç«¯è´Ÿè´£æ”¶é›† GPU çš„éƒ¨åˆ†ç»“æœï¼Œå¹¶è®¡ç®—æœ€ç»ˆçš„ RMS å€¼ã€‚

c
// ... Copy back to host
std::vector<float> blockSum_h(blocksPerGrid);
cudaMemcpy(blockSum_h.data(), blockSum_d, blocksPerGrid*sizeof(float), cudaMemcpyDeviceToHost);

float rmsn = 0.0f;
for (int j = 0; j < blocksPerGrid; j++) 
    rmsn += blockSum_h[j]; // ç´¯åŠ æ‰€æœ‰ Block çš„å¹³æ–¹å’Œ


// æœ€ç»ˆ RMS å…¬å¼è®¡ç®—
rmsn = sqrtf((rmsn/N) + eps);


  * **ç›®çš„:** ä¸²è¡Œç´¯åŠ æ‰€æœ‰ Block çš„å¹³æ–¹å’Œ (rmsn = sum (sum x^2))ã€‚
  * **RMS è®¡ç®—:** åº”ç”¨ RMS å…¬å¼ï¼šrmsn = sqrt(sum x^2 / N) + epsilonã€‚

> **ç¤ºä¾‹:** å‡è®¾ N=100ï¼ŒBlock 0 çš„å¹³æ–¹å’Œæ˜¯ 800ï¼ŒBlock 1 çš„å¹³æ–¹å’Œæ˜¯ 200ã€‚
>
> 1.  rmsn (ç´¯åŠ ) = 800 + 200 = 1000.
> 2.  rmsn = sqrt(1000 / 100) + epsilon = sqrt(10 + epsilon)ã€‚

-----

## ğŸ IV. Kernel 2: RMS å½’ä¸€åŒ–å˜æ¢ (`RMSNorm`)

è¿™ä¸ª Kernel ä½¿ç”¨ CPU è®¡ç®—å‡ºçš„ RMS å€¼ï¼Œå®Œæˆæœ€ç»ˆçš„æ ‡å‡†åŒ–å’Œç¼©æ”¾ã€‚

c
for (int k = idx; k < N; k+= blockDim.x*gridDim.x) 
    float x_hat = input[k] / rmsn; // å½’ä¸€åŒ– (x / RMS)
    output[k] = gamma * x_hat + beta; // ç¼©æ”¾å’Œåç§»



  * **åˆ†å·¥:** å†æ¬¡ä½¿ç”¨ **Grid-Stride Loop** æ¨¡å¼ï¼Œæ¯ä¸ªçº¿ç¨‹ k è´Ÿè´£å¤„ç† input æ•°ç»„ä¸­çš„å¤šä¸ªå…ƒç´ ã€‚
  * **è®¡ç®—:** å¯¹æ¯ä¸ªå…ƒç´  x_k æ‰§è¡Œ RMS å½’ä¸€åŒ–å…¬å¼ y_k = gamma ... (x_k / rmsn) + \betaã€‚
  * **å†™å›:** ç»“æœå†™å…¥ output æ•°ç»„ã€‚

//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question1:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question2:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­



//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question3:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­




//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question4:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­



//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question5:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­


//--------------------------------------------------------------------------------------------------




//--------------------------------------------------------------------------------------------------
/*
question6:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œæœ‰ä¾‹å­




//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question7:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­




//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question8:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­



//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question10:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question11:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question12:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question13:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question14:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question15:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question16:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question17:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question18:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question19:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question20:


//--------------------------------------------------------------------------------------------------
