#include <cuda_runtime.h>

const int TILE = 32;
__global__ void batchnorm(const float* input, const float* gamma, const float* beta, 
                     float* output, int N, int C, float eps) {
        __shared__ float sum[TILE][TILE];
        __shared__ float var[TILE][TILE];

        int local_row = threadIdx.y, local_col = threadIdx.x;
        int channel = blockIdx.x * blockDim.x + local_col;

        float s = 0.0f, v = 0.0f;

        if (channel < C) {
            for (int i = threadIdx.y; i < N; i += blockDim.y) {
                float val = input[i * C + channel]; // sweep across i's 
                s += val;
                v += val * val;
            }
        }

        sum[local_row][local_col] = s;
        var[local_row][local_col] = v;
        __syncthreads();

        for(int j = blockDim.y / 2; j > 0; j >>= 1){
            if(threadIdx.y < j){
                sum[local_row][local_col] += sum[local_row + j][local_col];
                var[local_row][local_col] += var[local_row + j][local_col];
            }
            __syncthreads();
        }

        float mean = sum[0][threadIdx.x] / static_cast<float>(N);
        float inv_std = fmaxf(var[0][threadIdx.x] / static_cast<float>(N) - mean * mean, 0.0f);
        inv_std = rsqrtf(inv_std + eps);

        float g = (channel < C) ? gamma[channel] : 0.0f;
        float b = (channel < C) ? beta[channel] : 0.0f;

        for (int i = local_row; i < N; i += blockDim.y) {
            if (channel < C ) {
                output[i * C + channel] = g * ((input[i * C + channel] - mean) * inv_std) + b;
            }
        }

}

extern "C" void solve(const float* input, const float* gamma, const float* beta, 
                     float* output, int N, int C, float eps) {
                        dim3 threadsPerBlock(32, 32);
                        dim3 blocksPerGrid((C + threadsPerBlock.x -1) / threadsPerBlock.x, 1);
                        batchnorm<<<blocksPerGrid, threadsPerBlock>>>(input, gamma, beta, output, N, C, eps);
}




//--------------------------------------------------------------------------------------------------
/*
question0:
ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

è¿™æ®µä»£ç å®ç°äº†ä¸€ä¸ª **æ‰¹æ ‡å‡†åŒ–ï¼ˆBatch Normalization, BNï¼‰** çš„ CUDA Kernelã€‚å®ƒçš„ç›®æ ‡æ˜¯åœ¨ GPU ä¸Šé«˜æ•ˆåœ°è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„**å‡å€¼ï¼ˆMeanï¼‰å’Œæ–¹å·®ï¼ˆVarianceï¼‰**ï¼Œç„¶ååº”ç”¨ BN å˜æ¢æ¥æ ‡å‡†åŒ–æ•°æ®ã€‚

è¯¥ Kernel é‡‡ç”¨äº† **Shared Memory Reductionï¼ˆå…±äº«å†…å­˜å½’çº¦ï¼‰** ç­–ç•¥æ¥åŠ é€Ÿå‡å€¼å’Œæ–¹å·®çš„è®¡ç®—ã€‚

-----

## âš™ï¸ I. æ ¸å¿ƒæ•°å­¦åŸç†ï¼šæ‰¹æ ‡å‡†åŒ–

æ‰¹æ ‡å‡†åŒ–çš„å˜æ¢å…¬å¼ä¸ºï¼š

hat(x_i) = (x_i - mu)/sqrt(sigma^2 + epsilon)
y_i = gamma hat(x_i) + beta

å…¶ä¸­ï¼š

  * mu (å‡å€¼) = 1/N sum_(i=1^N x_i)
  * sigma^2 (æ–¹å·®) = 1 / N * sum_(i=1^N (x_i - mu)^2 = 1 / N  * sum x_i^2 - mu^2 (ä½¿ç”¨æ–¹å·®çš„ç®€åŒ–å…¬å¼)

è¿™æ®µ Kernel çš„è®¡ç®—æµç¨‹å°±æ˜¯ï¼š**è®¡ç®— sum x_i å’Œ sum x_i^2 --> è®¡ç®— mu å’Œ sigma^2 --> åº”ç”¨ gamma, beta å˜æ¢ã€‚**

## ğŸš€ II. Kernel å†…éƒ¨æµç¨‹ (`batchnorm`)

æˆ‘ä»¬å‡è®¾ TILE=32ï¼ŒBlock å°ºå¯¸ä¸º 32 * 32 (blockDim.x=32, blockDim.y=32)ã€‚

### 1\. çº¿ç¨‹èº«ä»½ä¸åˆ†å·¥

  * **channel (åˆ—):** channel = blockIdx.x * 32 + local_colã€‚æ¯ä¸ªçº¿ç¨‹å—è´Ÿè´£è®¡ç®— 32 ä¸ªé€šé“çš„ç»Ÿè®¡é‡ï¼Œæ¯ä¸ªçº¿ç¨‹ local_col è´Ÿè´£ä¸€ä¸ªé€šé“ã€‚
  * **åˆ†å·¥:** çº¿ç¨‹å—çš„ x ç»´åº¦ (åˆ—) è´Ÿè´£**é€šé“**çš„å¹¶è¡ŒåŒ–ï¼Œy ç»´åº¦ (è¡Œ) è´Ÿè´£**æ ·æœ¬æ±‚å’Œ**çš„å¹¶è¡ŒåŒ–ã€‚

### 2\. ç¬¬ä¸€é˜¶æ®µï¼šå±€éƒ¨æ±‚å’Œ (Partial Summation)

c
if (channel < C) {
    for (int i = threadIdx.y; i < N; i += blockDim.y) {
        float val = input[i * C + channel]; // sweep across i's
        s += val;
        v += val * val;
    

sum[local_row][local_col] = s;
var[local_row][local_col] = v;
__syncthreads();


  * **ç›®çš„:** æ¯ä¸ªçº¿ç¨‹ç´¯ç§¯å®ƒè´Ÿè´£çš„æ ·æœ¬çš„ x å’Œ x^2 çš„å±€éƒ¨å’Œã€‚
  * **å¾ªç¯ (i):** çº¿ç¨‹ä½¿ç”¨ **Block-Stride Loop** (`i += blockDim.y`) éå†å®ƒè´Ÿè´£çš„æ‰€æœ‰æ ·æœ¬ã€‚
      * çº¿ç¨‹ t è´Ÿè´£ i = t, t+32, t+64, \dots è¡Œçš„æ ·æœ¬ã€‚
  * **Shared Memory å­˜å‚¨:** å±€éƒ¨æ±‚å’Œç»“æœ (s, v) è¢«å†™å…¥ sum å’Œ var å…±äº«å†…å­˜æ•°ç»„ã€‚
  * **`__syncthreads()`:** ç¡®ä¿æ‰€æœ‰çº¿ç¨‹éƒ½å®Œæˆäº†å±€éƒ¨æ±‚å’Œå’Œå†™å…¥ Shared Memoryã€‚

> **ç¤ºä¾‹:** å‡è®¾ N=1024 (æ ·æœ¬æ•°)ã€‚æ¯ä¸ªçº¿ç¨‹ threadIdx.y è´Ÿè´£ 1024 / 32 = 32 ä¸ªæ ·æœ¬ã€‚sum[ty][tx] å­˜å‚¨äº†çº¿ç¨‹ tx æ‰€è´Ÿè´£é€šé“ä¸Šï¼Œå®ƒé‚£ 32 ä¸ªæ ·æœ¬çš„ sum x å€¼ã€‚

### 3\. ç¬¬äºŒé˜¶æ®µï¼šShared Memory å½’çº¦ (Reduction)

c
for(int j = blockDim.y / 2; j > 0; j >>= 1){
    if(threadIdx.y < j){
        sum[local_row][local_col] += sum[local_row + j][local_col];
        // ...
    
    __syncthreads();



  * **ç›®çš„:** å°† 32 è¡Œ (y ç»´åº¦) çš„ sum å’Œ var å±€éƒ¨ç»“æœç´¯åŠ åˆ°**ç¬¬ 0 è¡Œ**ã€‚
  * **æœºåˆ¶:** è¿™æ˜¯ä¸€ä¸ªç»å…¸çš„ **å¹¶è¡Œæ ‘å½¢å½’çº¦** ç®—æ³•ã€‚
      * j ä» 16 å¼€å§‹ï¼Œå‡åŠåˆ° 1ã€‚
      * åªæœ‰å‰åŠéƒ¨åˆ†çš„çº¿ç¨‹ (threadIdx.y < j) å‚ä¸ç´¯åŠ ã€‚
  * **ç»“æœ:** å¾ªç¯ç»“æŸåï¼Œsum[0][threadIdx.x] å­˜å‚¨äº†**è¯¥é€šé“æ‰€æœ‰æ ·æœ¬çš„ sum x æ€»å’Œ**ã€‚var[0][threadIdx.x] å­˜å‚¨äº† sum x^2 çš„æ€»å’Œã€‚

### 4\. å‡å€¼ã€æ–¹å·®å’Œæ ‡å‡†å·®è®¡ç®—

c
float mean = sum[0][threadIdx.x] / static_cast<float>(N);
float inv_std = fmaxf(var[0][threadIdx.x] / static_cast<float>(N) - mean * mean, 0.0f);
inv_std = rsqrtf(inv_std + eps);


  * **å‡å€¼ (mu):** mu = sum x / Nã€‚
  * **æ–¹å·® (sigma^2):** sigma^2 = (sum x^2 / N) - mu^2ã€‚ä½¿ç”¨ `fmaxf(..., 0.0f)` ç¡®ä¿æ–¹å·®ä¸ä¸ºè´Ÿã€‚
  * **é€†æ ‡å‡†å·® (inv_std):** \frac{1{sqrt(sigma^2 + epsilonã€‚ä½¿ç”¨ `rsqrtf` (å¿«é€Ÿå€’æ•°å¹³æ–¹æ ¹) æé«˜æ•ˆç‡ã€‚

### 5\. æœ€ç»ˆç»“æœå†™å›

c
float g = (channel < C) ? gamma[channel] : 0.0f;
float b = (channel < C) ? beta[channel] : 0.0f;

for (int i = local_row; i < N; i += blockDim.y) {
    if (channel < C ) {
        // BN å˜æ¢: y = gamma * ((x - mu) * inv_std) + beta
        output[i * C + channel] = g * ((input[i * C + channel] - mean) * inv_std) + b;
    



  * **åˆ†å·¥:** çº¿ç¨‹ (ty, tx) å†æ¬¡ä½¿ç”¨ **Block-Stride Loop** éå†å®ƒè´Ÿè´£çš„**æ‰€æœ‰æ ·æœ¬**ï¼ˆi å¾ªç¯ï¼‰ã€‚
  * **è®¡ç®—:** åº”ç”¨ BN å…¬å¼ï¼Œä½¿ç”¨ gamma (g) å’Œ beta (b) å¯¹æ¯ä¸ªæ ·æœ¬çš„é€šé“å€¼è¿›è¡Œæ ‡å‡†åŒ–å’Œç¼©æ”¾åç§»ã€‚
  * **å†™å›:** ç»“æœå†™å…¥ output æ•°ç»„ã€‚

-----

## ğŸ’» III. ä¸»æœºç«¯ (`solve` å‡½æ•°)

`solve` å‡½æ•°é…ç½®äº† Grid å’Œ Blockï¼š

c
dim3 threadsPerBlock(32, 32);
dim3 blocksPerGrid((C + threadsPerBlock.x -1) / threadsPerBlock.x, 1);


  * **Grid ç»´åº¦:** y ç»´åº¦ä¸º 1ã€‚ x ç»´åº¦è¢«è®¾ç½®ä¸ºè¦†ç›–æ‰€æœ‰ C ä¸ªé€šé“ï¼ˆå‘ä¸Šå–æ•´é™¤æ³•ï¼‰ã€‚
  * **åˆ†å·¥:** æ¯ä¸ª Block è´Ÿè´£å¤„ç† **32 ä¸ªé€šé“**çš„å®Œæ•´ç»Ÿè®¡è®¡ç®—ã€‚

//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question1:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

è¯·ç»™å‡ºinputæ•°æ®æ ¼å¼ï¼Œè¯·ç»™å‡ºä¾‹å­

è¿™æ®µä»£ç çš„ Kernel (`batchnorm`) æ˜¯ä¸ºå¤„ç†ä¸€ä¸ª**äºŒç»´æ•°æ®ï¼ˆæ ·æœ¬ x ç‰¹å¾/é€šé“ï¼‰** è€Œè®¾è®¡çš„ï¼Œé€šå¸¸ç”¨äºç¥ç»ç½‘ç»œä¸­çš„æ¿€æ´»è¾“å…¥ã€‚

-----

## âš™ï¸ I. è¾“å…¥æ•°æ®æ ¼å¼ï¼šè¡Œä¸»åº (Row-Major)

è¾“å…¥æ•°æ® input è¢«è§†ä¸ºä¸€ä¸ª N * C çš„çŸ©é˜µï¼Œåœ¨å†…å­˜ä¸­æŒ‰è¡Œä¸»åºï¼ˆRow-Majorï¼‰å­˜å‚¨ä¸ºä¸€ç»´æ•°ç»„ã€‚

  * **ç»´åº¦:** N (æ ·æœ¬/æ‰¹æ¬¡å¤§å°) * C (ç‰¹å¾/é€šé“æ•°é‡)
  * **å†…å­˜å¸ƒå±€:** å…ƒç´  X_{i, j çš„ä¸€ç»´ç´¢å¼•æ˜¯ï¼ši * C + jã€‚

| å˜é‡ | å«ä¹‰ |
| :--- | :--- |
| N | æ ·æœ¬æ€»æ•° (è¡Œæ•°) |
| C | é€šé“/ç‰¹å¾æ€»æ•° (åˆ—æ•°) |

### Kernel ä¸­çš„è®¿é—®æ–¹å¼

Kernel åœ¨è®¡ç®— sum x å’Œ sum x^2 æ—¶ï¼Œè®¿é—®æ¨¡å¼æ˜¯**æŒ‰åˆ— (Channel) å‚ç›´æ‰«æ**ï¼š

c
// è®¿é—®ç´¢å¼•: i * C + channel
// çº¿ç¨‹ x ç»´åº¦å›ºå®š channelï¼Œçº¿ç¨‹ y ç»´åº¦å¾ªç¯éå†æ‰€æœ‰ i (æ ·æœ¬)
float val = input[i * C + channel]; 


-----

## ğŸ”¢ II. ç¤ºä¾‹ï¼šè¾“å…¥æ•°æ®å¸ƒå±€

å‡è®¾æˆ‘ä»¬è¦è®¡ç®— N=4 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰ C=3 ä¸ªé€šé“/ç‰¹å¾ã€‚

  * **çŸ©é˜µå°ºå¯¸:** 4 * 3
  * **æ€»å…ƒç´ æ•°:** 4 * 3 = 12 ä¸ª `float`ã€‚

### åŸå§‹çŸ©é˜µ X:

X = \begin{pmatrix X_{0,0 & X_{0,1 & X_{0,2 \\ X_{1,0 & X_{1,1 & X_{1,2 \\ X_{2,0 & X_{2,1 & X_{2,2 \\ X_{3,0 & X_{3,1 & X_{3,2 \end{pmatrix

### å†…å­˜ä¸­çš„ä¸€ç»´å­˜å‚¨ (input):

| ç´¢å¼• | å€¼ | å½’å± (æ ·æœ¬, é€šé“) | æ„ä¹‰ï¼š**æ‰€æœ‰é€šé“ 0 æ”¾åœ¨ä¸€èµ·ï¼Œæ‰€æœ‰é€šé“ 1 æ”¾åœ¨ä¸€èµ·ï¼Œ...** (è¿™æ˜¯ BatchNorm çš„è®¡ç®—æ–¹å¼) |
| :--- | :--- | :--- | :--- |
| 0 | X_{0,0 | (æ ·æœ¬ 0, é€šé“ 0) | **é€šé“ 0** |
| 1 | X_{0,1 | (æ ·æœ¬ 0, é€šé“ 1) | |
| 2 | X_{0,2 | (æ ·æœ¬ 0, é€šé“ 2) | |
| 3 | X_{1,0 | (æ ·æœ¬ 1, é€šé“ 0) | **é€šé“ 0** |
| 4 | X_{1,1 | (æ ·æœ¬ 1, é€šé“ 1) | |
| 5 | X_{1,2 | (æ ·æœ¬ 1, é€šé“ 2) | |
| 6 | X_{2,0 | (æ ·æœ¬ 2, é€šé“ 0) | **é€šé“ 0** |
| 7 | X_{2,1 | (æ ·æœ¬ 2, é€šé“ 1) | |
| 8 | X_{2,2 | (æ ·æœ¬ 2, é€šé“ 2) | |
| 9 | X_{3,0 | (æ ·æœ¬ 3, é€šé“ 0) | **é€šé“ 0** |
| 10 | X_{3,1 | (æ ·æœ¬ 3, é€šé“ 1) | |
| 11 | X_{3,2 | (æ ·æœ¬ 3, é€šé“ 2) | |

### ğŸ¯ çº¿ç¨‹çš„å®é™…æ“ä½œï¼š

å‡è®¾çº¿ç¨‹ tx=0 è´Ÿè´£è®¡ç®— **é€šé“ 0** çš„å‡å€¼ mu_0 å’Œæ–¹å·® sigma_0^2ã€‚

è¯¥çº¿ç¨‹ä¼šéå†æ‰€æœ‰æ ·æœ¬ï¼Œè¯»å–ï¼š

  * i=0: X_{0, 0 (ç´¢å¼• 0)
  * i=1: X_{1, 0 (ç´¢å¼• 3)
  * i=2: X_{2, 0 (ç´¢å¼• 6)
  * i=3: X_{3, 0 (ç´¢å¼• 9)

å³ï¼šè¯¥çº¿ç¨‹æ²¿ç€ C ç»´åº¦ä»¥ 3 ä¸ºæ­¥é•¿è¿›è¡Œæ‰«æã€‚

**ç»“è®ºï¼š** æ‚¨çš„æ•°æ®éœ€è¦æ˜¯ **è¡Œä¸»åº** å­˜å‚¨ï¼Œå…¶ä¸­ N æ˜¯è¡Œæ•°ï¼ŒC æ˜¯åˆ—æ•°ã€‚

//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question2:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­ã€‚
è¯·è§£é‡Šæ¯ä¸ªçº¿ç¨‹æ˜¯å¦‚ä½•åˆ†é…åˆ°æ¯ä¸ªsampleå’Œæ¯ä¸ªchannelçš„

1.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dim3 threadsPerBlock(32, 32);
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dim3 blocksPerGrid((C + threadsPerBlock.x -1) / threadsPerBlock.x, 1);


2. ä½ è¯´ï¼šåˆ†å·¥: çº¿ç¨‹å—çš„ x ç»´åº¦ (åˆ—) è´Ÿè´£é€šé“çš„å¹¶è¡ŒåŒ–ï¼Œy ç»´åº¦ (è¡Œ) è´Ÿè´£æ ·æœ¬æ±‚å’Œçš„å¹¶è¡ŒåŒ–ã€‚


è¿™æ˜¯ä¸€ä¸ªéå¸¸å¥½çš„é—®é¢˜ã€‚ç†è§£ CUDA æ˜¯å¦‚ä½•å°†å·¥ä½œåˆ†é…åˆ° N (æ ·æœ¬) å’Œ C (é€šé“/ç‰¹å¾) ä¸¤ä¸ªç»´åº¦ä¸Šçš„ï¼Œæ˜¯ç†è§£è¿™ä¸ªå¹¶è¡Œç®—æ³•çš„å…³é”®ã€‚

è¿™ä¸ª Kernel é‡‡ç”¨äº† **äºŒç»´çº¿ç¨‹å—** ç»“æ„ï¼Œå°†ä¸¤ä¸ªä¸åŒçš„è®¡ç®—ä»»åŠ¡åˆ†é…ç»™äº†çº¿ç¨‹å—çš„ X è½´å’Œ Y è½´ã€‚

-----

## âš™ï¸ I. çº¿ç¨‹å—é…ç½®ä¸åˆ†å·¥æ€»è§ˆ

æˆ‘ä»¬ä»¥ä¸€ä¸ªå…·ä½“çš„ä¾‹å­æ¥è§£é‡Šã€‚

### ç¤ºä¾‹é…ç½®

| å‚æ•° | è®¾å®šå€¼ | å«ä¹‰ |
| :--- | :--- | :--- |
| **N (æ ·æœ¬æ•°)** | 128 | æ€»å…±æœ‰ 128 è¡Œæ•°æ® |
| **C (é€šé“æ•°)** | 64 | æ€»å…±æœ‰ 64 åˆ—æ•°æ® |
| **threadsPerBlock** | (32, 32) | çº¿ç¨‹å—æ˜¯ 32 * 32 = 1024 ä¸ªçº¿ç¨‹ |

### ç»´åº¦åˆ†å·¥

  * **X ç»´åº¦ (åˆ— threadIdx.x):** è´Ÿè´£ **é€šé“ï¼ˆCï¼‰** çš„å¹¶è¡ŒåŒ–ã€‚æ¯ä¸ªçº¿ç¨‹åˆ—è´Ÿè´£å¤„ç†ä¸€ä¸ªå®Œæ•´çš„é€šé“ã€‚
  * **Y ç»´åº¦ (è¡Œ threadIdx.y):** è´Ÿè´£ **æ ·æœ¬ï¼ˆNï¼‰** ä¸Šçš„æ±‚å’Œå½’çº¦ã€‚

## ğŸš€ II. åˆ†é…åˆ°é€šé“ (X ç»´åº¦)

Grid çš„é…ç½®ç¡®ä¿äº†æ‰€æœ‰ C ä¸ªé€šé“éƒ½è¢«è¦†ç›–ã€‚

### 1\. Grid é…ç½®è§£æ

c
dim3 blocksPerGrid((C + threadsPerBlock.x -1) / threadsPerBlock.x, 1);
// C=64, threadsPerBlock.x=32ã€‚ blocksPerGrid.x = (64 + 31) / 32 = 2


  * Grid ç»´åº¦æ˜¯ (2, 1)ã€‚è¿™æ„å‘³ç€æ€»å…±æœ‰ **2 ä¸ªçº¿ç¨‹å—**åœ¨ X æ–¹å‘ä¸Šè¿è¡Œã€‚

### 2\. çº¿ç¨‹åˆ°é€šé“çš„æ˜ å°„

æ¯ä¸ªçº¿ç¨‹çš„ X ç´¢å¼• (`threadIdx.x`) ç»“åˆå…¶ Block ID (`blockIdx.x`) ç¡®å®šå®ƒè´Ÿè´£å“ªä¸ªé€šé“ã€‚

channel = blockIdx.x * blockDim.x + threadIdx.x

| Block ID blockIdx.x | çº¿ç¨‹ threadIdx.x èŒƒå›´ | è®¡ç®— channel èŒƒå›´ | è´Ÿè´£çš„é€šé“ |
| :--- | :--- | :--- | :--- |
| **0** | 0 åˆ° 31 | 0 * 32 + (0 åˆ° 31) | 0 åˆ° 31 |
| **1** | 0 åˆ° 31 | 1 * 32 + (0 åˆ° 31) | 32 åˆ° 63 |

**ç»“è®ºï¼š** åœ¨ X è½´ä¸Šï¼Œ**æ¯ 32 ä¸ªçº¿ç¨‹çš„åˆ—** æ„æˆä¸€ä¸ª Blockï¼Œå®ƒä»¬å…±åŒå¤„ç† 32 ä¸ªé€šé“ï¼Œæ¯ä¸ªçº¿ç¨‹è´Ÿè´£å…¶ä¸­ä¸€ä¸ªé€šé“çš„**æ‰€æœ‰åç»­è®¡ç®—**ã€‚

-----

## ğŸ§  III. åˆ†é…åˆ°æ ·æœ¬æ±‚å’Œ (Y ç»´åº¦)

æ¯ä¸ªçº¿ç¨‹åˆ—ï¼ˆå³æ¯ä¸ªé€šé“ï¼‰ç°åœ¨æœ‰ 32 ä¸ªçº¿ç¨‹åœ¨ Y è½´ä¸Šï¼ˆ`threadIdx.y`ï¼‰åä½œï¼Œè®¡ç®—è¿™ä¸ªé€šé“çš„ sum x å’Œ sum x^2ã€‚

### 1\. å±€éƒ¨æ±‚å’Œçš„åˆ†æ‘Š

è¿™æ˜¯é€šè¿‡ **Block-Stride Loop** å®ç°çš„ã€‚

c
for (int i = threadIdx.y; i < N; i += blockDim.y) {
    float val = input[i * C + channel]; // æ‰«è¿‡æ‰€æœ‰æ ·æœ¬ i
    s += val; // å±€éƒ¨å’Œ
    v += val * val; // å±€éƒ¨å¹³æ–¹å’Œ



å‡è®¾æˆ‘ä»¬èšç„¦äº **é€šé“ 0** (ç”±çº¿ç¨‹ threadIdx.x=0 è´Ÿè´£)ã€‚

| çº¿ç¨‹ Y ç´¢å¼• threadIdx.y | è´Ÿè´£çš„æ ·æœ¬ i (æ­¥é•¿ 32) | å±€éƒ¨æ±‚å’Œé¡¹æ•° |
| :--- | :--- | :--- |
| **0** | 0, 32, 64, 96 | 4 |
| **1** | 1, 33, 65, 97 | 4 |
| **31** | 31, 63, 95, 127 | 4 |

**ç»“è®ºï¼š** åœ¨ Y è½´ä¸Šï¼Œ**32 ä¸ªçº¿ç¨‹** å°† 128 ä¸ªæ ·æœ¬çš„æ±‚å’Œä»»åŠ¡å‡åŒ€åˆ†æ‘Šï¼Œæ¯ä¸ªçº¿ç¨‹è®¡ç®— 4 ä¸ªæ ·æœ¬çš„å±€éƒ¨ sum x å’Œ sum x^2ã€‚

### 2\. å½’çº¦åˆ°æœ€ç»ˆç»Ÿè®¡é‡

å±€éƒ¨æ±‚å’Œå®Œæˆåï¼Œ32 ä¸ªå±€éƒ¨ç»“æœå­˜å‚¨åœ¨ sum[0\dots31][tx] ä¸­ã€‚

**å½’çº¦å¾ªç¯** (ä½¿ç”¨ Shared Memory æ ‘å½¢å½’çº¦) å°†è¿™ 32 ä¸ªå±€éƒ¨å’Œç´¯åŠ åˆ° sum[0][tx] ä¸­ã€‚

mean = sum[0][tx] / N
var = var[0][tx] / N - mean^2

**ç»“è®ºï¼š** æ¯ä¸ªé€šé“ï¼ˆç”±ä¸€ä¸ª X çº¿ç¨‹åˆ—ä»£è¡¨ï¼‰ç°åœ¨å·²ç»é«˜æ•ˆåœ°è®¡ç®—å‡ºäº†è¯¥é€šé“çš„ **æœ€ç»ˆ mu å’Œ sigma^2**ï¼Œå­˜å‚¨åœ¨ sum[0][tx] å’Œ var[0][tx] ä¸­ã€‚

### 3\. åº”ç”¨ BN å˜æ¢ (ç»“æœå†™å›)

æœ€åï¼Œæ‰€æœ‰çº¿ç¨‹å†æ¬¡è¿›å…¥ Block-Stride Loopï¼Œä½¿ç”¨åˆšæ‰è®¡ç®—å‡ºçš„ mean å’Œ inv_std åº”ç”¨æ ‡å‡†åŒ–å…¬å¼ï¼š

c
for (int i = local_row; i < N; i += blockDim.y) {
    output[i * C + channel] = g * ((input[i * C + channel] - mean) * inv_std) + b;



  * **åˆ†å·¥ä¸æ±‚å’Œé˜¶æ®µç›¸åŒï¼š** çº¿ç¨‹ threadIdx.y è´Ÿè´£å¯¹ç›¸åŒçš„ 4 ä¸ªæ ·æœ¬åº”ç”¨ BN å˜æ¢ï¼Œå¹¶å°†ç»“æœå†™å› outputã€‚

è¿™ç§ 32 * 32 çš„äºŒç»´åˆ†å·¥ï¼Œå®Œç¾åœ°å°† C è½´çš„ç‹¬ç«‹æ€§ï¼ˆé€šé“ç»Ÿè®¡é‡äº’ä¸å¹²æ‰°ï¼‰ä¸ N è½´çš„ä¾èµ–æ€§ï¼ˆæ ·æœ¬å¿…é¡»è¢«æ±‚å’Œï¼‰ç»“åˆèµ·æ¥ï¼Œå®ç°äº†é«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—ã€‚

//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question3:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

ä¸ºå•¥è¯´å±€éƒ¨æ±‚å’Œé¡¹æ•°æ˜¯4ï¼Ÿ

çº¿ç¨‹ Y ç´¢å¼• threadIdx.y     è´Ÿè´£çš„æ ·æœ¬ i (æ­¥é•¿ 32)          å±€éƒ¨æ±‚å’Œé¡¹æ•°
0                            0, 32, 64, 96                  4
1                            1, 33, 65, 97                  4
31                          31, 63, 95, 127                 4


answerï¼š
è¿™æ˜¯å› ä¸ºåœ¨ä¹‹å‰çš„ç¤ºä¾‹ä¸­ï¼Œæˆ‘é‡‡ç”¨äº†ç‰¹å®šçš„ **æ ·æœ¬æ•° N** å’Œ **çº¿ç¨‹å— Y ç»´åº¦å¤§å°** æ¥è¿›è¡Œç®€åŒ–è®¡ç®—ã€‚

è®©æˆ‘å†æ¬¡è§£é‡Šè¿™ä¸ªâ€œ4â€æ˜¯å¦‚ä½•å¾—å‡ºçš„ï¼Œå®ƒä»£è¡¨äº†æ¯ä¸ªçº¿ç¨‹éœ€è¦æ‰§è¡Œçš„ **å¾ªç¯è¿­ä»£æ¬¡æ•°**ã€‚

-----

## âš™ï¸ é—®é¢˜çš„æ ¸å¿ƒï¼šå¾ªç¯è¿­ä»£æ¬¡æ•°

åœ¨æ‚¨çš„ Batch Normalization Kernel ä¸­ï¼Œå±€éƒ¨æ±‚å’Œå¾ªç¯æ˜¯ï¼š

c
// å‡è®¾ blockDim.y = 32
for (int i = threadIdx.y; i < N; i += blockDim.y) {
    // ... ç´¯åŠ  sum å’Œ v ...



**å±€éƒ¨æ±‚å’Œé¡¹æ•°** å°±æ˜¯è¿™ä¸ª `for` å¾ªç¯çš„è¿­ä»£æ¬¡æ•°ã€‚

### ç¤ºä¾‹å‚æ•°å›é¡¾

æˆ‘ä¹‹å‰ä½¿ç”¨çš„ç¤ºä¾‹å‚æ•°æ˜¯ï¼š

  * **N (æ ·æœ¬æ€»æ•°)** = **128**
  * **blockDim.y (Y è½´çº¿ç¨‹æ•°)** = **32**

### è®¡ç®—è¿‡ç¨‹ï¼š

è¿­ä»£æ¬¡æ•°çš„å…¬å¼æ˜¯ï¼š

è¿­ä»£æ¬¡æ•° = N/blockDim.y

è¿­ä»£æ¬¡æ•° = 128/32 = 4

å› æ­¤ï¼Œå¯¹äºæ¯ä¸ªçº¿ç¨‹ t æ¥è¯´ï¼š

  * çº¿ç¨‹ t ä» i=t å¼€å§‹ã€‚
  * ä¸‹ä¸€æ¬¡è¿­ä»£æ˜¯ i = t + 32ã€‚
  * å†ä¸‹ä¸€æ¬¡æ˜¯ i = t + 64ã€‚
  * å†ä¸‹ä¸€æ¬¡æ˜¯ i = t + 96ã€‚

**ç¬¬ 5 æ¬¡è¿­ä»£** å°†æ˜¯ i = t + 128ã€‚ç”±äº t >= 0ï¼Œæ‰€ä»¥ i >= 128ï¼Œä¸æ»¡è¶³ i < N=128 çš„å¾ªç¯æ¡ä»¶ï¼Œå¾ªç¯åœæ­¢ã€‚

æ‰€ä»¥ï¼Œæ¯ä¸ªçº¿ç¨‹æ€»å…±æ‰§è¡Œäº† **4 æ¬¡** è¿­ä»£ï¼Œå¤„ç†äº† 4 ä¸ªæ ·æœ¬ã€‚

-----

## ğŸ¯ æ€»ç»“

è¿™é‡Œçš„ **4** æ˜¯ä¸€ä¸ª**ç»“æœ**ï¼Œæ˜¯æ ¹æ®æ‚¨è®¾å®šçš„ **Block çº¿ç¨‹æ•° 32** å’Œæˆ‘å‡è®¾çš„ **æ ·æœ¬æ€»æ•° 128** ä¸¥æ ¼è®¡ç®—å¾—å‡ºçš„ã€‚å®ƒç¡®ä¿äº† 32 ä¸ªçº¿ç¨‹æ­£å¥½èƒ½å¤Ÿå®Œç¾åœ°åˆ†æ‘Š 128 ä¸ªæ ·æœ¬çš„æ±‚å’Œå·¥ä½œã€‚

å¦‚æœ N æ˜¯ 256ï¼Œé‚£ä¹ˆè¿­ä»£æ¬¡æ•°å°†æ˜¯ 256 / 32 = 8 æ¬¡ã€‚
//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question4:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

ä¸ºå•¥è¿™é‡Œæ˜¯threadIdx.x, è€Œä¸æ˜¯local_col?

        float mean = sum[0][threadIdx.x] / static_cast<float>(N);
        float inv_std = fmaxf(var[0][threadIdx.x] / static_cast<float>(N) - mean * mean, 0.0f);

è¿™æ˜¯ä¸€ä¸ªå…³äº **CUDA å˜é‡ä½œç”¨åŸŸå’Œä¸€è‡´æ€§** çš„å¥½é—®é¢˜ã€‚åœ¨è¿™æ®µä»£ç ä¸­ï¼Œä½¿ç”¨ `threadIdx.x` è€Œä¸æ˜¯ `local_col` æ˜¯å› ä¸ºï¼š

1.  **å®ƒä»¬çš„å€¼æ˜¯ç›¸åŒçš„**ï¼š`local_col` æ˜¯é€šè¿‡ `local_col = threadIdx.x` å®šä¹‰çš„ã€‚
2.  **`threadIdx.x` å…·æœ‰å¸¸é‡ç‰¹æ€§**ï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç›´æ¥ä½¿ç”¨ `threadIdx.x`ï¼ˆCUDA çš„å†…ç½®å˜é‡ï¼‰å¯ä»¥è®©ç¼–è¯‘å™¨æ›´å¥½åœ°è¯†åˆ«å®ƒçš„ **å¸¸é‡æ€§ (constness)**ï¼Œå¯èƒ½æœ‰åŠ©äºä¼˜åŒ–ã€‚

ç„¶è€Œï¼Œåœ¨è¿™ä¸ªç‰¹å®šçš„ä½ç½®ï¼Œ**ä¸»è¦åŸå› åœ¨äºç®€åŒ–å’Œä»£ç é£æ ¼**ï¼Œä½†æˆ‘ä»¬å¿…é¡»ç¡®è®¤å®ƒä»¬æŒ‡ä»£çš„æ˜¯æ­£ç¡®çš„é€šé“ç´¢å¼•ã€‚

-----

## âš™ï¸ ä¸ºä»€ä¹ˆå¯ä»¥ä½¿ç”¨ threadIdx.x / local_col

åœ¨ Kernel çš„ç¬¬ä¸€é˜¶æ®µï¼Œé€šé“ç´¢å¼•æ˜¯è¿™æ ·å®šä¹‰çš„ï¼š

c
int local_row = threadIdx.y, local_col = threadIdx.x; // local_col = threadIdx.x
int channel = blockIdx.x * blockDim.x + local_col; // channel ä¾èµ–äº local_col


åœ¨æ‰§è¡Œå½’çº¦æ“ä½œåï¼Œ`sum[0][...]` æ•°ç»„çš„ **åˆ—ç´¢å¼•** ä»ç„¶å¯¹åº”äºè¯¥çº¿ç¨‹è´Ÿè´£çš„é€šé“ï¼š

1.  **å½’çº¦å‰:** sum[ty][tx] å­˜å‚¨çº¿ç¨‹ tx è´Ÿè´£çš„å±€éƒ¨å’Œã€‚
2.  **å½’çº¦å:** sum[0][tx] å­˜å‚¨çº¿ç¨‹ tx è´Ÿè´£é€šé“çš„ **æœ€ç»ˆæ€»å’Œ**ã€‚

å› æ­¤ï¼Œæ— è®ºä½ ä½¿ç”¨ `threadIdx.x` è¿˜æ˜¯ `local_col`ï¼Œå®ƒä»¬çš„å€¼éƒ½æ˜¯ç›¸åŒçš„ï¼Œéƒ½æ˜¯ 0 åˆ° 31ï¼Œå¹¶æ­£ç¡®åœ°æŒ‡å‘è¯¥çº¿ç¨‹ï¼ˆæˆ–è¯¥é€šé“ï¼‰åœ¨å…±äº«å†…å­˜ä¸­å½’çº¦ç»“æœçš„åˆ—ã€‚

### ç¤ºä¾‹

å‡è®¾çº¿ç¨‹ tx=5 è´Ÿè´£ **é€šé“ 37** (å¦‚æœ blockIdx.x=1)ã€‚

1.  **åŠ è½½é˜¶æ®µ:** çº¿ç¨‹ tx=5 å°†ç»“æœå†™å…¥ sum[\dots][5]ã€‚
2.  **å½’çº¦é˜¶æ®µ:** å½’çº¦å‘ç”Ÿåœ¨ Y ç»´åº¦ï¼Œæœ€ç»ˆç»“æœç´¯ç§¯åˆ° sum[0][5]ã€‚
3.  **è¯»å–é˜¶æ®µ:** çº¿ç¨‹ tx=5 å¿…é¡»è¯»å– sum[0][5]ã€‚

<!-- end list -->

  * ä½¿ç”¨ `threadIdx.x`: çº¿ç¨‹è¯»å– `sum[0][threadIdx.x]` --> çº¿ç¨‹ 5 è¯»å– sum[0][5]ã€‚ (**âœ… æ­£ç¡®**)
  * ä½¿ç”¨ `local_col`: çº¿ç¨‹è¯»å– `sum[0][local_col]` --> çº¿ç¨‹ 5 è¯»å– sum[0][5]ã€‚ (**âœ… æ­£ç¡®**)

**ç»“è®º:** åœ¨è¿™æ®µä»£ç ä¸­ï¼Œç›´æ¥ä½¿ç”¨ **`threadIdx.x`** æ˜¯å®Œå…¨æ­£ç¡®çš„ï¼Œå› ä¸ºå®ƒå¤©ç„¶åœ°ä»£è¡¨äº†çº¿ç¨‹åœ¨ X è½´ä¸Šçš„ä½ç½®ï¼Œè€Œ X è½´çš„ä½ç½®åˆç›´æ¥æ˜ å°„åˆ° Shared Memory æ•°ç»„çš„åˆ—ç´¢å¼•ã€‚ä½¿ç”¨ `local_col` åªæ˜¯å¤šäº†ä¸€ä¸ªå˜é‡åã€‚

//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question5:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

ä¸ºå•¥è¿™é‡Œæ˜¯threadIdx.x, è€Œä¸æ˜¯channel?

        float mean = sum[0][threadIdx.x] / static_cast<float>(N);
        float inv_std = fmaxf(var[0][threadIdx.x] / static_cast<float>(N) - mean * mean, 0.0f);


answer:
è¿™æ˜¯ä¸€ä¸ªå…³äº **æ•°æ®ä½œç”¨åŸŸå’Œå†…å­˜è®¿é—®** çš„å…³é”®ç‚¹ã€‚

åœ¨è¿™ä¸ª Kernel ä¸­ï¼Œ`sum` å’Œ `var` æ˜¯ **å…±äº«å†…å­˜ (Shared Memory)** æ•°ç»„ï¼Œå®ƒä»¬çš„ä½œç”¨åŸŸå’Œå¤§å°è¢«ä¸¥æ ¼é™åˆ¶åœ¨ **å½“å‰çº¿ç¨‹å—ï¼ˆBlockï¼‰** å†…éƒ¨ã€‚

-----

## âš™ï¸ I. æ ¸å¿ƒåŸå› ï¼šå…±äº«å†…å­˜çš„ä½œç”¨åŸŸ

### 1\. å…±äº«å†…å­˜æ˜¯å±€éƒ¨æ•°ç»„

`sum` å’Œ `var` æ•°ç»„çš„å®šä¹‰å¦‚ä¸‹ï¼š

c
__shared__ float sum[TILE][TILE]; // [32][32]
__shared__ float var[TILE][TILE]; // [32][32]


  * **å¤§å°:** å®ƒä»¬éƒ½æ˜¯ 32 * 32 çš„çŸ©é˜µã€‚
  * **ç´¢å¼•èŒƒå›´:** å®ƒä»¬çš„æœ‰æ•ˆç´¢å¼•èŒƒå›´æ˜¯ 0 åˆ° 31ã€‚

### 2\. çº¿ç¨‹èº«ä»½ä¸å…±äº«å†…å­˜ç´¢å¼•

æ¯ä¸ªçº¿ç¨‹å—åªè´Ÿè´£å¤„ç† **32 ä¸ªé€šé“**ã€‚åœ¨å…±äº«å†…å­˜ä¸­ï¼š

  * **è¡Œç´¢å¼• (Y):** ç”± `threadIdx.y` æˆ– `local_row` ç¡®å®šã€‚
  * **åˆ—ç´¢å¼• (X):** ç”± `threadIdx.x` æˆ– `local_col` ç¡®å®šã€‚

çº¿ç¨‹ tx è´Ÿè´£å°†å®ƒè®¡ç®—çš„ç»Ÿè®¡é‡å­˜å‚¨åœ¨ **sum[\dots][threadIdx.x]** è¿™ä¸€åˆ—ä¸­ã€‚

### 3\. channel æ˜¯å…¨å±€ç´¢å¼•

channel = blockIdx.x * blockDim.x + threadIdx.x

  * channel æ˜¯ä¸€ä¸ª **å…¨å±€ç´¢å¼•**ï¼Œå®ƒçš„å€¼èŒƒå›´å¯èƒ½æ˜¯ 0 åˆ°å‡ åƒï¼Œå–å†³äºæ€»é€šé“æ•° Cã€‚
  * **channel æ— æ³•ç›´æ¥ä½œä¸º sum æ•°ç»„çš„ç´¢å¼•ï¼Œå› ä¸ºå®ƒä¼šä¸¥é‡è¶Šç•Œ**ã€‚

-----

## ğŸ”¢ II. ç¤ºä¾‹å¯¹æ¯”

å‡è®¾ C=96 (é€šé“æ€»æ•°)ï¼ŒblockDim.x=32ã€‚Grid æœ‰ 3 ä¸ª Blockã€‚

| çº¿ç¨‹å— blockIdx.x | çº¿ç¨‹ threadIdx.x | **å…±äº«å†…å­˜ç´¢å¼•** (X è½´) | **å…¨å±€é€šé“ channel** (X è½´) |
| :--- | :--- | :--- | :--- |
| **0** | 10 | **10** | 0 * 32 + 10 = 10 |
| **1** | 10 | **10** | 1 * 32 + 10 = 42 |
| **2** | 10 | **10** | 2 * 32 + 10 = 74 |

### é”™è¯¯ç¤ºèŒƒï¼šä½¿ç”¨ channel

å¦‚æœä»£ç å†™æˆ `sum[0][channel]`ï¼š

  * Block 0 çš„çº¿ç¨‹ 10 ä¼šå°è¯•è¯»å– `sum[0][10]`ã€‚ (**æ­£ç¡®**)
  * Block 1 çš„çº¿ç¨‹ 10 ä¼šå°è¯•è¯»å– `sum[0][42]`ã€‚ (**é”™è¯¯ï¼**)
      * sum æ•°ç»„çš„æœ€å¤§ç´¢å¼•æ˜¯ 31ã€‚ç´¢å¼• 42 ä¼šå¯¼è‡´ **è¶Šç•Œè®¿é—® (Out-of-bounds access)**ï¼Œè¿™æ˜¯ä¸¥é‡é”™è¯¯ã€‚

### æ­£ç¡®ç¤ºèŒƒï¼šä½¿ç”¨ threadIdx.x

ä»£ç ä½¿ç”¨äº† `sum[0][threadIdx.x]`ï¼š

  * Block 0 çš„çº¿ç¨‹ 10 è¯»å– `sum[0][10]`ã€‚ (**æ­£ç¡®**)
  * Block 1 çš„çº¿ç¨‹ 10 è¯»å– `sum[0][10]`ã€‚ (**æ­£ç¡®**)
  * Block 2 çš„çº¿ç¨‹ 10 è¯»å– `sum[0][10]`ã€‚ (**æ­£ç¡®**)

**ç»“è®ºï¼š** æ¯ä¸ª Block ç‹¬ç«‹åœ°ä½¿ç”¨è‡ªå·±çš„ 32 ä¸ªçº¿ç¨‹æ¥å¤„ç† 32 ä¸ªé€šé“ã€‚å¯¹äºä»»ä½•ä¸€ä¸ª Block æ¥è¯´ï¼Œå®ƒæ‰€å¤„ç†çš„é€šé“çš„ç»Ÿè®¡ç»“æœéƒ½å­˜å‚¨åœ¨ sum æ•°ç»„çš„ 0 åˆ° 31 åˆ—ä¸­ï¼Œè€Œ threadIdx.x å°±æ˜¯ 0 åˆ° 31 çš„æ­£ç¡®ç´¢å¼•ã€‚



//--------------------------------------------------------------------------------------------------




//--------------------------------------------------------------------------------------------------
/*
question6:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œæœ‰ä¾‹å­

what is inv_std = rsqrtf(inv_std + eps)?

That line of code calculates the **inverse standard deviation** (inv_std) for a Batch Normalization operation.

Here's a breakdown of the process and why that specific formula is used:

-----

## ğŸ”¬ Calculation Breakdown

The line of code is the final step in determining the denominator of the Batch Normalization (BN) formula:

inv_std = 1/sqrt(sigma^2 + epsilon)

The input variable `inv_std` (before the calculation) actually holds the calculated **variance** (sigma^2), which was derived from the previous steps:

sigma^2 = sum x^2/N - mu^2

The code executes the following transformation:

### 1\. Adding Epsilon (sigma^2 + epsilon)

c
inv_std + eps


  * The small constant epsilon (epsilon) is added to the calculated variance (sigma^2).
  * **Purpose:** Epsilon is a **numerical stabilizer**. It prevents division by zero or division by an extremely small number if the variance (sigma^2) is zero or close to zero, which can happen with constant input data.

### 2\. Inverse Square Root (rsqrtf)

c
rsqrtf( ... )


  * The function rsqrtf calculates the **fast reciprocal square root** (1 / sqrt(x) in single precision.
  * **Purpose:** In the final BN formula, we need to divide by the standard deviation (sigma). Since division is computationally expensive on a GPU, it's faster to calculate the inverse standard deviation (1 / sigma) and then perform a multiplication.
    BN Step:  x - mu/sqrt(sigma^2 + epsilon)  becomes  (x - mu) * ( 1/sqrt(sigma^2 + epsilon ) )

### 3\. Result

The variable `inv_std` is updated from holding the variance (sigma^2) to holding the **inverse standard deviation** (1 / sqrt(sigma^2 + epsilon), making it ready for the final multiplication step of the Batch Normalization transformation.


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question7:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­




//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question8:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­



//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question10:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question11:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question12:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question13:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question14:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question15:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question16:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question17:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question18:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question19:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question20:


//--------------------------------------------------------------------------------------------------
