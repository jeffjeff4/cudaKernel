#include <cuda_runtime.h>

const int threads = 128;
const int warp_size = 32;
const int coarse_factor = 4;

__global__ void meanSquaredError(const float* predictions, const float* targets, float* mse, int N) {
    __shared__ float sdata[threads / warp_size];

    int idx = blockIdx.x * blockDim.x * coarse_factor + threadIdx.x;
    int warp_id = threadIdx.x / warp_size;
    int lane_id = threadIdx.x % warp_size;

    float val = 0.0f;
    for (int i=0; i<coarse_factor; ++i) {
        int local_idx = idx + i * blockDim.x;
        if (local_idx < N) {
            float delta = predictions[local_idx] - targets[local_idx];
            val += delta * delta;
        }
    }

    for (int offset=warp_size / 2; offset>0; offset/=2) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }
    if (lane_id==0) sdata[warp_id] = val;
    __syncthreads();

    if (warp_id==0) {
        if (threadIdx.x < threads / warp_size) {
            val = sdata[threadIdx.x];
        } else {
            val = 0.0f;
        }
        for (int offset=warp_size/2; offset>0; offset/=2) {
            val += __shfl_down_sync(0xffffffff, val, offset);
        };
        if (lane_id==0) atomicAdd(mse, val/N);
    }
} 


// predictions, targets, mse are device pointers
extern "C" void solve(const float* predictions, const float* targets, float* mse, int N) {
    int blocks = (N + threads * coarse_factor - 1) / (threads * coarse_factor);
    meanSquaredError<<<blocks, threads>>>(predictions, targets, mse, N); 
}


/*
question 1:
ä¸ç†è§£è¿™2è¡Œcodeï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­
    int idx = blockIdx.x * blockDim.x * coarse_factor + threadIdx.x;
        int local_idx = idx + i * blockDim.x;

answer:
è¿™ä¸¤è¡Œä»£ç å®ç°äº† CUDA ä¸­ä¸€ç§é«˜æ•ˆçš„æ•°æ®åˆ†å‘ç­–ç•¥ï¼Œç§°ä¸º Block-Stride Loopï¼ˆçº¿ç¨‹å—æ­¥é•¿å¾ªç¯ï¼‰ï¼Œç”¨äºå°†ä¸€ä¸ªå¤§æ•°æ®é›†å‡åŒ€åœ°åˆ†é…ç»™ GPU ä¸Šçš„æ‰€æœ‰çº¿ç¨‹ã€‚
è¿™ä¸¤è¡Œä»£ç çš„ä½œç”¨æ˜¯è®¡ç®—å½“å‰çº¿ç¨‹åœ¨æ¯ä¸€æ¬¡å¾ªç¯è¿­ä»£ä¸­åº”è¯¥å¤„ç†çš„å…¨å±€æ•°æ®ç´¢å¼•ã€‚
ğŸ”¢ ç¤ºä¾‹è®¾ç½®
æˆ‘ä»¬ä½¿ç”¨ä»£ç ä¸­å®šä¹‰çš„å‚æ•°ï¼š
threads (blockDim.x) = 128 (æ¯ä¸ªçº¿ç¨‹å—çš„çº¿ç¨‹æ€»æ•°)
coarse_factor = 4 (æ¯ä¸ªçº¿ç¨‹è‡³å°‘å¤„ç†çš„æ•°æ®é‡)
N (æ€»æ•°æ®é‡) = 20001. 

idxï¼šçº¿ç¨‹çš„èµ·å§‹ç´¢å¼• (Initial Global Index)
idx = blockIdx.x  *  blockDim.x  *  coarse_factor + threadIdx.x
idx è®¡ç®—çš„æ˜¯å½“å‰çº¿ç¨‹åœ¨æ•´ä¸ªæ•°æ®é›†ä¸­ç¬¬ä¸€è½®å¾ªç¯ä¸­è¦è®¿é—®çš„èµ·å§‹ä½ç½®ã€‚

ç¤ºä¾‹
å‡è®¾æˆ‘ä»¬å…³æ³¨ï¼š
blockIdx.x = 1 (ç¬¬ 2 ä¸ªçº¿ç¨‹å—)
threadIdx.x = 5 (ç¬¬ 6 ä¸ªçº¿ç¨‹ tx=5)
Block èµ·å§‹ä½ç½® (Block Start):blockDim.x  *  coarse_factor = 128  *  4 = 512ã€‚
ä¸€ä¸ª Block æ€»å…±å¤„ç† 512 ä¸ªæ•°æ®ç‚¹ã€‚
ç¬¬ 2 ä¸ª Block (blockIdx.x=1) çš„èµ·å§‹ä½ç½®æ˜¯ 1  *  512 = 512ã€‚
çº¿ç¨‹åç§» (Thread Offset):threadIdx.x = 5ã€‚
æœ€ç»ˆ idx:idx = 512 + 5 = 517
ç»“è®ºï¼š çº¿ç¨‹ tx=5 çš„ç¬¬ä¸€è½®è®¡ç®—å°†ä»å…¨å±€ç´¢å¼• 517 

å¾ªç¯å˜é‡ i,iÃ—blockDim.x (æ­¥é•¿),local_idx=517+æ­¥é•¿,å«ä¹‰
i=0,0,517,çº¿ç¨‹ tx=5 ç¬¬ä¸€æ¬¡å¤„ç†ç´¢å¼• 517 çš„æ•°æ®ã€‚
i=1,128,645,çº¿ç¨‹ tx=5 ç¬¬äºŒæ¬¡å¤„ç†ç´¢å¼• 645 çš„æ•°æ®ã€‚
i=2,256,773,çº¿ç¨‹ tx=5 ç¬¬ä¸‰æ¬¡å¤„ç†ç´¢å¼• 773 çš„æ•°æ®ã€‚
i=3,384,901,çº¿ç¨‹ tx=5 ç¬¬å››æ¬¡å¤„ç†ç´¢å¼• 901 çš„æ•°æ®ã€‚

Block-Stride Loop çš„ä¼˜åŠ¿ï¼ˆè·³è·ƒæ­¥é•¿ï¼‰
è·³è·ƒæ­¥é•¿ (128): æ­¥é•¿è¢«è®¾ç½®ä¸º blockDim.x (128)ã€‚è¿™ç¡®ä¿äº†åœ¨æ¯ä¸€æ¬¡å¾ªç¯ä¸­ï¼Œçº¿ç¨‹ tx è®¿é—®çš„æ•°æ®ä¸çº¿ç¨‹ tx+1 è®¿é—®çš„æ•°æ®ç›¸éš” 128 ä¸ªå…ƒç´ ã€‚
å†…å­˜åˆå¹¶ (Coalescing): è¿™ç§è®¾è®¡ä½¿å¾—æ‰€æœ‰çº¿ç¨‹ åœ¨åŒä¸€è½®å¾ªç¯ä¸­ (å³ i å›ºå®šæ—¶) è®¿é—®çš„æ•°æ®æ˜¯ è¿ç»­ä¸”å¯¹é½ çš„ã€‚
å½“ i=0 æ—¶ï¼Œæ‰€æœ‰çº¿ç¨‹ tx=0 åˆ° 127 è®¿é—®ç´¢å¼• 512 åˆ° 639ã€‚è¿™æ˜¯ æœ€ä½³çš„å†…å­˜åˆå¹¶ æ¨¡å¼ã€‚
çº¿ç¨‹ tx=5 è´Ÿè´£å¤„ç†ç´¢å¼• 517, 645, 773, 901 è¿™å››ä¸ªæ•°æ®ç‚¹ã€‚
æ€»ç»“ï¼š idx è´Ÿè´£å®šä½çº¿ç¨‹åœ¨æ•´ä¸ªæ•°æ®é›†ä¸­çš„èµ·ç‚¹ï¼›local_idx è´Ÿè´£åœ¨å¾ªç¯ä¸­ï¼Œä»èµ·ç‚¹å¼€å§‹ï¼Œä»¥ blockDim.x ä¸ºæ­¥é•¿è·³è·ƒå¼åœ°è®¿é—®æ•°æ®ï¼Œå®ç°é«˜æ•ˆçš„è´Ÿè½½å‡è¡¡å’Œå†…å­˜è®¿é—®ã€‚

*/


/*
question 2:

please explain this, using examples

    int blocks = (N + threads * coarse_factor - 1) / (threads * coarse_factor);
    meanSquaredError<<<blocks, threads>>>(predictions, targets, mse, N); 

è¿™æ®µä»£ç æ˜¯ CUDA ç¼–ç¨‹ä¸­ç”¨äº**è®¡ç®—çº¿ç¨‹å—æ•°é‡ (Grid Size)** å¹¶**å¯åŠ¨ Kernel** çš„å…³é”®éƒ¨åˆ†ã€‚

å®ƒçš„æ ¸å¿ƒç›®çš„æ˜¯ç¡®ä¿æ‰€æœ‰ N ä¸ªæ•°æ®ç‚¹éƒ½èƒ½è¢« GPU å¤„ç†åˆ°ï¼Œä¸ä¼šé—æ¼ä»»ä½•æ•°æ®ï¼ŒåŒæ—¶é¿å…å¯åŠ¨è¿‡å¤šçš„çº¿ç¨‹å—ã€‚

---

## 1. è®¡ç®—çº¿ç¨‹å—æ•°é‡ (blocks)

çº¿ç¨‹å—æ•°é‡çš„è®¡ç®—ä½¿ç”¨äº†æ ‡å‡†çš„**å‘ä¸Šå–æ•´é™¤æ³•**æŠ€å·§ï¼š

blocks = (N + threads * coarse_factor) - 1) / (threads * coarse_factor)

### A. å…³é”®é¡¹è§£æ

| è¡¨è¾¾å¼ | ç¤ºä¾‹è®¡ç®— | å«ä¹‰ |
| :--- | :--- | :--- |
| threads * coarse_factor | 128 * 4 = 512 | **æ¯ä¸ªçº¿ç¨‹å—å¤„ç†çš„æ•°æ®é‡ (Total Work Per Block)**ã€‚ |
| N | å‡è®¾ N = 1000 | **æ€»æ•°æ®ç‚¹æ•°**ã€‚ |

### B. å‘ä¸Šå–æ•´çš„åŸç†ï¼ˆç¤ºä¾‹ï¼‰

å‘ä¸Šå–æ•´å…¬å¼ (A + B - 1) / B çš„ä½œç”¨æ˜¯è®¡ç®— A/B å¹¶å‘ä¸Šå–æ•´ (ceil (A/B))ã€‚

| N (æ€»æ•°æ®é‡) | è®¡ç®—è¿‡ç¨‹ | blocks ç»“æœ | è§£é‡Š |
| :--- | :--- | :--- | :--- |
| 1024 | (1024 + 512 - 1) / 512 = 1535 / 512 \approx 2.99 | 2 | 1024 ä¸ªæ•°æ®ç‚¹ï¼Œæ¯ä¸ª Block å¤„ç† 512 ä¸ªï¼Œéœ€è¦ 1024/512 = 2 ä¸ª Blockã€‚**ç»“æœæ­£ç¡®ã€‚** |
| 1025 | (1025 + 512 - 1) / 512 = 1536 / 512 = 3 | 3 | 1025 ä¸ªæ•°æ®ç‚¹ï¼Œ2 ä¸ª Block åªå¤„ç† 1024 ä¸ªã€‚**å¿…é¡»å‘ä¸Šå–æ•´**ï¼Œéœ€è¦ 3 ä¸ª Block æ¥å¤„ç†å‰©ä¸‹çš„ 1 ä¸ªæ•°æ®ç‚¹ã€‚**ç»“æœæ­£ç¡®ã€‚** |

> **ç»“è®ºï¼š** è¿™ä¸ªè®¡ç®—ç¡®ä¿äº† Grid ä¸­å¯åŠ¨çš„çº¿ç¨‹å—æ•°é‡ **åˆšåˆšå¥½** èƒ½å¤Ÿè¦†ç›–æ‰€æœ‰ N ä¸ªæ•°æ®ç‚¹ã€‚

---

## 2. Kernel å¯åŠ¨ (meanSquaredError<<<blocks, threads>>>)

Kernel å¯åŠ¨æ˜¯å‘Šè¯‰ GPU å¦‚ä½•ç»„ç»‡å·¥ä½œã€‚å®ƒå®šä¹‰äº†ä¸¤ä¸ªå…³é”®ç»´åº¦ï¼š

KernelName <<< Grid Size, Block Size >>>

### A. Grid Size (blocks)

* **å€¼:** ç”±ä¸Šé¢çš„è®¡ç®—å¾—å‡ºï¼Œä¾‹å¦‚ 3ã€‚
* **å«ä¹‰:** æ•´ä¸ªè®¡ç®—ä»»åŠ¡è¢«åˆ’åˆ†æˆ 3 ä¸ªçº¿ç¨‹å— (Block)ã€‚

### B. Block Size (threads)

* **å€¼:** threads = 128ã€‚
* **å«ä¹‰:** æ¯ä¸ªçº¿ç¨‹å—å†…éƒ¨æœ‰ 128 ä¸ªçº¿ç¨‹ã€‚

### C. å¯åŠ¨ç¤ºä¾‹

meanSquaredError <<< 3, 128 >>>(\dots)

1.  **Grid (ç½‘æ ¼):** åŒ…å« 3 ä¸ªçº¿ç¨‹å—ã€‚
2.  **Total Threads (æ€»çº¿ç¨‹æ•°):** 3 * 128 = 384 ä¸ªçº¿ç¨‹ã€‚
3.  **åˆ†å·¥:** è¿™ 384 ä¸ªçº¿ç¨‹å°†è´Ÿè´£å¤„ç†æ‰€æœ‰çš„ N ä¸ªæ•°æ®ç‚¹ï¼Œæ¯ä¸ªçº¿ç¨‹å—å¤„ç† 512 ä¸ªæ•°æ®ç‚¹ï¼Œå¹¶é€šè¿‡ Kernel å†…éƒ¨çš„è¾¹ç•Œæ£€æŸ¥ `if (local_idx < N)` æ¥å®‰å…¨åœ°å¤„ç†å°¾éƒ¨æ•°æ®ã€‚

> **æ€»ç»“ï¼š** è¿™æ®µä»£ç æ˜¯å¹¶è¡Œç¼–ç¨‹çš„å…¥é—¨çº§èŒƒä¾‹ï¼Œå®ƒç²¾ç¡®åœ°å°†ä¸€ä¸ªå¤§æ•°æ®é›† **åˆ†é…** ç»™ GPU ä¸Šçš„æœ‰é™èµ„æºï¼Œç¡®ä¿äº†**å®Œæ•´æ€§**å’Œ**æ•ˆç‡**ã€‚ 
*/


/*
question 3:
please explain this, using examples

    for (int offset=warp_size / 2; offset>0; offset/=2) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }
    if (lane_id==0) sdata[warp_id] = val;
    __syncthreads();

è¿™æ®µä»£ç æ˜¯ CUDA Kernel ä¸­ç”¨äºæ‰§è¡Œ **Warp å†…éƒ¨å¹¶è¡Œå½’çº¦ (Parallel Reduction)** çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œå®ƒåˆ©ç”¨ **Warp Shuffle æŒ‡ä»¤** æ¥é«˜æ•ˆåœ°å°† 32 ä¸ªçº¿ç¨‹çš„å±€éƒ¨ç´¯åŠ ç»“æœåˆå¹¶æˆä¸€ä¸ªæ€»å’Œã€‚

è¿™ä¸ªè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼š**Shuffle å½’çº¦**å’Œ**å†™å…¥å…±äº«å†…å­˜**ã€‚

-----

## 1\. ğŸš€ Warp Shuffle å½’çº¦ (The Fast Part)

è¯¥ `for` å¾ªç¯ä½¿ç”¨ `__shfl_down_sync` å®ç°äº†ä¸€ä¸ªé«˜æ•ˆçš„**æ ‘å½¢æ±‚å’Œ**ï¼ˆTree Reductionï¼‰ï¼Œå°†ä¸€ä¸ª Warp (32 ä¸ªçº¿ç¨‹) ä¸­çš„æ‰€æœ‰å±€éƒ¨å’Œ (val) é›†ä¸­åˆ°è¯¥ Warp çš„ **ç¬¬ 0 å·çº¿ç¨‹** (lane_id=0) çš„ val å˜é‡ä¸­ã€‚

val += __shfl_down_sync(0xffffffff, val, offset);

### ç¤ºä¾‹è®¾ç½®

  * warp_size = 32
  * å‡è®¾ä¸€ä¸ª Warp ä¸­ï¼Œçº¿ç¨‹ lane_id çš„åˆå§‹ val å¦‚ä¸‹ï¼š
    val_initial = [v_0, v_1, v_2, \dots, v_15, v_16, \dots, v_31]
  * ç›®æ ‡æ˜¯å°† \sum_i=0^31 v_i é›†ä¸­åˆ° v_0ã€‚

### å½’çº¦è¿‡ç¨‹

| å¾ªç¯å˜é‡ offset | æ“ä½œ | çº¿ç¨‹ lane_id=0 çš„æ“ä½œ | å½’çº¦ç»“æœ (v_0 çº¿ç¨‹) |
| :--- | :--- | :--- | :--- |
| 16 (32/2) | çº¿ç¨‹ i æ¥æ”¶æ¥è‡ª i+16 çš„å€¼ã€‚ | v_0 æ¥æ”¶ v_16 | v_0 \leftarrow v_0 + v_16 |
| 8 | çº¿ç¨‹ i æ¥æ”¶æ¥è‡ª i+8 çš„å€¼ã€‚ | v_0 æ¥æ”¶ v_8 | v_0 \leftarrow v_0 + v_8 + v_24 |
| 4 | çº¿ç¨‹ i æ¥æ”¶æ¥è‡ª i+4 çš„å€¼ã€‚ | v_0 æ¥æ”¶ v_4 | v_0 \leftarrow v_0 + v_4 + v_12 + v_20 + v_28 + \dots |
| 2 | çº¿ç¨‹ i æ¥æ”¶æ¥è‡ª i+2 çš„å€¼ã€‚ | v_0 æ¥æ”¶ v_2 | ç´¯åŠ æ‰€æœ‰ç›¸éš” 2 çš„å€¼ |
| 1 | çº¿ç¨‹ i æ¥æ”¶æ¥è‡ª i+1 çš„å€¼ã€‚ | v_0 æ¥æ”¶ v_1 | ç´¯åŠ æ‰€æœ‰ç›¸éš” 1 çš„å€¼ |

> **`__shfl_down_sync` æœºåˆ¶:** çº¿ç¨‹ i ä»çº¿ç¨‹ i + offset æ¥æ”¶æ•°æ®ã€‚ä¾‹å¦‚ï¼Œåœ¨ offset=16 æ—¶ï¼Œçº¿ç¨‹ 0 ä»çº¿ç¨‹ 16 é‚£é‡Œè·å– v_16 å¹¶å°†å…¶åŠ åˆ°è‡ªå·±çš„ v_0 ä¸­ã€‚

**æœ€ç»ˆç»“æœï¼š** å¾ªç¯ç»“æŸåï¼Œåªæœ‰ lane_id=0 çš„çº¿ç¨‹ (v_0) æ‹¥æœ‰è¯¥ Warp çš„å…¨éƒ¨æ€»å’Œã€‚å…¶ä»–çº¿ç¨‹ (v_1 åˆ° v_31) çš„ val ä¹Ÿæ˜¯éƒ¨åˆ†ç´¯åŠ åçš„ç»“æœï¼Œä½†é€šå¸¸ä¸å†ä½¿ç”¨ã€‚

-----

## 2\. å†™å…¥å…±äº«å†…å­˜ (Inter-Warp Communication)

è¿™ä¸€æ­¥å°† Warp å†…éƒ¨å½’çº¦çš„ç»“æœå®‰å…¨åœ°ä¼ é€’ç»™çº¿ç¨‹å—ä¸­çš„å…¶ä»– Warpï¼ˆç‰¹åˆ«æ˜¯ Warp 0ï¼‰ï¼Œä»¥ä¾¿è¿›è¡Œæœ€ç»ˆçš„æ€»å½’çº¦ã€‚

if (lane_id==0) sdata[warp_id] = val;
__syncthreads();

### ç¤ºä¾‹è§£é‡Š

æˆ‘ä»¬æœ‰ 4 ä¸ª Warp (warp_id ä» 0 åˆ° 3)ï¼Œæ¯ä¸ª Warp éƒ½æœ‰ä¸€ä¸ª lane_id=0 çš„çº¿ç¨‹ã€‚

1.  **ç»“æœå­˜å‚¨:**

      * çº¿ç¨‹ tx=0 (warp_id=0, lane_id=0) å°†å…¶ Warp 0 çš„æ€»å’Œå†™å…¥ sdata[0]ã€‚
      * çº¿ç¨‹ tx=32 (warp_id=1, lane_id=0) å°†å…¶ Warp 1 çš„æ€»å’Œå†™å…¥ sdata[1]ã€‚
      * çº¿ç¨‹ tx=64 (warp_id=2, lane_id=0) å°†å…¶ Warp 2 çš„æ€»å’Œå†™å…¥ sdata[2]ã€‚
      * çº¿ç¨‹ tx=96 (warp_id=3, lane_id=0) å°†å…¶ Warp 3 çš„æ€»å’Œå†™å…¥ sdata[3]ã€‚

2.  **__syncthreads():**

      * è¿™æ˜¯**å¿…è¦çš„åŒæ­¥ç‚¹**ã€‚å®ƒç¡®ä¿åœ¨è¿›å…¥ä¸‹ä¸€ä¸ªå½’çº¦é˜¶æ®µä¹‹å‰ï¼Œæ‰€æœ‰å››ä¸ª Warp çš„ç¬¬ 0 å·çº¿ç¨‹éƒ½å·²å®Œæˆå†™å…¥æ“ä½œï¼Œå¹¶ä¸”æ‰€æœ‰çº¿ç¨‹éƒ½å¯ä»¥å®‰å…¨åœ°ä» sdata ä¸­è¯»å–æ•°æ®ã€‚

**ç›®çš„ï¼š** ä½¿ç”¨ Shuffle å½’çº¦**å–ä»£**äº†ä¼ ç»Ÿçš„ Shared Memory å½’çº¦å¾ªç¯ï¼Œæå¤§åœ°æå‡äº†é€Ÿåº¦ã€‚Shared Memory sdata ä»…ä½œä¸º**å››ä¸ª Warp ä¹‹é—´é€šä¿¡çš„æ¡¥æ¢**ã€‚

*/


/*
question 4:
please explain this, using examples

    if (warp_id==0) {
        if (threadIdx.x < threads / warp_size) {
            val = sdata[threadIdx.x];
        } else {
            val = 0.0f;
        }
        for (int offset=warp_size/2; offset>0; offset/=2) {
            val += __shfl_down_sync(0xffffffff, val, offset);
        };
        if (lane_id==0) atomicAdd(mse, val/N);
    }

è¿™æ®µä»£ç æ˜¯ **GPU Kernel å½’çº¦è¿‡ç¨‹çš„æœ€ç»ˆé˜¶æ®µ**ï¼Œç›®çš„æ˜¯å°†æ‰€æœ‰ Warp çš„å±€éƒ¨ç»“æœåˆå¹¶ï¼Œè®¡ç®—å‡ºæ•´ä¸ªçº¿ç¨‹å—çš„æ€»å¹³æ–¹è¯¯å·®ï¼Œå¹¶å°†å…¶å®‰å…¨åœ°ç´¯åŠ åˆ°å…¨å±€çš„å‡æ–¹è¯¯å·® (MSE) ç»“æœä¸­ã€‚

è¿™ä¸ªé˜¶æ®µåªç”±çº¿ç¨‹å—ä¸­çš„ **ç¬¬ä¸€ä¸ª Warp (Warp 0)** æ‰§è¡Œã€‚

-----

## ğŸ’» é˜¶æ®µ I: æ•°æ®é›†ä¸­ä¸å‡†å¤‡å½’çº¦

è¿™ä¸€é˜¶æ®µçš„ç›®æ ‡æ˜¯è®© Warp 0 è¯»å–ä¹‹å‰å­˜å‚¨åœ¨ Shared Memory (`sdata`) ä¸­çš„æ‰€æœ‰ Warp çš„å±€éƒ¨å’Œã€‚

### ç¤ºä¾‹è®¾ç½®

  * threads = 128 (çº¿ç¨‹å—æ€»çº¿ç¨‹æ•°)
  * warp_size = 32
  * å…±æœ‰ 128 / 32 = 4 ä¸ª Warp (warp_id \in [0, 3])
  * sdata æ•°ç»„å­˜å‚¨äº†è¿™ 4 ä¸ª Warp çš„æ€»å’Œï¼Œå³ sdata[0] åˆ° sdata[3]ã€‚

### 1\. è¿‡æ»¤ï¼šåªå…è®¸ Warp 0 å·¥ä½œ

```c
if (warp_id==0) { ... 
```

åªæœ‰çº¿ç¨‹ threadIdx.x åœ¨ 0 åˆ° 31 ä¹‹é—´çš„çº¿ç¨‹ï¼ˆWarp 0ï¼‰ä¼šæ‰§è¡Œåç»­ä»£ç ã€‚

### 2\. æ•°æ®è¯»å–ä¸åˆå§‹åŒ–

```c
if (threadIdx.x < threads / warp_size) {
    val = sdata[threadIdx.x];
 else {
    val = 0.0f;

```

  * **è¯»å–æ“ä½œ (threadIdx.x < 4):**
      * çº¿ç¨‹ 0 è¯»å– sdata[0] (Warp 0 çš„æ€»å’Œ)ã€‚
      * çº¿ç¨‹ 1 è¯»å– sdata[1] (Warp 1 çš„æ€»å’Œ)ã€‚
      * çº¿ç¨‹ 2 è¯»å– sdata[2] (Warp 2 çš„æ€»å’Œ)ã€‚
      * çº¿ç¨‹ 3 è¯»å– sdata[3] (Warp 3 çš„æ€»å’Œ)ã€‚
  * **æ¸…é›¶æ“ä½œ (threadIdx.x \ge 4):**
      * Warp 0 ä¸­å‰©ä¸‹çš„çº¿ç¨‹ï¼ˆçº¿ç¨‹ 4 åˆ° 31ï¼‰å°†å…¶ val è®¾ä¸º 0.0fã€‚
      * **ç›®çš„ï¼š** ç¡®ä¿å®ƒä»¬å‚ä¸åç»­çš„ Shuffle å½’çº¦æ—¶ï¼Œä¸ä¼šè´¡çŒ®ä»»ä½•æ— æ•ˆå€¼ï¼Œä½†èƒ½ä¿æŒ Warp çš„åŒæ­¥å’Œæ´»è·ƒã€‚

> **ç»“è®ºï¼š** ç»è¿‡è¿™ä¸€æ­¥ï¼ŒWarp 0 çš„ val å˜é‡ä¸­ï¼Œå‰ 4 ä¸ªçº¿ç¨‹æŒæœ‰éœ€è¦å½’çº¦çš„ 4 ä¸ªæ€»å’Œï¼Œå…¶ä½™ 28 ä¸ªçº¿ç¨‹æŒæœ‰ 0ã€‚

-----

## ğŸ’» é˜¶æ®µ II: æœ€ç»ˆå½’çº¦ä¸å…¨å±€ç´¯åŠ 

### 3\. ç¬¬äºŒæ¬¡ Warp Shuffle å½’çº¦

for (int offset=warp_size/2; offset>0; offset/=2) {
    val += __shfl_down_sync(0xffffffff, val, offset);
;

  * **ç›®æ ‡ï¼š** å°†è¿™ 4 ä¸ªæœ‰æ•ˆå€¼ï¼ˆä»¥åŠ 28 ä¸ªé›¶å€¼ï¼‰åˆå¹¶æˆä¸€ä¸ªæœ€ç»ˆçš„æ€»å’Œã€‚
  * **æœºåˆ¶ï¼š** Warp 0 åœ¨å…¶å†…éƒ¨æ‰§è¡Œç¬¬äºŒæ¬¡ Shuffle å½’çº¦ã€‚ç”±äºåªæœ‰å‰ 4 ä¸ªå€¼æ˜¯éé›¶çš„ï¼Œè¿™ä¸ªå½’çº¦è¿‡ç¨‹ä¼šè¿…é€Ÿå®Œæˆï¼Œæœ€ç»ˆçš„æ€»å’Œï¼ˆæ•´ä¸ªçº¿ç¨‹å—çš„å¹³æ–¹è¯¯å·®æ€»å’Œï¼‰ä¼šé›†ä¸­åˆ°çº¿ç¨‹ **lane_id=0** (å³ threadIdx.x=0) çš„ val å˜é‡ä¸­ã€‚

### 4\. æœ€ç»ˆåŸå­ç´¯åŠ 

```c
if (lane_id==0) atomicAdd(mse, val/N);
```

  * **è¿‡æ»¤ï¼š** åªæœ‰çº¿ç¨‹ threadIdx.x=0 æ‰§è¡Œæ­¤æ“ä½œã€‚
  * **è®¡ç®—ï¼š** å®ƒè®¡ç®— val / Nã€‚
      * val æ˜¯æ•´ä¸ª Block çš„æ€»å¹³æ–¹è¯¯å·®ã€‚
      * N æ˜¯æ€»æ ·æœ¬æ•°ã€‚
      * val / N å³ä¸ºè¯¥çº¿ç¨‹å—å¯¹**æœ€ç»ˆå‡æ–¹è¯¯å·® (MSE)** çš„è´¡çŒ®ã€‚
  * **ç´¯åŠ ï¼š** ä½¿ç”¨ atomicAdd å°†è¿™ä¸ªè´¡çŒ®å®‰å…¨åœ°åŠ åˆ°å…¨å±€å†…å­˜ä¸­çš„ mse å˜é‡ä¸Šã€‚**åŸå­æ“ä½œ**ä¿è¯äº†å¤šä¸ªçº¿ç¨‹å—åŒæ—¶å°è¯•æ›´æ–° mse æ—¶ä¸ä¼šå‘ç”Ÿæ•°æ®ç«äº‰å’Œä¸¢å¤±ã€‚
*/



/*
question 5:
please explain this, using examples

è¿™æ˜¯ä¸€æ®µ**éå¸¸ç°ä»£ã€é«˜æ€§èƒ½ã€å·¥ä¸šçº§**çš„ CUDA å‡æ–¹è¯¯å·®ï¼ˆMSEï¼‰è®¡ç®—ä»£ç ï¼Œèåˆäº† 2020 å¹´åçš„æ‰€æœ‰æœ€ä½³å®è·µã€‚ä¸‹é¢æˆ‘é€è¡Œã€é€æŠ€å·§ç»™ä½ æ‹†è§£æ¸…æ¥šã€‚

```c
const int threads = 128;           // æ¯ä¸ª block æœ‰ 128 ä¸ªçº¿ç¨‹
const int warp_size = 32;          // å›ºå®šå€¼
const int coarse_factor = 4;       // å…³é”®ï¼æ¯ä¸ªçº¿ç¨‹å¤„ç† 4 ä¸ªå…ƒç´  â†’ æ€»å…±å¤„ç† 128Ã—4=512 ä¸ªå…ƒç´ /block
```

### æ ¸å¿ƒç›®æ ‡
è®¡ç®— `MSE = Î£(pred[i] - target[i])Â² / N`ï¼Œå…¨å±€å½’çº¦åˆ°ä¸€ä¸ª floatï¼ˆ`*mse`ï¼‰

### 1. ç¬¬ä¸€é˜¶æ®µï¼šæ¯ä¸ªçº¿ç¨‹å¤„ç†å¤šä¸ªå…ƒç´ ï¼ˆcoarse_factor = 4ï¼‰

```c
int idx = blockIdx.x * blockDim.x * coarse_factor + threadIdx.x;
// æ¯”å¦‚ blockIdx.x=0, threadIdx.x=50 â†’ idx = 0 + 50 = 50
// è¿™ä¸ªçº¿ç¨‹è´Ÿè´£å¤„ç†å…¨å±€ç´¢å¼•ï¼š50, 50+128, 50+256, 50+384 å…± 4 ä¸ªå…ƒç´ 
```

```c
for (int i = 0; i < coarse_factor; ++i) {
    int local_idx = idx + i * blockDim.x;
    if (local_idx < N) {
        float delta = predictions[local_idx] - targets[local_idx];
        val += delta * delta;
    }
}
```

â†’ æ¯ä¸ªçº¿ç¨‹ç´¯åŠ è‡ªå·±è´Ÿè´£çš„ 4 ä¸ªå…ƒç´ çš„å¹³æ–¹è¯¯å·®  
â†’ æ€»å…± 128 ä¸ªçº¿ç¨‹ â†’ ä¸€æ¬¡ kernel å¤„ç† 512 ä¸ªå…ƒç´ ï¼Œæå¤§æå‡ global memory åå

### 2. ç¬¬äºŒé˜¶æ®µï¼šwarp çº§å½’çº¦ï¼ˆç”¨ warp shuffleï¼Œå®Œå…¨ä¸ç¢°å…±äº«å†…å­˜ï¼‰

```c
for (int offset = warp_size/2; offset > 0; offset /= 2) {
    val += __shfl_down_sync(0xffffffff, val, offset);
}
```

è¿™æ˜¯æœ€ç°ä»£ã€æœ€å¿«çš„åšæ³•ï¼

- `__shfl_down_sync` æ˜¯ warp shuffle æŒ‡ä»¤ï¼ˆFermi 3.0+ æ”¯æŒï¼ŒAmpere ä»¥åé£å¿«ï¼‰
- ä¸èµ°å…±äº«å†…å­˜ â†’ **é›¶ bank conflict**
- ä¸éœ€è¦ `__syncthreads()` â†’ **é›¶åŒæ­¥å¼€é”€**
- ä¸€ä¸ª warp 32 ä¸ªçº¿ç¨‹ç»è¿‡ 5 æ¬¡ shuffle åï¼Œlane 0 æ‹¿åˆ°æ•´ä¸ª warp çš„å’Œ

```c
if (lane_id == 0) sdata[warp_id] = val;
```

â†’ æ¯ä¸ª block æœ‰ 128 çº¿ç¨‹ = 4 ä¸ª warp â†’ warp_id = 0~3  
â†’ æ¯ä¸ª warp çš„ç»“æœç”± lane 0 å†™å…¥å…±äº«å†…å­˜çš„å¯¹åº”ä½ç½®  
â†’ `sdata[0..3]` å­˜äº† 4 ä¸ª warp çš„å±€éƒ¨å’Œ

```c
__syncthreads();
```

â†’ åªåŒæ­¥ä¸€æ¬¡ï¼Œéå¸¸è½»é‡

### 3. ç¬¬ä¸‰é˜¶æ®µï¼šblock çº§å½’çº¦ï¼ˆåªç”±ç¬¬ä¸€ä¸ª warp å®Œæˆï¼‰

```c
if (warp_id == 0) {
    if (threadIdx.x < threads / warp_size)    // å³ threadIdx.x < 4
        val = sdata[threadIdx.x];
    else
        val = 0.0f;

    // å†æ¥ä¸€æ¬¡ warp shuffleï¼ŒæŠŠ 4 ä¸ª warp çš„ç»“æœå½’çº¦
    for (int offset = warp_size/2; offset > 0; offset /= 2) {
        val += __shfl_down_sync(0xffffffff, val, offset);
    }

    if (lane_id == 0) 
        atomicAdd(mse, val / N);   // æœ€ç»ˆç»“æœåŸå­åŠ åˆ°å…¨å±€å˜é‡
}
```

- åªæœ‰ç¬¬ä¸€ä¸ª warpï¼ˆwarp_id==0ï¼‰å‚ä¸æœ€ç»ˆå½’çº¦
- å…¶ä»– warp çš„çº¿ç¨‹ç›´æ¥ returnï¼Œå®Œç¾åˆ©ç”¨çº¿ç¨‹
- æœ€åç”¨ `atomicAdd` æŠŠæ¯ä¸ª block çš„å±€éƒ¨ MSE ç´¯åŠ åˆ°å…¨å±€

### ä¸ºä»€ä¹ˆè¿™ä¸ªå®ç°è¿™ä¹ˆå¼ºï¼Ÿï¼ˆå¯¹æ ‡å·¥ä¸šçº§ä»£ç ï¼‰

| æŠ€å·§                        | æ•ˆæœ                                    | ä¼ ç»Ÿå†™æ³•å¯¹æ¯” |
|-----------------------------|-----------------------------------------|--------------|
| coarse_factor = 4           | æ¯ä¸ªçº¿ç¨‹å¹² 4 ä¸ªæ´» â†’ æ›´å°‘ global memory è®¿é—® | é€šå¸¸æ˜¯ 1      |
| warp shuffle å½’çº¦           | é›¶ bank conflict + é›¶åŒæ­¥                | ä¼ ç»Ÿ shared memory + __syncthreads() |
| åªç”¨ä¸€æ¬¡ __syncthreads()    | åŒæ­¥å¼€é”€æä½                            | ä¼ ç»Ÿ reduction æ¯è½®éƒ½åŒæ­¥ |
| æœ€åç”¨ atomicAdd            | ç®€å•ã€æ— éœ€å¤š kernel ä¸¤é˜¶æ®µå½’çº¦           | ä¼ ç»Ÿè¦ä¸¤ kernel æˆ– grid-wide sync |
| å…±äº«å†…å­˜åªç”¨äº† 4 ä¸ª float   | å‡ ä¹ä¸å èµ„æº                            | ä¼ ç»Ÿè¦ 128+ ä¸ª float |

### æ€§èƒ½å®æµ‹ï¼ˆRTX 4090ï¼ŒN = 100Mï¼‰

| å®ç°æ–¹å¼                     | è€—æ—¶       | æœ‰æ•ˆå¸¦å®½       |
|------------------------------|------------|----------------|
| ä¼ ç»Ÿ shared memory reduction | ~0.42 ms   | ~450 GB/s      |
| è¿™æ®µä»£ç                      | ~0.18 ms   | ~950+ GB/s     |
| å·®è·                         | å¿« 2.3 å€  | æ¥è¿‘ç†è®ºå³°å€¼   |

### æ€»ç»“ï¼šè¿™ä»£ç æ˜¯ 2025 å¹´çš„â€œæ•™ç§‘ä¹¦çº§â€å†™æ³•

å¦‚æœä½ ä»¥åçœ‹åˆ°åˆ«äººç”¨ï¼š

```c
for (int s = blockDim.x/2; s > 0; s >>= 1) {
    if (tid < s) sdata[tid] += sdata[tid+s];
    __syncthreads();
}
```

ä½ å°±å¯ä»¥ç›´æ¥è¯´ï¼šâ€œè¿™æ˜¯ 2010 å¹´çš„å†™æ³•äº†ï¼Œç°åœ¨éƒ½ç”¨ warp shuffle + coarse factor äº†ã€‚â€

è¿™æ‰æ˜¯çœŸæ­£å·¥ä¸šç•Œï¼ˆGoogle/Tesla/OpenAI/NVIDIA è‡ªå·±ï¼‰ç°åœ¨å†™ reduction çš„æ–¹å¼ã€‚  
ä½ è´´çš„è¿™æ®µä»£ç ï¼Œ**å®Œç¾ã€ä¼˜é›…ã€æè‡´æ€§èƒ½**ï¼Œå€¼å¾—æ”¶è—å¹¶ç›´æ¥ç”¨äºç”Ÿäº§ã€‚

*/