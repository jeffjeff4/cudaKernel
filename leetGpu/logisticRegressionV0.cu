#include <cuda_runtime.h>
#include <math.h>
#include <stdlib.h>

// Small L2 regularization to make the solution well-defined and close
// to the reference "expected" coefficients on separable data.
static const double REG_LAMBDA = 1e-6;

// Numerically stable sigmoid in double precision
static inline double sigmoid_double(double z) {
    if (z >= 0.0) {
        double e = exp(-z);
        return 1.0 / (1.0 + e);
    } else {
        double e = exp(z);
        return e / (1.0 + e);
    }
}

/**
 * Solve A x = b for x using Gaussian elimination with partial pivoting.
 * A is modified in-place to its upper-triangular form; b is also modified.
 * d = dimension.
 */
static void solve_linear_system(double* A, double* b, double* x, int d) {
    // Forward elimination
    for (int i = 0; i < d; ++i) {
        // Pivot: find row k >= i with max |A[k, i]|
        int pivot_row = i;
        double max_val = fabs(A[i * d + i]);
        for (int k = i + 1; k < d; ++k) {
            double val = fabs(A[k * d + i]);
            if (val > max_val) {
                max_val = val;
                pivot_row = k;
            }
        }

        // Swap rows i and pivot_row in A and b if needed
        if (pivot_row != i) {
            for (int j = 0; j < d; ++j) {
                double tmp = A[i * d + j];
                A[i * d + j] = A[pivot_row * d + j];
                A[pivot_row * d + j] = tmp;
            }
            double tmpb = b[i];
            b[i] = b[pivot_row];
            b[pivot_row] = tmpb;
        }

        // If pivot is extremely small, add tiny jitter to keep system solvable
        double pivot = A[i * d + i];
        if (fabs(pivot) < 1e-12) {
            pivot = (pivot >= 0.0 ? 1e-12 : -1e-12);
            A[i * d + i] = pivot;
        }

        // Eliminate below pivot
        for (int k = i + 1; k < d; ++k) {
            double factor = A[k * d + i] / pivot;
            if (factor == 0.0) continue;

            // Row operation on A
            for (int j = i; j < d; ++j) {
                A[k * d + j] -= factor * A[i * d + j];
            }
            // Row operation on b
            b[k] -= factor * b[i];
        }
    }

    // Back substitution
    for (int i = d - 1; i >= 0; --i) {
        double sum = b[i];
        for (int j = i + 1; j < d; ++j) {
            sum -= A[i * d + j] * x[j];
        }
        double diag = A[i * d + i];
        if (fabs(diag) < 1e-12) {
            diag = (diag >= 0.0 ? 1e-12 : -1e-12);
        }
        x[i] = sum / diag;
    }
}

/**
 * Host-side Newton / IRLS solver for L2-regularized logistic regression.
 * X_dev, y_dev, beta_dev are device pointers. This function copies data
 * to host, runs Newton in double, then copies beta back to device.
 */
extern "C" void solve(const float* X_dev, const float* y_dev, float* beta_dev,
                      int n_samples, int n_features) {
    const int n = n_samples;
    const int d = n_features;

    if (n <= 0 || d <= 0) {
        return;
    }

    // Temporary host buffers (double precision for stability)
    double* X = (double*)malloc((size_t)n * d * sizeof(double));
    double* y = (double*)malloc((size_t)n * sizeof(double));
    double* beta = (double*)malloc((size_t)d * sizeof(double));
    double* grad = (double*)malloc((size_t)d * sizeof(double));
    double* delta = (double*)malloc((size_t)d * sizeof(double));
    double* H = (double*)malloc((size_t)d * d * sizeof(double));
    double* p = (double*)malloc((size_t)n * sizeof(double));
    double* w = (double*)malloc((size_t)n * sizeof(double));

    if (!X || !y || !beta || !grad || !delta || !H || !p || !w) {
        // Allocation failure: free what we can and return
        if (X) free(X);
        if (y) free(y);
        if (beta) free(beta);
        if (grad) free(grad);
        if (delta) free(delta);
        if (H) free(H);
        if (p) free(p);
        if (w) free(w);
        return;
    }

    // Copy X, y from device to host (via temporary float buffers)
    float* X_tmp = (float*)malloc((size_t)n * d * sizeof(float));
    float* y_tmp = (float*)malloc((size_t)n * sizeof(float));
    if (!X_tmp || !y_tmp) {
        if (X_tmp) free(X_tmp);
        if (y_tmp) free(y_tmp);
        free(X); free(y); free(beta); free(grad); free(delta); free(H); free(p); free(w);
        return;
    }

    cudaMemcpy(X_tmp, X_dev, (size_t)n * d * sizeof(float), cudaMemcpyDeviceToHost);
    cudaMemcpy(y_tmp, y_dev, (size_t)n * sizeof(float), cudaMemcpyDeviceToHost);

    // Convert to double
    for (int i = 0; i < n * d; ++i) {
        X[i] = (double)X_tmp[i];
    }
    for (int i = 0; i < n; ++i) {
        y[i] = (double)y_tmp[i];
    }

    free(X_tmp);
    free(y_tmp);

    // Initialize beta = 0
    for (int j = 0; j < d; ++j) {
        beta[j] = 0.0;
    }

    // Newton / IRLS hyperparameters
    const int    MAX_ITER = 25;
    const double TOL      = 1e-8;

    for (int iter = 0; iter < MAX_ITER; ++iter) {
        // 1) Compute p_i = sigmoid(x_i^T beta), w_i = p_i (1 - p_i)
        for (int i = 0; i < n; ++i) {
            double z = 0.0;
            const double* Xi = X + (size_t)i * d;
            for (int j = 0; j < d; ++j) {
                z += Xi[j] * beta[j];
            }
            double pi = sigmoid_double(z);
            p[i] = pi;
            w[i] = pi * (1.0 - pi);
        }

        // 2) Gradient: grad = X^T (p - y) + lambda * beta
        //    We build it as: grad[j] = lambda * beta[j] + sum_i X_ij * (p_i - y_i)
        double max_abs_grad = 0.0;

        for (int j = 0; j < d; ++j) {
            grad[j] = REG_LAMBDA * beta[j];
        }

        for (int i = 0; i < n; ++i) {
            double t = p[i] - y[i];
            const double* Xi = X + (size_t)i * d;
            for (int j = 0; j < d; ++j) {
                grad[j] += Xi[j] * t;
            }
        }

        for (int j = 0; j < d; ++j) {
            double g = fabs(grad[j]);
            if (g > max_abs_grad) max_abs_grad = g;
        }

        if (max_abs_grad < TOL) {
            // Converged
            break;
        }

        // 3) Hessian: H = X^T W X + lambda * I
        //    W is diagonal with w_i = p_i (1 - p_i)
        //    We exploit symmetry (compute upper triangle and mirror).
        for (int j = 0; j < d * d; ++j) {
            H[j] = 0.0;
        }

        for (int i = 0; i < n; ++i) {
            const double* Xi = X + (size_t)i * d;
            double wi = w[i];
            if (wi == 0.0) continue;
            for (int j = 0; j < d; ++j) {
                double xij = Xi[j];
                double w_xij = wi * xij;
                for (int k = 0; k <= j; ++k) {
                    H[j * d + k] += w_xij * Xi[k];
                }
            }
        }

        // Mirror the symmetric Hessian and add regularization on the diagonal
        for (int j = 0; j < d; ++j) {
            for (int k = 0; k < j; ++k) {
                H[k * d + j] = H[j * d + k];
            }
            H[j * d + j] += REG_LAMBDA;
        }

        // 4) Solve H * delta = grad  (for minimizing L)
        //    Then beta <- beta - delta
        for (int j = 0; j < d; ++j) {
            delta[j] = 0.0;
        }

        // We will overwrite grad when solving; pass it as RHS b.
        solve_linear_system(H, grad, delta, d);

        // Update beta
        for (int j = 0; j < d; ++j) {
            beta[j] -= delta[j];
        }
    }

    // Copy final beta back to device as float
    float* beta_tmp = (float*)malloc((size_t)d * sizeof(float));
    if (beta_tmp) {
        for (int j = 0; j < d; ++j) {
            beta_tmp[j] = (float)beta[j];
        }
        cudaMemcpy(beta_dev, beta_tmp, (size_t)d * sizeof(float), cudaMemcpyHostToDevice);
        free(beta_tmp);
    }

    // Free host buffers
    free(X);
    free(y);
    free(beta);
    free(grad);
    free(delta);
    free(H);
    free(p);
    free(w);
}




//--------------------------------------------------------------------------------------------------
/*
question0:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

è¿™æ®µä»£ç å®ç°äº†ä¸€ä¸ª**L2 æ­£åˆ™åŒ–é€»è¾‘å›å½’ï¼ˆL2-Regularized Logistic Regressionï¼‰æ±‚è§£å™¨**ï¼Œå®ƒä½¿ç”¨ **ç‰›é¡¿æ³• (Newton's Method)** æˆ–å…¶å˜ä½“ **è¿­ä»£é‡åŠ æƒæœ€å°äºŒä¹˜æ³• (IRLS)** åœ¨ **CPU ç«¯** è¿›è¡Œæ±‚è§£ã€‚

è¯¥ä»£ç çš„ç‰¹ç‚¹æ˜¯ï¼šåˆ©ç”¨äº† **CUDA** è¿è¡Œæ—¶ API å°†æ•°æ®ä» GPU ä¼ è¾“åˆ° CPUï¼Œå¹¶åœ¨ CPU çš„ **åŒç²¾åº¦ (double)** ç¯å¢ƒä¸­æ‰§è¡Œå¤æ‚çš„è¿­ä»£ä¼˜åŒ–è®¡ç®—ï¼Œä»¥ä¿è¯æ•°å€¼ç¨³å®šæ€§ã€‚

-----

## âš™ï¸ I. æ ¸å¿ƒç®—æ³•ä¸æ•°æ®ç»“æ„

### 1\. ç®—æ³•æ ¸å¿ƒï¼šç‰›é¡¿æ³• / IRLS

ç‰›é¡¿æ³•æ˜¯ä¸€ç§ç”¨äºå¯»æ‰¾å‡½æ•°æ ¹çš„è¿­ä»£ä¼˜åŒ–ç®—æ³•ã€‚åœ¨é€»è¾‘å›å½’ä¸­ï¼Œå®ƒé€šè¿‡ä»¥ä¸‹æ­¥éª¤è¿­ä»£é€¼è¿‘æœ€ä¼˜è§£ \betaï¼š

\beta_t+1 = \beta_t - H^-1 \nabla L(\beta_t)

  * \nabla L(\beta_t)ï¼šæŸå¤±å‡½æ•° L çš„**æ¢¯åº¦ (Gradient)**ã€‚
  * Hï¼šæŸå¤±å‡½æ•° L çš„ **Hessian çŸ©é˜µ**ï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰ã€‚
  * H * \delta = \nabla Lï¼šæ±‚è§£ç‰›é¡¿æ­¥é•¿ \deltaï¼Œç„¶åæ›´æ–° \beta <-- \beta - \deltaã€‚

### 2\. å…³é”®æ•°æ®ç»“æ„ (Host Buffers)

ä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼Œæ‰€æœ‰è®¡ç®—éƒ½åœ¨åŒç²¾åº¦ (`double`) ä¸‹è¿›è¡Œã€‚

  * X, yï¼šè®­ç»ƒæ•°æ®å’Œæ ‡ç­¾ã€‚
  * betaï¼šå¾…æ±‚è§£çš„ç³»æ•°å‘é‡ã€‚
  * gradï¼šæŸå¤±å‡½æ•°çš„æ¢¯åº¦å‘é‡ (\nabla L)ã€‚
  * Hï¼šHessian çŸ©é˜µã€‚
  * pï¼šé¢„æµ‹æ¦‚ç‡å‘é‡ (p_i = \textsigmoid(x_i^T \beta))ã€‚
  * wï¼šæƒé‡å‘é‡ (w_i = p_i (1 - p_i)ï¼Œç”¨äºæ„å»º Hessian çŸ©é˜µ H = X^T W X + \lambda I)ã€‚

-----

## ğŸš€ II. è¾…åŠ©å‡½æ•°è§£é‡Š

### 1\. æ•°å€¼ç¨³å®š Sigmoid (`sigmoid_double`)

c
static inline double sigmoid_double(double z) 
    if (z >= 0.0)    else   



  * **ç›®çš„:** è®¡ç®— Sigmoid å‡½æ•° \frac11 + e^-zã€‚
  * **ä¼˜åŒ–:** ä¸ºäº†é¿å…å½“ z å¾ˆå¤§æ—¶ e^-z ä¸‹æº¢ï¼ˆunderflowï¼‰æˆ– e^z æº¢å‡ºï¼ˆoverflowï¼‰å¯¼è‡´çš„æ•°å€¼ä¸ç¨³å®šï¼Œå‡½æ•°ä½¿ç”¨æ¡ä»¶åˆ¤æ–­ï¼š
      * å½“ z \ge 0 æ—¶ï¼Œè®¡ç®— \frac11 + e^-zã€‚
      * å½“ z < 0 æ—¶ï¼Œç­‰æ•ˆè®¡ç®— \frace^z1 + e^zã€‚

### 2\. çº¿æ€§ç³»ç»Ÿæ±‚è§£ (`solve_linear_system`)

c
static void solve_linear_system(double* A, double* b, double* x, int d) 
    // Forward elimination
    // Back substitution



  * **ç›®çš„:** æ±‚è§£ç‰›é¡¿æ­¥é•¿æ‰€éœ€çš„çº¿æ€§æ–¹ç¨‹ç»„ A * \delta = b (H * \delta = grad)ã€‚
  * **æ–¹æ³•:** ä½¿ç”¨ **é«˜æ–¯æ¶ˆå…ƒæ³• (Gaussian Elimination)**ï¼Œå¹¶ç»“åˆ**éƒ¨åˆ†ä¸»å…ƒé€‰æ‹© (Partial Pivoting)** æ¥å¢å¼ºæ•°å€¼ç¨³å®šæ€§ã€‚
      * **ä¸»å…ƒé€‰æ‹©:** æ‰¾åˆ°å½“å‰åˆ—ä¸­ç»å¯¹å€¼æœ€å¤§çš„å…ƒç´ ä½œä¸ºä¸»å…ƒï¼Œå¹¶äº¤æ¢è¡Œï¼Œé¿å…é™¤ä»¥æ¥è¿‘é›¶çš„æ•°ã€‚

-----

## ğŸ§­ III. è¿­ä»£æ±‚è§£ (`solve` å‡½æ•°æ ¸å¿ƒå¾ªç¯)

`solve` å‡½æ•°å°†è®¾å¤‡æ•°æ®å¤åˆ¶åˆ°ä¸»æœºï¼Œç„¶ååœ¨ä¸€ä¸ªå¾ªç¯ä¸­æ‰§è¡Œç‰›é¡¿æ³•çš„ 4 ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼Œç›´åˆ°æ”¶æ•›æˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° (25 æ¬¡)ã€‚

### é˜¶æ®µ 1ï¼šè®¡ç®—é¢„æµ‹æ¦‚ç‡å’Œæƒé‡ (p å’Œ w)

c
// 1) Compute p_i = sigmoid(x_i^T beta), w_i = p_i (1 - p_i)
for (int i = 0; i < n; ++i) 
    // ... è®¡ç®—çº¿æ€§é¢„æµ‹ z = x_i^T * beta ...
    double pi = sigmoid_double(z);
    p[i] = pi;
    w[i] = pi * (1.0 - pi); // è®¡ç®—æƒé‡ w_i



### é˜¶æ®µ 2ï¼šè®¡ç®—æ¢¯åº¦ (grad)

\nabla L = X^T (p - y) + \lambda \beta

c
// 2) Gradient: grad = X^T (p - y) + lambda * beta
// ... åˆå§‹åŒ– grad[j] = REG_LAMBDA * beta[j] (L2 æ­£åˆ™é¡¹) ...
for (int i = 0; i < n; ++i) 
    double t = p[i] - y[i]; // è®¡ç®—æ®‹å·® (p - y)
    for (int j = 0; j < d; ++j) 
        grad[j] += Xi[j] * t; // ç´¯åŠ  X^T * (p - y)
    



  * å¦‚æœæ¢¯åº¦çš„ç»å¯¹å€¼æœ€å¤§å€¼å°äºå®¹å¿åº¦ (TOL)ï¼Œåˆ™è®¤ä¸º**æ”¶æ•›**ï¼Œé€€å‡ºå¾ªç¯ã€‚

### é˜¶æ®µ 3ï¼šè®¡ç®— Hessian çŸ©é˜µ (H)

H = X^T W X + \lambda I

c
// 3) Hessian: H = X^T W X + lambda * I
// ... (åˆå§‹åŒ– H) ...
for (int i = 0; i < n; ++i)  // éå†æ ·æœ¬
    double wi = w[i];
    for (int j = 0; j < d; ++j) 
        double w_xij = wi * Xi[j];
        for (int k = 0; k <= j; ++k) 
            H[j * d + k] += w_xij * Xi[k]; // åˆ©ç”¨å¯¹ç§°æ€§åªè®¡ç®—ä¸Šä¸‰è§’
        
    

// Mirror the symmetric Hessian and add regularization on the diagonal
// ... H[k*d + j] = H[j*d + k] ... 
// ... H[j*d + j] += REG_LAMBDA ... (å¯¹è§’çº¿åŠ ä¸Š L2 æ­£åˆ™é¡¹)


  * **W** æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œå…¶å…ƒç´  w_i ç”± p_i(1-p_i) æ„æˆã€‚ä»£ç é€šè¿‡ç›´æ¥å°† w_i ä¹˜åˆ° X_ij ä¸Šï¼Œé¿å…äº†æ˜¾å¼æ„é€  W çŸ©é˜µã€‚

### é˜¶æ®µ 4ï¼šæ±‚è§£å’Œæ›´æ–° (\beta)

c
// 4) Solve H * delta = grad  (for minimizing L)
solve_linear_system(H, grad, delta, d); // æ±‚è§£æ­¥é•¿ delta
// Update beta
for (int j = 0; j < d; ++j) 
    beta[j] -= delta[j]; // beta <- beta - delta



-----

## ğŸ IV. ç»“è®º

è¿™ä¸ª `solve` å‡½æ•°å±•ç¤ºäº†åœ¨ CUDA ç¯å¢ƒä¸‹è¿›è¡Œå¤æ‚ä¼˜åŒ–çš„ä¸€ä¸ª**æ··åˆç­–ç•¥**ï¼šå°†æ•°æ®ä» GPU å†…å­˜ç§»åŠ¨åˆ° CPU å†…å­˜ï¼Œåˆ©ç”¨ CPU çš„é«˜ç²¾åº¦æµ®ç‚¹è¿ç®—å’Œå¤æ‚çš„çº¿æ€§ä»£æ•°æ±‚è§£å™¨æ¥ä¿è¯ **æ•°å€¼çš„ç¨³å®šæ€§** å’Œ **æ±‚è§£çš„æ­£ç¡®æ€§**ï¼Œç„¶åå°†æœ€ç»ˆç»“æœä¼ å› GPUã€‚

//--------------------------------------------------------------------------------------------------

//--------------------------------------------------------------------------------------------------
/*
question1:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question2:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------

//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------

//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------

//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------

//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------

//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------

//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------

//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------

//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------

//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question0:


//--------------------------------------------------------------------------------------------------


