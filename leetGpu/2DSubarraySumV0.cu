
#include <cuda_runtime.h>

#define TILE_X 32
#define TILE_Y 32
#define CFACTOR 8
__global__ void subarray_sum_2d(const int* input, int* output, int N, int M, int S_ROW, 
                            int E_ROW, int S_COL, int E_COL){
    int col = threadIdx.x + CFACTOR * blockDim.x * blockIdx.x;
    int row = threadIdx.y + CFACTOR * blockDim.y * blockIdx.y;
    
    __shared__ int Ms[TILE_X * TILE_Y];

    int v = 0;
    for(int i = 0; i < CFACTOR; i++){
        int row1 = row + i * blockDim.y;
        for(int j = 0; j < CFACTOR; j++){
            int col1 = col + j * blockDim.x;
            if(S_ROW <= row1 && row1 <= E_ROW && S_COL <= col1 && col1 <= E_COL){
                v += input[row1 * M + col1];
            }
        }
    }
    Ms[threadIdx.y * TILE_X + threadIdx.x] = v;
    __syncthreads();

    int p = threadIdx.y * TILE_X + threadIdx.x;
    for(int j = TILE_X * TILE_Y / 2; j > 0; j >>= 1){
        if(p < j){
            Ms[p] += Ms[p + j];
        }
        //correct
        __syncthreads();
    }
    //wrong
    __syncthreads();

    if(p == 0){
        atomicAdd(output, Ms[0]);
    }
}


// input, output are device pointers (i.e. pointers to memory on the GPU)
extern "C" void solve(const int* input, int* output, int N, int M, int S_ROW, int E_ROW, int S_COL, int E_COL) {
    dim3 threadsPerBlock(TILE_Y, TILE_X);
    dim3 blocksPerGrid(
        (N + CFACTOR * TILE_Y - 1) / (CFACTOR * TILE_Y),
        (M + CFACTOR * TILE_X - 1) / (CFACTOR * TILE_X)
    );

    subarray_sum_2d<<<blocksPerGrid, threadsPerBlock>>>(
        input, output, N, M, S_ROW, E_ROW, S_COL, E_COL
    );

}


//--------------------------------------------------------------------------------------------------
/*
question0:
ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

è¿™æ®µ CUDA ä»£ç å®ç°äº†ä¸€ä¸ªé«˜æ•ˆçš„ **å¹¶è¡Œå­æ•°ç»„æ±‚å’Œï¼ˆParallel Subarray Summationï¼‰** ç®—æ³•ï¼Œç”¨äºè®¡ç®—ä¸€ä¸ªå¤§å‹äºŒç»´çŸ©é˜µä¸­**æŒ‡å®šçŸ©å½¢åŒºåŸŸ**å†…æ‰€æœ‰å…ƒç´ çš„æ€»å’Œã€‚

å®ƒä½¿ç”¨äº†ç»å…¸çš„ **Shared Memory å½’çº¦ (Reduction)** å’Œ **åä½œå¼ Block-Stride Loop** æŠ€æœ¯ã€‚

-----

## âš™ï¸ I. æ ¸å¿ƒç›®æ ‡å’Œåˆ†å·¥æœºåˆ¶

### 1\. ç›®æ ‡

è®¡ç®—çŸ©é˜µ input ä¸­ï¼Œç”±èµ·å§‹åæ ‡ (S_ROW, S_COL) åˆ°ç»“æŸåæ ‡ (E_ROW, E_COL) å®šä¹‰çš„å­çŸ©é˜µçš„æ€»å’Œã€‚

### 2\. å®å®šä¹‰å’Œåˆ†å·¥

| å® | å€¼ | å«ä¹‰ |
| :--- | :--- | :--- |
| TILE_X, TILE_Y | 32, 32 | çº¿ç¨‹å—çš„ç»´åº¦ (blockDim) |
| CFACTOR | 8 | ç²—ç²’åº¦å› å­ï¼ˆæ¯ä¸ªçº¿ç¨‹è´Ÿè´£çš„å¾ªç¯æ¬¡æ•°ï¼‰ |
| **æ€»çº¿ç¨‹æ•°** | 32 * 32 = 1024 | |

  * **æ¯ä¸ª Block è´Ÿè´£çš„æ€»åŒºåŸŸ:** 32 * 8 * 32 * 8 = 256 * 256 çš„ä¸€ä¸ª Tile åŒºåŸŸã€‚
  * **çº¿ç¨‹åˆ†å·¥:** æ¯ä¸ªçº¿ç¨‹è´Ÿè´£è®¡ç®— CFACTOR * CFACTOR = 8 * 8 = 64 ä¸ªå…ƒç´ çš„å’Œï¼ˆé€šè¿‡ i å’Œ j å¾ªç¯å®ç°ï¼‰ã€‚

-----

## ğŸš€ II. Kernel å†…éƒ¨æµç¨‹

### 1\. ç´¢å¼•è®¡ç®— (Block-Interleaved Start)

c
int col = threadIdx.x + CFACTOR * blockDim.x * blockIdx.x;
int row = threadIdx.y + CFACTOR * blockDim.y * blockIdx.y;


  * **ç›®çš„:** è®¡ç®—å½“å‰çº¿ç¨‹åœ¨æ•´ä¸ªå¤§çŸ©é˜µ input ä¸­çš„**èµ·å§‹åæ ‡** (row, col)ã€‚
  * **åŸç†:**
      * threadIdx.x / threadIdx.yï¼šçº¿ç¨‹åœ¨ Block å†…çš„åç§»ã€‚
      * CFACTOR * blockDim.x * blockIdx.xï¼šè¿™æ˜¯ Block çº§åˆ«çš„è·³è·ƒï¼Œç¡®ä¿æ¯ä¸ª Block ä»æ­£ç¡®çš„å¤§ Tile èµ·å§‹ç‚¹å¼€å§‹ã€‚

> **ç¤ºä¾‹:** å‡è®¾ tx=5, ty=1 ä½äº bx=1, by=0 çš„ Blockã€‚
>
>   * **row** (è¡Œ): 1 + 8 * 32 * 0 = 1
>   * **col** (åˆ—): 5 + 8 * 32 * 1 = 5 + 256 = 261
>   * **ç»“è®º:** çº¿ç¨‹ (1, 5) çš„è®¡ç®—ä»çŸ©é˜µçš„ (1, 261) ä½ç½®å¼€å§‹ã€‚

### 2\. å±€éƒ¨æ±‚å’Œ (Block-Stride Loop)

c
for(int i = 0; i < CFACTOR; i++){
    int row1 = row + i * blockDim.y;
    for(int j = 0; j < CFACTOR; j++){
        int col1 = col + j * blockDim.x;
        if(S_ROW <= row1 && row1 <= E_ROW && S_COL <= col1 && col1 <= E_COL){
            v += input[row1 * M + col1]; // ç´¯åŠ æœ‰æ•ˆå…ƒç´ 
        
    



  * **ç›®çš„:** æ¯ä¸ªçº¿ç¨‹è®¡ç®—å®ƒè´Ÿè´£çš„ 8 * 8 = 64 ä¸ªå…ƒç´ ä¹‹å’Œ vã€‚
  * **å¾ªç¯æ­¥é•¿:**
      * row1 = row + i * blockDim.y (32)ã€‚
      * col1 = col + j * blockDim.x (32)ã€‚
  * **äº¤é”™è®¿é—®:** è¿™ç¡®ä¿äº†çº¿ç¨‹ tx å’Œ tx+1 åœ¨ i=0, j=0 æ—¶è¯»å–çš„ç´¢å¼•æ˜¯ col å’Œ col+1ï¼ˆè¿ç»­ï¼‰ï¼Œå®ç°äº† **å†…å­˜åˆå¹¶**ã€‚
  * **è¾¹ç•Œæ£€æŸ¥:** `if (S_ROW <= row1 && ...)`ï¼šè¿™æ˜¯æœ€å…³é”®çš„é€»è¾‘ï¼Œå®ƒæ£€æŸ¥å½“å‰è®¡ç®—çš„åæ ‡ (row1, col1) æ˜¯å¦è½åœ¨äº†ç”¨æˆ·æŒ‡å®šçš„çŸ©å½¢å­åŒºåŸŸå†…ã€‚åªæœ‰åœ¨å­åŒºåŸŸå†…çš„å…ƒç´ æ‰ä¼šè¢«ç´¯åŠ ã€‚

### 3\. Shared Memory å­˜å‚¨

c
__shared__ int Ms[TILE_X * TILE_Y]; // 1024 é•¿åº¦
Ms[threadIdx.y * TILE_X + threadIdx.x] = v;
__syncthreads();


  * **ç›®çš„:** å°†çº¿ç¨‹çš„å±€éƒ¨å’Œ v å­˜å…¥ Ms æ•°ç»„ã€‚
  * **ç´¢å¼•:** ä½¿ç”¨æ ‡å‡†çš„ 2D åˆ° 1D æ˜ å°„ (ty * 32 + tx)ã€‚

### 4\. Block å†…éƒ¨å½’çº¦ (Reduction)

c
int p = threadIdx.y * TILE_X + threadIdx.x;
for(int j = TILE_X * TILE_Y / 2; j > 0; j >>= 1){
    if(p < j){
        Ms[p] += Ms[p + j];
    
    __syncthreads();



  * **ç›®çš„:** å¯¹ Ms æ•°ç»„æ‰§è¡Œæ ‡å‡†çš„ **å¹¶è¡Œå½’çº¦ï¼ˆæ±‚å’Œï¼‰**ã€‚
  * **æœºåˆ¶:** è¿™æ˜¯ä¸€ä¸ªç»å…¸çš„ **æ ‘å½¢å½’çº¦** ç®—æ³•ã€‚æ‰€æœ‰ 1024 ä¸ªçº¿ç¨‹åä½œï¼Œå°†æ‰€æœ‰å±€éƒ¨å’Œç´¯åŠ åˆ° Ms[0]ã€‚

### 5\. æœ€ç»ˆåŸå­ç´¯åŠ 

c
if(p == 0){
    atomicAdd(output, Ms[0]);



  * **ç›®çš„:** åªæœ‰çº¿ç¨‹ p=0 (å³ tx=0, ty=0) å°†æ•´ä¸ª Block çš„æœ€ç»ˆæ€»å’Œ Ms[0] ä½¿ç”¨ atomicAdd ç´¯åŠ åˆ°å…¨å±€ç»“æœ output ä¸­ã€‚

## ğŸ’» III. ä¸»æœºç«¯å¯åŠ¨ (`solve`)

c
// ...
dim3 dimGrid(
    (N + CFACTOR * TILE_Y - 1) / (CFACTOR * TILE_Y), // Grid Y size
    (M + CFACTOR * TILE_X - 1) / (CFACTOR * TILE_X) // Grid X size
);


  * **Grid å°ºå¯¸:** Grid çš„ Y è½´è´Ÿè´£è¦†ç›– N ç»´åº¦ï¼ˆè¡Œï¼‰ï¼ŒX è½´è´Ÿè´£è¦†ç›– M ç»´åº¦ï¼ˆåˆ—ï¼‰ã€‚
  * **åŸç†:** ä½¿ç”¨ **å‘ä¸Šå–æ•´é™¤æ³•** ç¡®ä¿ Grid å¯åŠ¨è¶³å¤Ÿçš„ Blocks æ¥è¦†ç›– N * M çŸ©é˜µçš„æ‰€æœ‰éƒ¨åˆ†ã€‚
      * **é™¤æ•°:** CFACTOR * TILE_Y (å³ 256) æ˜¯æ¯ä¸ª Block åœ¨è¯¥ç»´åº¦ä¸Šè´Ÿè´£çš„æ€»å…ƒç´ æ•°ã€‚
  * **å¯åŠ¨:** `subarray_sum_2d<<<dimGrid, threadsPerBlock>>>` å¯åŠ¨ Kernelã€‚

//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question1:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­

why this is correct:
    int p = threadIdx.y * TILE_X + threadIdx.x;
    for(int j = TILE_X * TILE_Y / 2; j > 0; j >>= 1){
        if(p < j){
            Ms[p] += Ms[p + j];
        }
        __syncthreads();
    }

but this is wrong
    int p = threadIdx.y * TILE_X + threadIdx.x;
    for(int j = TILE_X * TILE_Y / 2; j > 0; j >>= 1){
        if(p < j){
            Ms[p] += Ms[p + j];
        }
    }
    __syncthreads();


answer:
That's an excellent question that highlights the absolute necessity of **synchronization** in collaborative parallel algorithms like **Reduction**.

The first code block is **correct** because it uses `__syncthreads()` inside the loop, guaranteeing **data integrity** at every stage of the parallel reduction. The second code block is **wrong** because the lack of synchronization causes **data races** and **unpredictable results**.

Here is a breakdown of why synchronization is mandatory in the parallel reduction loop.

-----

## 1\. ğŸ¯ The Goal: Parallel Tree Reduction

The code implements a **Tree Reduction** algorithm in shared memory. The goal is to sum 1024 values (stored in Ms[0] to Ms[1023]) into a single value (Ms[0]) in \log_2(1024) = 10 steps.

  * In each step (j), threads simultaneously read data from Ms[p] and Ms[p+j] and write the result back to Ms[p].
  * The array \mathbf{Ms} is **Shared Memory**, meaning all threads read from and write to the same physical memory space.

-----

## 2\. âŒ Why the Second Block is WRONG (Data Race)

In the second code block, where `__syncthreads()` is *outside* the loop:

c
// WRONG: Synchronization happens only once at the end
for(int j = TILE_X * TILE_Y / 2; j > 0; j >>= 1){
    if(p < j){
        Ms[p] += Ms[p + j]; // Threads read and write without coordination
    }
}
__syncthreads(); // Too late!


**The Problem:** **Data Race Condition**

1.  **Step 1:** Let j=512. Thread p=0 reads Ms[0] and Ms[512]. Thread p=511 reads Ms[511] and Ms[1023]. They compute and write back to their respective Ms[p].
2.  **Step 2:** j becomes 256. Threads now try to read the **new** values computed in Step 1.
3.  **The Failure:** The GPU scheduler (Warp Scheduler) might execute the instruction for thread p=100 (which is reading Ms[356] in Step 2) *before* thread p=356 has finished writing its result from Step 1.
      * Since there is no guarantee that all Step 1 writes are complete before Step 2 reads begin, threads will likely read **stale or partially written data**.
      * The final sum Ms[0] will be incorrect and non-deterministic (it changes every time you run the code).

-----

## 3\. âœ… Why the First Block is CORRECT (Synchronization)

In the first code block, `__syncthreads()` is **inside** the loop:

c
for(int j = TILE_X * TILE_Y / 2; j > 0; j >>= 1){
    if(p < j){
        Ms[p] += Ms[p + j];
    }
    __syncthreads(); // MANDATORY synchronization point
}


**The Solution:** **Barrier Synchronization**

1.  **Start of Loop (Step J):** All threads are synchronized.
2.  **Computation:** Threads read Ms[p] and Ms[p+j], compute the sum, and write back to Ms[p].
3.  **`__syncthreads()`:** This barrier forces **all threads in the Block** to wait until **every single thread has finished its write operation** for Step J.
4.  **Next Step (Step J/2):** When the loop continues to j/2, all threads are guaranteed to be reading the **latest, complete, and correct** results from the previous step.

**Conclusion:** The `__syncthreads()` inside the loop creates the **parallel execution barrier** necessary for the collaborative tree reduction to work correctly, preventing data races and ensuring the final result is the true sum.

//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question2:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­



//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question3:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­




//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question4:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­



//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question5:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­


//--------------------------------------------------------------------------------------------------




//--------------------------------------------------------------------------------------------------
/*
question6:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œæœ‰ä¾‹å­




//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question7:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­




//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question8:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­



//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question10:

ä¸ç†è§£ï¼Œè¯·è§£é‡Šï¼Œç”¨ä¾‹å­


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question11:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question12:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question13:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question14:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question15:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question16:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question17:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question18:


//--------------------------------------------------------------------------------------------------


//--------------------------------------------------------------------------------------------------
/*
question19:


//--------------------------------------------------------------------------------------------------



//--------------------------------------------------------------------------------------------------
/*
question20:


//--------------------------------------------------------------------------------------------------
